# # # """
# # # Culture Collector — Task 5.0c (CS3)
# # # app/pipelines/glassdoor_collector.py

# # # 3-source culture signal collector:
# # #   1. Glassdoor   — via RapidAPI (real-time-glassdoor-data)
# # #   2. Indeed      — via Playwright + BeautifulSoup
# # #   3. CareerBliss — via Playwright + BeautifulSoup

# # # Source reliability: Glassdoor 0.85, Indeed 0.80, CareerBliss 0.75
# # # Scoring uses Decimal precision throughout.

# # # Requirements:
# # #     pip install playwright httpx beautifulsoup4 python-dotenv
# # #     playwright install chromium

# # # Usage (ticker REQUIRED to protect API quota):
# # #     python -m pipelines.culture_collector NVDA
# # #     python -m pipelines.culture_collector NVDA JPM WMT
# # #     python -m pipelines.culture_collector --all
# # #     python -m pipelines.culture_collector NVDA --no-cache
# # #     python -m pipelines.culture_collector NVDA --sources=glassdoor,indeed,careerbliss

# # # Tickers (5):
# # #     NVDA, JPM, WMT, GE, DG
# # # """

# # # import json
# # # import logging
# # # import os
# # # import re
# # # import sys
# # # import time
# # # from dataclasses import dataclass, field, asdict
# # # from datetime import datetime, timezone, timedelta
# # # from decimal import Decimal, ROUND_HALF_UP
# # # from pathlib import Path
# # # from typing import Any, Dict, List, Optional

# # # import httpx
# # # from dotenv import load_dotenv

# # # _THIS_FILE = Path(__file__).resolve()
# # # _APP_DIR = _THIS_FILE.parent.parent
# # # _PROJECT_ROOT = _APP_DIR.parent
# # # for _p in [str(_PROJECT_ROOT), str(_APP_DIR)]:
# # #     if _p not in sys.path:
# # #         sys.path.insert(0, _p)

# # # load_dotenv(_PROJECT_ROOT / ".env")

# # # logging.basicConfig(level=logging.DEBUG, format="%(levelname)s | %(message)s")
# # # logger = logging.getLogger(__name__)

# # # logging.getLogger("httpx").setLevel(logging.WARNING)
# # # logging.getLogger("httpcore").setLevel(logging.WARNING)
# # # logging.getLogger("boto3").setLevel(logging.WARNING)
# # # logging.getLogger("botocore").setLevel(logging.WARNING)
# # # logging.getLogger("urllib3").setLevel(logging.WARNING)
# # # logging.getLogger("s3transfer").setLevel(logging.WARNING)


# # # # =====================================================================
# # # # DATA MODELS
# # # # =====================================================================

# # # @dataclass
# # # class CultureReview:
# # #     review_id: str
# # #     rating: float
# # #     title: str
# # #     pros: str
# # #     cons: str
# # #     advice_to_management: Optional[str] = None
# # #     is_current_employee: bool = True
# # #     job_title: str = ""
# # #     review_date: Optional[datetime] = None
# # #     source: str = "unknown"

# # #     def __post_init__(self):
# # #         if self.review_date is None:
# # #             self.review_date = datetime.now(timezone.utc)


# # # @dataclass
# # # class CultureSignal:
# # #     company_id: str
# # #     ticker: str
# # #     innovation_score: Decimal = Decimal("50.00")
# # #     data_driven_score: Decimal = Decimal("0.00")
# # #     change_readiness_score: Decimal = Decimal("50.00")
# # #     ai_awareness_score: Decimal = Decimal("0.00")
# # #     overall_score: Decimal = Decimal("25.00")
# # #     review_count: int = 0
# # #     avg_rating: Decimal = Decimal("0.00")
# # #     current_employee_ratio: Decimal = Decimal("0.000")
# # #     confidence: Decimal = Decimal("0.000")
# # #     source_breakdown: Dict[str, int] = field(default_factory=dict)
# # #     positive_keywords_found: List[str] = field(default_factory=list)
# # #     negative_keywords_found: List[str] = field(default_factory=list)

# # #     def to_json(self, indent=2):
# # #         d = asdict(self)
# # #         for k, v in d.items():
# # #             if isinstance(v, Decimal):
# # #                 d[k] = float(v)
# # #         return json.dumps(d, indent=indent, default=str)


# # # # =====================================================================
# # # # COMPANY REGISTRY — 13 tickers
# # # # =====================================================================

# # # COMPANY_REGISTRY = {
# # #     "NVDA": {
# # #         "name": "NVIDIA", "sector": "Technology",
# # #         "glassdoor_id": "NVIDIA",
# # #         "indeed_slugs": ["NVIDIA"],
# # #         "careerbliss_slug": "nvidia",
# # #     },
# # #     "JPM": {
# # #         "name": "JPMorgan Chase", "sector": "Financial Services",
# # #         "glassdoor_id": "JPMorgan-Chase",
# # #         "indeed_slugs": ["JPMorgan-Chase", "jpmorgan-chase"],
# # #         "careerbliss_slug": "jpmorgan-chase",
# # #     },
# # #     "WMT": {
# # #         "name": "Walmart", "sector": "Consumer Retail",
# # #         "glassdoor_id": "Walmart",
# # #         "indeed_slugs": ["Walmart"],
# # #         "careerbliss_slug": "walmart",
# # #     },
# # #     "GE": {
# # #         "name": "GE Aerospace", "sector": "Industrials Manufacturing",
# # #         "glassdoor_id": "GE-Aerospace",
# # #         "indeed_slugs": ["GE-Aerospace", "General-Electric"],
# # #         "careerbliss_slug": "ge-aerospace",
# # #     },
# # #     "DG": {
# # #         "name": "Dollar General", "sector": "Consumer Retail",
# # #         "glassdoor_id": "Dollar-General",
# # #         "indeed_slugs": ["Dollar-General"],
# # #         "careerbliss_slug": "dollar-general",
# # #     },
# # # }

# # # ALLOWED_TICKERS = set(COMPANY_REGISTRY.keys())
# # # VALID_SOURCES = {"glassdoor", "indeed", "careerbliss"}


# # # def validate_ticker(ticker):
# # #     t = ticker.upper()
# # #     if t not in ALLOWED_TICKERS:
# # #         raise ValueError(
# # #             f"Unknown ticker '{t}'. Allowed: {', '.join(sorted(ALLOWED_TICKERS))}"
# # #         )
# # #     return t


# # # def all_tickers():
# # #     return sorted(ALLOWED_TICKERS)


# # # # =====================================================================
# # # # HELPERS
# # # # =====================================================================

# # # def _normalize_date(raw):
# # #     if not raw:
# # #         return None
# # #     raw = raw.strip()
# # #     iso = re.match(r"(\d{4}-\d{2}-\d{2})", raw)
# # #     if iso:
# # #         try:
# # #             return datetime.strptime(iso.group(1), "%Y-%m-%d").replace(tzinfo=timezone.utc)
# # #         except ValueError:
# # #             pass
# # #     for fmt in ("%B %d, %Y", "%b %d, %Y", "%d %B %Y", "%d %b %Y",
# # #                 "%m/%d/%Y", "%d/%m/%Y", "%Y/%m/%d"):
# # #         try:
# # #             return datetime.strptime(raw, fmt).replace(tzinfo=timezone.utc)
# # #         except ValueError:
# # #             continue
# # #     rel = re.match(r"(\d+)\s+(day|week|month|year)s?\s+ago", raw, re.I)
# # #     if rel:
# # #         num = int(rel.group(1))
# # #         unit = rel.group(2).lower()
# # #         days = {"day": 1, "week": 7, "month": 30, "year": 365}[unit]
# # #         return datetime.now(timezone.utc) - timedelta(days=num * days)
# # #     return None


# # # # =====================================================================
# # # # CULTURE COLLECTOR
# # # # =====================================================================

# # # class CultureCollector:

# # #     # ── Keyword lists ───────────────────────────────────────

# # #     INNOVATION_POSITIVE = [
# # #         "innovative", "cutting-edge", "forward-thinking",
# # #         "encourages new ideas", "experimental", "creative freedom",
# # #         "startup mentality", "move fast", "disruptive",
# # #         "innovation", "pioneering", "bleeding edge",
# # #     ]
# # #     INNOVATION_NEGATIVE = [
# # #         "bureaucratic", "slow to change", "resistant",
# # #         "outdated", "stuck in old ways", "red tape",
# # #         "politics", "siloed", "hierarchical",
# # #         "stagnant", "old-fashioned", "behind the times",
# # #     ]
# # #     DATA_DRIVEN_KEYWORDS = [
# # #         "data-driven", "metrics", "evidence-based",
# # #         "analytical", "kpis", "dashboards", "data culture",
# # #         "measurement", "quantitative",
# # #         "data informed", "analytics", "data-centric",
# # #     ]
# # #     AI_AWARENESS_KEYWORDS = [
# # #         "ai", "artificial intelligence", "machine learning",
# # #         "automation", "data science", "ml", "algorithms",
# # #         "predictive", "neural network",
# # #         "deep learning", "nlp", "llm", "generative ai",
# # #         "chatbot", "computer vision",
# # #     ]
# # #     CHANGE_POSITIVE = [
# # #         "agile", "adaptive", "fast-paced", "embraces change",
# # #         "continuous improvement", "growth mindset",
# # #         "evolving", "dynamic", "transforming",
# # #     ]
# # #     CHANGE_NEGATIVE = [
# # #         "rigid", "traditional", "slow", "risk-averse",
# # #         "change resistant", "old school",
# # #         "inflexible", "set in their ways", "fear of change",
# # #     ]

# # #     SOURCE_RELIABILITY = {
# # #         "glassdoor":   Decimal("0.85"),
# # #         "indeed":      Decimal("0.80"),
# # #         "careerbliss": Decimal("0.75"),
# # #         "unknown":     Decimal("0.70"),
# # #     }

# # #     RAPIDAPI_HOST = "real-time-glassdoor-data.p.rapidapi.com"
# # #     RAPIDAPI_BASE = f"https://{RAPIDAPI_HOST}"

# # #     def __init__(self, cache_dir="data/culture_cache"):
# # #         self.cache_dir = Path(cache_dir)
# # #         self.cache_dir.mkdir(parents=True, exist_ok=True)
# # #         self._browser = None
# # #         self._playwright = None

# # #     # ── Browser management ──────────────────────────────────

# # #     def _get_browser(self):
# # #         if self._browser is None:
# # #             from playwright.sync_api import sync_playwright
# # #             self._playwright = sync_playwright().start()
# # #             self._browser = self._playwright.chromium.launch(
# # #                 headless=True,
# # #                 args=[
# # #                     "--disable-blink-features=AutomationControlled",
# # #                     "--no-sandbox",
# # #                     "--disable-dev-shm-usage",
# # #                     "--disable-infobars",
# # #                     "--window-size=1920,1080",
# # #                 ],
# # #             )
# # #             logger.info("Playwright browser launched")
# # #         return self._browser

# # #     def _new_page(self, stealth=True):
# # #         browser = self._get_browser()
# # #         ctx = browser.new_context(
# # #             user_agent=(
# # #                 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
# # #                 "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
# # #             ),
# # #             viewport={"width": 1920, "height": 1080},
# # #             locale="en-US",
# # #             timezone_id="America/New_York",
# # #             extra_http_headers={
# # #                 "Accept-Language": "en-US,en;q=0.9",
# # #                 "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
# # #                 "Sec-Fetch-Dest": "document",
# # #                 "Sec-Fetch-Mode": "navigate",
# # #                 "Sec-Fetch-Site": "none",
# # #                 "Sec-Fetch-User": "?1",
# # #                 "Upgrade-Insecure-Requests": "1",
# # #             },
# # #         )
# # #         page = ctx.new_page()
# # #         if stealth:
# # #             page.add_init_script("""
# # #                 Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
# # #                 Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
# # #                 Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
# # #                 window.chrome = { runtime: {} };
# # #                 const origQuery = window.navigator.permissions.query;
# # #                 window.navigator.permissions.query = (p) =>
# # #                     p.name === 'notifications'
# # #                         ? Promise.resolve({ state: Notification.permission })
# # #                         : origQuery(p);
# # #             """)
# # #         page.route(
# # #             "**/*.{png,jpg,jpeg,gif,svg,webp,woff,woff2,ttf,mp4,webm}",
# # #             lambda route: route.abort(),
# # #         )
# # #         return page

# # #     def close_browser(self):
# # #         if self._browser:
# # #             self._browser.close()
# # #             self._browser = None
# # #         if self._playwright:
# # #             self._playwright.stop()
# # #             self._playwright = None
# # #             logger.info("Playwright browser closed")

# # #     # ── SOURCE 1: GLASSDOOR via RapidAPI ────────────────────

# # #     def _get_api_key(self):
# # #         key = os.getenv("RAPIDAPI_KEY", "")
# # #         if not key:
# # #             raise EnvironmentError(
# # #                 "RAPIDAPI_KEY not set. Add it to your .env file.\n"
# # #                 "Get a free key at: https://rapidapi.com/letscrape-6bRBa3QguO5/"
# # #                 "api/real-time-glassdoor-data"
# # #             )
# # #         return key

# # #     def _api_headers(self):
# # #         return {
# # #             "x-rapidapi-key": self._get_api_key(),
# # #             "x-rapidapi-host": self.RAPIDAPI_HOST,
# # #         }

# # #     def fetch_glassdoor(self, ticker, max_pages=3, timeout=30.0):
# # #         ticker = ticker.upper()
# # #         reg = COMPANY_REGISTRY[ticker]
# # #         company_id = reg["glassdoor_id"]
# # #         reviews = []

# # #         for page_num in range(1, max_pages + 1):
# # #             params = {
# # #                 "company_id": company_id,
# # #                 "page": str(page_num),
# # #                 "sort": "POPULAR",
# # #                 "language": "en",
# # #                 "only_current_employees": "false",
# # #                 "extended_rating_data": "false",
# # #                 "domain": "www.glassdoor.com",
# # #             }
# # #             url = f"{self.RAPIDAPI_BASE}/company-reviews"
# # #             logger.info(f"[{ticker}][glassdoor] Fetching page {page_num}...")

# # #             try:
# # #                 resp = httpx.get(url, headers=self._api_headers(), params=params, timeout=timeout)
# # #                 resp.raise_for_status()
# # #                 raw_data = resp.json()
# # #             except httpx.HTTPStatusError as e:
# # #                 logger.error(f"[{ticker}][glassdoor] HTTP {e.response.status_code} on page {page_num}")
# # #                 break
# # #             except Exception as e:
# # #                 logger.error(f"[{ticker}][glassdoor] Request failed: {e}")
# # #                 break

# # #             reviews_raw = raw_data.get("data", {}).get("reviews", [])
# # #             total = raw_data.get("data", {}).get("review_count", "?")
# # #             pages_total = raw_data.get("data", {}).get("page_count", "?")

# # #             if not reviews_raw:
# # #                 logger.info(f"[{ticker}][glassdoor] No more reviews at page {page_num}")
# # #                 break

# # #             for r in reviews_raw:
# # #                 parsed = self._parse_glassdoor_review(ticker, r)
# # #                 if parsed:
# # #                     reviews.append(parsed)

# # #             logger.info(
# # #                 f"[{ticker}][glassdoor] Page {page_num}: {len(reviews_raw)} reviews "
# # #                 f"(total available: {total}, pages: {pages_total})"
# # #             )
# # #             if page_num < max_pages:
# # #                 time.sleep(0.5)

# # #         logger.info(f"[{ticker}][glassdoor] Total fetched: {len(reviews)}")
# # #         return reviews

# # #     def _parse_glassdoor_review(self, ticker, raw):
# # #         try:
# # #             rid = f"glassdoor_{ticker}_{raw.get('review_id', 'unknown')}"
# # #             rating = float(raw.get("rating", 3.0))
# # #             title = raw.get("summary") or raw.get("title") or ""
# # #             pros = raw.get("pros") or ""
# # #             cons = raw.get("cons") or ""
# # #             advice = raw.get("advice_to_management") or None
# # #             job_title = raw.get("job_title") or ""
# # #             is_current = bool(raw.get("is_current_employee", False))
# # #             emp_status = raw.get("employment_status", "")
# # #             if isinstance(emp_status, str) and emp_status.upper() == "REGULAR":
# # #                 is_current = True
# # #             review_date = None
# # #             raw_date = raw.get("review_datetime") or None
# # #             if raw_date and isinstance(raw_date, str):
# # #                 review_date = _normalize_date(raw_date[:10])
# # #             return CultureReview(
# # #                 review_id=rid, rating=min(5.0, max(1.0, rating)),
# # #                 title=title[:200], pros=pros[:2000], cons=cons[:2000],
# # #                 advice_to_management=advice, is_current_employee=is_current,
# # #                 job_title=job_title, review_date=review_date, source="glassdoor",
# # #             )
# # #         except Exception as e:
# # #             logger.warning(f"[{ticker}][glassdoor] Parse error: {e}")
# # #             return None

# # #     # ── SOURCE 2: INDEED via Playwright + BeautifulSoup ─────

# # #     def scrape_indeed(self, ticker, limit=50):
# # #         from bs4 import BeautifulSoup
# # #         ticker = ticker.upper()
# # #         slugs = COMPANY_REGISTRY[ticker]["indeed_slugs"]
# # #         reviews = []

# # #         # Calculate pages needed (Indeed shows ~20 reviews per page)
# # #         max_pages = max(1, (limit // 20) + 1)

# # #         for slug in slugs:
# # #             for page_num in range(max_pages):
# # #                 start = page_num * 20
# # #                 url = f"https://www.indeed.com/cmp/{slug}/reviews"
# # #                 if page_num > 0:
# # #                     url = f"{url}?start={start}"
# # #                 logger.info(f"[{ticker}][indeed] Scraping page {page_num + 1}: {url}")
# # #                 try:
# # #                     page = self._new_page(stealth=True)
# # #                     page.goto(url, wait_until="domcontentloaded", timeout=30000)
# # #                     time.sleep(3)
# # #                     if "blocked" in page.title().lower():
# # #                         logger.warning(f"[{ticker}][indeed] Blocked: {page.title()}")
# # #                         page.close()
# # #                         break
# # #                     try:
# # #                         page.wait_for_selector(
# # #                             '[data-testid*="review"], .cmp-ReviewsList, '
# # #                             '[data-tn-component="reviewsList"]',
# # #                             timeout=10000,
# # #                         )
# # #                     except Exception:
# # #                         logger.warning(f"[{ticker}][indeed] Review container not found on page {page_num + 1}")
# # #                         page.close()
# # #                         break
# # #                     for _ in range(3):
# # #                         page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
# # #                         time.sleep(1)
# # #                     html = page.content()
# # #                     page.close()

# # #                     soup = BeautifulSoup(html, "html.parser")
# # #                     cards = (
# # #                         soup.find_all("div", {"data-testid": re.compile(r"review")})
# # #                         or soup.find_all("div", class_=re.compile(r"cmp-Review(?!sList)"))
# # #                         or soup.find_all("div", class_=re.compile(r"review", re.I))
# # #                     )

# # #                     if not cards:
# # #                         logger.info(f"[{ticker}][indeed] No more reviews at page {page_num + 1}")
# # #                         break

# # #                     page_count = 0
# # #                     for i, card in enumerate(cards):
# # #                         text = card.get_text(separator=" ", strip=True)
# # #                         if len(text) < 30:
# # #                             continue
# # #                         title_el = card.find(
# # #                             ["h2", "h3", "a", "span"],
# # #                             class_=re.compile(r"title|header", re.I),
# # #                         )
# # #                         title = title_el.get_text(strip=True) if title_el else text[:100]
# # #                         rating = 3.0
# # #                         star_el = card.find(attrs={"aria-label": re.compile(r"\d.*star", re.I)})
# # #                         if star_el:
# # #                             m = re.search(r"(\d+\.?\d*)", star_el.get("aria-label", ""))
# # #                             if m:
# # #                                 rating = float(m.group(1))
# # #                         pros_text, cons_text = "", ""
# # #                         for label in card.find_all(string=re.compile(r"^Pros?$", re.I)):
# # #                             p = label.find_parent()
# # #                             if p and p.find_next_sibling():
# # #                                 pros_text = p.find_next_sibling().get_text(separator=" ", strip=True)
# # #                         for label in card.find_all(string=re.compile(r"^Cons?$", re.I)):
# # #                             p = label.find_parent()
# # #                             if p and p.find_next_sibling():
# # #                                 cons_text = p.find_next_sibling().get_text(separator=" ", strip=True)
# # #                         if not pros_text and not cons_text:
# # #                             pros_text = text
# # #                         date_el = card.find("time") or card.find(class_=re.compile(r"date", re.I))
# # #                         review_date = None
# # #                         if date_el:
# # #                             raw_d = date_el.get("datetime") or date_el.get("content") or date_el.get_text(strip=True)
# # #                             review_date = _normalize_date(raw_d)
# # #                         is_current = "current" in text.lower() and "former" not in text.lower()
# # #                         global_idx = len(reviews)
# # #                         reviews.append(CultureReview(
# # #                             review_id=f"indeed_{ticker}_{global_idx}",
# # #                             rating=min(5.0, max(1.0, rating)),
# # #                             title=title[:200], pros=pros_text[:2000], cons=cons_text[:2000],
# # #                             is_current_employee=is_current, review_date=review_date,
# # #                             source="indeed",
# # #                         ))
# # #                         page_count += 1

# # #                     logger.info(f"[{ticker}][indeed] Page {page_num + 1}: {page_count} reviews (total: {len(reviews)})")

# # #                     if len(reviews) >= limit:
# # #                         break

# # #                     if page_num < max_pages - 1:
# # #                         time.sleep(2)

# # #                 except Exception as e:
# # #                     logger.warning(f"[{ticker}][indeed] Error on page {page_num + 1} for slug '{slug}': {e}")
# # #                     try:
# # #                         page.close()
# # #                     except Exception:
# # #                         pass
# # #                     break

# # #             if reviews:
# # #                 logger.info(f"[{ticker}][indeed] Extracted {len(reviews)} total reviews")
# # #                 break
# # #         return reviews[:limit]

# # #     # ── SOURCE 3: CAREERBLISS via Playwright + BeautifulSoup ─

# # #     def scrape_careerbliss(self, ticker, limit=50):
# # #         from bs4 import BeautifulSoup
# # #         ticker = ticker.upper()
# # #         slug = COMPANY_REGISTRY[ticker]["careerbliss_slug"]
# # #         reviews = []
# # #         dq = chr(34)

# # #         url = f"https://www.careerbliss.com/{slug}/reviews/"
# # #         logger.info(f"[{ticker}][careerbliss] Scraping: {url}")

# # #         try:
# # #             page = self._new_page(stealth=True)
# # #             page.goto(url, wait_until="domcontentloaded", timeout=30000)
# # #             time.sleep(3)

# # #             title_text = page.title().lower()
# # #             if any(w in title_text for w in ["blocked", "denied", "captcha"]):
# # #                 logger.warning(f"[{ticker}][careerbliss] Blocked: {page.title()}")
# # #                 page.close()
# # #                 return reviews

# # #             for _ in range(5):
# # #                 page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
# # #                 time.sleep(1)

# # #             for _ in range(3):
# # #                 try:
# # #                     more = page.query_selector(
# # #                         'a:has-text("More Reviews"), a:has-text("Show More"), '
# # #                         'button:has-text("More"), a.next'
# # #                     )
# # #                     if more and more.is_visible():
# # #                         more.click()
# # #                         time.sleep(2)
# # #                     else:
# # #                         break
# # #                 except Exception:
# # #                     break

# # #             html = page.content()
# # #             page.close()

# # #             soup = BeautifulSoup(html, "html.parser")

# # #             cards = (
# # #                 soup.find_all("div", class_=re.compile(r"review", re.I))
# # #                 or soup.find_all("li", class_=re.compile(r"review", re.I))
# # #                 or soup.find_all("article")
# # #             )

# # #             seen = set()
# # #             for i, card in enumerate(cards):
# # #                 text = card.get_text(separator=" ", strip=True)
# # #                 if len(text) < 30:
# # #                     continue

# # #                 key = text[:80].lower()
# # #                 if key in seen:
# # #                     continue
# # #                 seen.add(key)

# # #                 if any(bp in text.lower() for bp in [
# # #                     "careerbliss", "share salary", "update your browser",
# # #                     "search by job title", "browse salaries",
# # #                 ]):
# # #                     continue

# # #                 # Extract review text
# # #                 review_text = ""
# # #                 for s in card.stripped_strings:
# # #                     if s.startswith(dq) and s.endswith(dq) and len(s) > 30:
# # #                         review_text = s.strip(dq)
# # #                         break

# # #                 if not review_text:
# # #                     for el in card.find_all(["p", "span", "div"]):
# # #                         t = el.get_text(separator=" ", strip=True)
# # #                         if len(t) > len(review_text) and len(t) > 30:
# # #                             if not re.match(r"^(Person|People|Work|Support|Rewards|Growth)", t):
# # #                                 review_text = t

# # #                 if not review_text or len(review_text) < 20:
# # #                     review_text = text

# # #                 # Extract rating
# # #                 rating = 3.0
# # #                 rating_el = card.find(attrs={"aria-label": re.compile(r"\d.*star", re.I)})
# # #                 if rating_el:
# # #                     m = re.search(r"(\d+\.?\d*)", rating_el.get("aria-label", ""))
# # #                     if m:
# # #                         rating = float(m.group(1))
# # #                 else:
# # #                     rating_match = re.search(r"(\d+\.?\d*)\s*(?:/|out of)\s*5", text)
# # #                     if rating_match:
# # #                         rating = float(rating_match.group(1))
# # #                     else:
# # #                         num_match = re.search(r"(?:rating|score)[:\s]*(\d+\.?\d*)", text, re.I)
# # #                         if num_match:
# # #                             val = float(num_match.group(1))
# # #                             rating = val if val <= 5 else val / 20.0

# # #                 # Extract job title
# # #                 job_title = ""
# # #                 job_el = card.find(class_=re.compile(r"job.?title|position|role", re.I))
# # #                 if job_el:
# # #                     job_title = job_el.get_text(strip=True)
# # #                 else:
# # #                     for tag in card.find_all(["strong", "b", "span"]):
# # #                         t = tag.get_text(strip=True)
# # #                         if 3 < len(t) < 60 and "review" not in t.lower():
# # #                             job_title = t
# # #                             break

# # #                 # Extract date
# # #                 review_date = None
# # #                 date_el = card.find("time") or card.find(class_=re.compile(r"date", re.I))
# # #                 if date_el:
# # #                     raw_d = date_el.get("datetime") or date_el.get("content") or date_el.get_text(strip=True)
# # #                     review_date = _normalize_date(raw_d)

# # #                 reviews.append(CultureReview(
# # #                     review_id=f"careerbliss_{ticker}_{i}",
# # #                     rating=min(5.0, max(1.0, rating)),
# # #                     title=review_text[:100], pros=review_text[:2000], cons="",
# # #                     is_current_employee=True, job_title=job_title[:100],
# # #                     review_date=review_date, source="careerbliss",
# # #                 ))

# # #             logger.info(f"[{ticker}][careerbliss] Extracted {len(reviews)} reviews")

# # #         except Exception as e:
# # #             logger.warning(f"[{ticker}][careerbliss] Error: {e}")
# # #             try:
# # #                 page.close()
# # #             except Exception:
# # #                 pass

# # #         return reviews[:limit]

# # #     # ── CACHING ─────────────────────────────────────────────

# # #     def _cache_path(self, ticker, source):
# # #         return self.cache_dir / f"{ticker.upper()}_{source}.json"

# # #     def _save_cache(self, ticker, source, reviews):
# # #         p = self._cache_path(ticker, source)
# # #         try:
# # #             data = []
# # #             for r in reviews:
# # #                 data.append({
# # #                     "review_id": r.review_id, "rating": r.rating,
# # #                     "title": r.title, "pros": r.pros, "cons": r.cons,
# # #                     "advice_to_management": r.advice_to_management,
# # #                     "is_current_employee": r.is_current_employee,
# # #                     "job_title": r.job_title,
# # #                     "review_date": r.review_date.isoformat() if r.review_date else None,
# # #                     "source": r.source,
# # #                 })
# # #             p.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")
# # #             logger.info(f"[{ticker}][{source}] Cached {len(reviews)} reviews -> {p}")
# # #         except Exception as e:
# # #             logger.warning(f"[{ticker}][{source}] Cache save failed: {e}")

# # #     def _load_cache(self, ticker, source):
# # #         p = self._cache_path(ticker, source)
# # #         if not p.exists():
# # #             return None
# # #         try:
# # #             data = json.loads(p.read_text(encoding="utf-8"))
# # #             reviews = []
# # #             for d in data:
# # #                 rd = None
# # #                 if d.get("review_date"):
# # #                     try:
# # #                         rd = datetime.fromisoformat(d["review_date"])
# # #                     except (ValueError, TypeError):
# # #                         rd = None
# # #                 reviews.append(CultureReview(
# # #                     review_id=d["review_id"], rating=d["rating"],
# # #                     title=d["title"], pros=d["pros"], cons=d["cons"],
# # #                     advice_to_management=d.get("advice_to_management"),
# # #                     is_current_employee=d.get("is_current_employee", True),
# # #                     job_title=d.get("job_title", ""),
# # #                     review_date=rd, source=d.get("source", source),
# # #                 ))
# # #             logger.info(f"[{ticker}][{source}] Loaded {len(reviews)} from cache")
# # #             return reviews
# # #         except Exception as e:
# # #             logger.warning(f"[{ticker}][{source}] Cache load failed: {e}")
# # #             return None

# # #     # ── MULTI-SOURCE FETCH ──────────────────────────────────

# # #     # Review count targets
# # #     MIN_REVIEWS = 100
# # #     MAX_REVIEWS = 200

# # #     def fetch_all_reviews(self, ticker, sources, max_pages=3, use_cache=True):
# # #         ticker = ticker.upper()
# # #         all_reviews = []
# # #         num_sources = len(sources)

# # #         # Distribute target evenly across sources, with headroom
# # #         per_source_target = self.MAX_REVIEWS // max(num_sources, 1)

# # #         for source in sources:
# # #             if use_cache:
# # #                 cached = self._load_cache(ticker, source)
# # #                 if cached is not None:
# # #                     all_reviews.extend(cached[:per_source_target])
# # #                     continue
# # #             revs = []
# # #             try:
# # #                 if source == "glassdoor":
# # #                     # ~10 per page, so for 67 reviews need ~7 pages
# # #                     pages_needed = max(max_pages, (per_source_target // 10) + 1)
# # #                     revs = self.fetch_glassdoor(ticker, max_pages=pages_needed)
# # #                 elif source == "indeed":
# # #                     revs = self.scrape_indeed(ticker, limit=per_source_target)
# # #                 elif source == "careerbliss":
# # #                     revs = self.scrape_careerbliss(ticker, limit=per_source_target)
# # #                 else:
# # #                     logger.warning(f"[{ticker}] Unknown source: {source}")
# # #                     continue
# # #             except Exception as e:
# # #                 logger.error(f"[{ticker}][{source}] FAILED: {e}")
# # #             if revs:
# # #                 self._save_cache(ticker, source, revs)
# # #             all_reviews.extend(revs[:per_source_target])

# # #         total = len(all_reviews)

# # #         # Cap at MAX_REVIEWS
# # #         if total > self.MAX_REVIEWS:
# # #             logger.info(f"[{ticker}] Capping reviews from {total} to {self.MAX_REVIEWS}")
# # #             all_reviews = all_reviews[:self.MAX_REVIEWS]

# # #         # Warn if below MIN_REVIEWS
# # #         if total < self.MIN_REVIEWS:
# # #             logger.warning(
# # #                 f"[{ticker}] Only {total} reviews collected (minimum target: {self.MIN_REVIEWS}). "
# # #                 f"Score confidence may be lower."
# # #             )

# # #         logger.info(
# # #             f"[{ticker}] Total reviews collected: {len(all_reviews)} "
# # #             f"(target: {self.MIN_REVIEWS}-{self.MAX_REVIEWS})"
# # #         )
# # #         return all_reviews

# # #     # ── SCORING ─────────────────────────────────────────────

# # #     def analyze_reviews(self, company_id, ticker, reviews):
# # #         if not reviews:
# # #             logger.warning(f"[{ticker}] No reviews to analyze")
# # #             return CultureSignal(company_id=company_id, ticker=ticker)

# # #         inn_pos = inn_neg = Decimal("0")
# # #         dd = ai_m = Decimal("0")
# # #         ch_pos = ch_neg = Decimal("0")
# # #         total_w = Decimal("0")
# # #         rating_sum = 0.0
# # #         current_count = 0
# # #         pos_kw = []
# # #         neg_kw = []
# # #         src_counts = {}
# # #         now = datetime.now(timezone.utc)

# # #         for idx, r in enumerate(reviews):
# # #             text = f"{r.pros} {r.cons}".lower()
# # #             if r.advice_to_management:
# # #                 text += f" {r.advice_to_management}".lower()

# # #             days_old = (now - r.review_date).days if r.review_date else -1
# # #             rec_w = Decimal("1.0") if days_old < 730 else Decimal("0.5")
# # #             emp_w = Decimal("1.2") if r.is_current_employee else Decimal("1.0")
# # #             src_w = self.SOURCE_RELIABILITY.get(r.source, Decimal("0.70"))
# # #             w = rec_w * emp_w * src_w
# # #             total_w += w
# # #             rating_sum += r.rating
# # #             if r.is_current_employee:
# # #                 current_count += 1
# # #             src_counts[r.source] = src_counts.get(r.source, 0) + 1

# # #             review_hits = []
# # #             for kw in self.INNOVATION_POSITIVE:
# # #                 if kw in text:
# # #                     inn_pos += w
# # #                     review_hits.append(f"+innov:{kw}")
# # #                     if kw not in pos_kw:
# # #                         pos_kw.append(kw)
# # #             for kw in self.INNOVATION_NEGATIVE:
# # #                 if kw in text:
# # #                     inn_neg += w
# # #                     review_hits.append(f"-innov:{kw}")
# # #                     if kw not in neg_kw:
# # #                         neg_kw.append(kw)
# # #             for kw in self.DATA_DRIVEN_KEYWORDS:
# # #                 if kw in text:
# # #                     dd += w
# # #                     review_hits.append(f"+data:{kw}")
# # #             for kw in self.AI_AWARENESS_KEYWORDS:
# # #                 if kw in text:
# # #                     ai_m += w
# # #                     review_hits.append(f"+ai:{kw}")
# # #             for kw in self.CHANGE_POSITIVE:
# # #                 if kw in text:
# # #                     ch_pos += w
# # #                     review_hits.append(f"+change:{kw}")
# # #                     if kw not in pos_kw:
# # #                         pos_kw.append(kw)
# # #             for kw in self.CHANGE_NEGATIVE:
# # #                 if kw in text:
# # #                     ch_neg += w
# # #                     review_hits.append(f"-change:{kw}")
# # #                     if kw not in neg_kw:
# # #                         neg_kw.append(kw)

# # #             current_tag = "current" if r.is_current_employee else "former"
# # #             hits_str = ", ".join(review_hits) if review_hits else "(no keyword hits)"
# # #             logger.debug(
# # #                 f"[{ticker}] Review #{idx+1} [{r.source}] "
# # #                 f"rating={r.rating} {current_tag} "
# # #                 f"days_old={days_old} "
# # #                 f"rec_w={rec_w} emp_w={emp_w} src_w={src_w} w={w} | "
# # #                 f"{hits_str}"
# # #             )

# # #         if total_w > 0:
# # #             inn_s = (inn_pos - inn_neg) / total_w * 50 + 50
# # #             dd_s = dd / total_w * 100
# # #             ai_s = ai_m / total_w * 100
# # #             ch_s = (ch_pos - ch_neg) / total_w * 50 + 50
# # #         else:
# # #             inn_s = Decimal("50")
# # #             dd_s = Decimal("0")
# # #             ai_s = Decimal("0")
# # #             ch_s = Decimal("50")

# # #         c = lambda v: max(Decimal("0"), min(Decimal("100"), v))
# # #         inn_s, dd_s, ai_s, ch_s = c(inn_s), c(dd_s), c(ai_s), c(ch_s)

# # #         overall = (
# # #             Decimal("0.30") * inn_s + Decimal("0.25") * dd_s
# # #             + Decimal("0.25") * ai_s + Decimal("0.20") * ch_s
# # #         ).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)

# # #         conf = min(Decimal("0.5") + Decimal(str(len(reviews))) / 200, Decimal("0.90"))
# # #         source_bonus = min(Decimal(str(len(src_counts))) * Decimal("0.03"), Decimal("0.10"))
# # #         conf = min(conf + source_bonus, Decimal("0.95"))

# # #         avg_rating = Decimal(str(round(rating_sum / len(reviews), 2)))
# # #         current_ratio = Decimal(str(round(current_count / len(reviews), 3)))

# # #         logger.info(f"[{ticker}] {'=' * 44}")
# # #         logger.info(f"[{ticker}]   SCORING SUMMARY")
# # #         logger.info(f"[{ticker}] {'=' * 44}")
# # #         logger.info(f"[{ticker}]   Reviews analyzed:       {len(reviews)}")
# # #         logger.info(f"[{ticker}]   Sources:                {src_counts}")
# # #         logger.info(f"[{ticker}]   Total weight:           {total_w}")
# # #         logger.info(f"[{ticker}]   Avg rating (raw):       {avg_rating}/5.0")
# # #         logger.info(f"[{ticker}]   Current employees:      {current_count}/{len(reviews)} ({current_ratio})")
# # #         logger.info(f"[{ticker}]   -- Raw accumulators --")
# # #         logger.info(f"[{ticker}]     innov_pos={inn_pos}  innov_neg={inn_neg}  net={inn_pos - inn_neg}")
# # #         logger.info(f"[{ticker}]     data_mentions={dd}")
# # #         logger.info(f"[{ticker}]     ai_mentions={ai_m}")
# # #         logger.info(f"[{ticker}]     change_pos={ch_pos}  change_neg={ch_neg}  net={ch_pos - ch_neg}")
# # #         logger.info(f"[{ticker}]   -- Component scores --")
# # #         logger.info(f"[{ticker}]     Innovation:       {inn_s.quantize(Decimal('0.01'))}  (x0.30 -> {(Decimal('0.30') * inn_s).quantize(Decimal('0.01'))})")
# # #         logger.info(f"[{ticker}]     Data-Driven:      {dd_s.quantize(Decimal('0.01'))}  (x0.25 -> {(Decimal('0.25') * dd_s).quantize(Decimal('0.01'))})")
# # #         logger.info(f"[{ticker}]     AI Awareness:     {ai_s.quantize(Decimal('0.01'))}  (x0.25 -> {(Decimal('0.25') * ai_s).quantize(Decimal('0.01'))})")
# # #         logger.info(f"[{ticker}]     Change Readiness: {ch_s.quantize(Decimal('0.01'))}  (x0.20 -> {(Decimal('0.20') * ch_s).quantize(Decimal('0.01'))})")
# # #         logger.info(f"[{ticker}]   -- Final --")
# # #         logger.info(f"[{ticker}]     OVERALL SCORE:    {overall}/100")
# # #         logger.info(f"[{ticker}]     Confidence:       {conf.quantize(Decimal('0.001'))} (base + {source_bonus} source bonus)")
# # #         if pos_kw:
# # #             logger.info(f"[{ticker}]     (+) Keywords: {', '.join(pos_kw)}")
# # #         if neg_kw:
# # #             logger.info(f"[{ticker}]     (-) Keywords: {', '.join(neg_kw)}")
# # #         logger.info(f"[{ticker}] {'=' * 44}")

# # #         return CultureSignal(
# # #             company_id=company_id, ticker=ticker,
# # #             innovation_score=inn_s.quantize(Decimal("0.01")),
# # #             data_driven_score=dd_s.quantize(Decimal("0.01")),
# # #             change_readiness_score=ch_s.quantize(Decimal("0.01")),
# # #             ai_awareness_score=ai_s.quantize(Decimal("0.01")),
# # #             overall_score=overall, review_count=len(reviews),
# # #             avg_rating=avg_rating, current_employee_ratio=current_ratio,
# # #             confidence=conf.quantize(Decimal("0.001")),
# # #             source_breakdown=src_counts,
# # #             positive_keywords_found=pos_kw, negative_keywords_found=neg_kw,
# # #         )

# # #     # ── S3 UPLOAD ─────────────────────────────────────────────

# # #     def _get_s3_service(self):
# # #         """Initialize S3 client directly using .env credentials."""
# # #         if not hasattr(self, '_s3_client'):
# # #             try:
# # #                 import boto3
# # #                 bucket = os.getenv("S3_BUCKET", "")
# # #                 key_id = os.getenv("AWS_ACCESS_KEY_ID", "")
# # #                 secret = os.getenv("AWS_SECRET_ACCESS_KEY", "")
# # #                 region = os.getenv("AWS_REGION", "us-east-1")

# # #                 if not bucket or not key_id or not secret:
# # #                     logger.warning(
# # #                         "S3 not configured. Set S3_BUCKET, AWS_ACCESS_KEY_ID, "
# # #                         "AWS_SECRET_ACCESS_KEY in .env"
# # #                     )
# # #                     self._s3_client = None
# # #                     self._s3_bucket = None
# # #                     return None

# # #                 self._s3_client = boto3.client(
# # #                     's3',
# # #                     aws_access_key_id=key_id,
# # #                     aws_secret_access_key=secret,
# # #                     region_name=region,
# # #                 )
# # #                 self._s3_bucket = bucket
# # #                 logger.info(f"S3 initialized: bucket={bucket}, region={region}")
# # #             except Exception as e:
# # #                 logger.error(f"S3 initialization failed: {e}")
# # #                 self._s3_client = None
# # #                 self._s3_bucket = None
# # #         return self._s3_client

# # #     def _upload_raw_to_s3(self, ticker, reviews):
# # #         """Upload raw reviews to S3: glassdoor_signals/raw/{TICKER}_raw.json"""
# # #         client = self._get_s3_service()
# # #         if not client:
# # #             return None
# # #         ticker = ticker.upper()
# # #         raw_data = []
# # #         for r in reviews:
# # #             raw_data.append({
# # #                 "review_id": r.review_id, "rating": r.rating,
# # #                 "title": r.title, "pros": r.pros, "cons": r.cons,
# # #                 "advice_to_management": r.advice_to_management,
# # #                 "is_current_employee": r.is_current_employee,
# # #                 "job_title": r.job_title,
# # #                 "review_date": r.review_date.isoformat() if r.review_date else None,
# # #                 "source": r.source,
# # #             })
# # #         s3_key = f"glassdoor_signals/raw/{ticker}_raw.json"
# # #         payload = json.dumps(
# # #             {"ticker": ticker, "review_count": len(raw_data), "reviews": raw_data},
# # #             indent=2, default=str,
# # #         )
# # #         try:
# # #             client.put_object(
# # #                 Bucket=self._s3_bucket, Key=s3_key,
# # #                 Body=payload.encode("utf-8"),
# # #                 ContentType="application/json",
# # #             )
# # #             logger.info(f"[{ticker}] Uploaded {len(raw_data)} raw reviews to S3: {s3_key}")
# # #             return s3_key
# # #         except Exception as e:
# # #             logger.error(f"[{ticker}] S3 raw upload failed: {e}")
# # #             return None

# # #     def _upload_output_to_s3(self, signal):
# # #         """Upload scored output to S3: glassdoor_signals/output/{TICKER}_culture.json"""
# # #         client = self._get_s3_service()
# # #         if not client:
# # #             return None
# # #         ticker = signal.ticker.upper()
# # #         output_data = asdict(signal)
# # #         for k, v in output_data.items():
# # #             if isinstance(v, Decimal):
# # #                 output_data[k] = float(v)
# # #         s3_key = f"glassdoor_signals/output/{ticker}_culture.json"
# # #         payload = json.dumps(output_data, indent=2, default=str)
# # #         try:
# # #             client.put_object(
# # #                 Bucket=self._s3_bucket, Key=s3_key,
# # #                 Body=payload.encode("utf-8"),
# # #                 ContentType="application/json",
# # #             )
# # #             logger.info(f"[{ticker}] Uploaded culture signal to S3: {s3_key}")
# # #             return s3_key
# # #         except Exception as e:
# # #             logger.error(f"[{ticker}] S3 output upload failed: {e}")
# # #             return None

# # #     # ── MAIN ENTRY POINTS ──────────────────────────────────

# # #     def collect_and_analyze(self, ticker, sources=None, max_pages=3, use_cache=True):
# # #         ticker = validate_ticker(ticker)
# # #         if sources is None:
# # #             sources = ["glassdoor", "indeed", "careerbliss"]
# # #         sources = [s for s in sources if s in VALID_SOURCES]
# # #         reg = COMPANY_REGISTRY[ticker]
# # #         logger.info(f"{'=' * 55}")
# # #         logger.info(f"CULTURE COLLECTION: {ticker} ({reg['name']})")
# # #         logger.info(f"   Sector: {reg['sector']}")
# # #         logger.info(f"   Sources: {', '.join(sources)}")
# # #         logger.info(f"{'=' * 55}")

# # #         reviews = self.fetch_all_reviews(ticker, sources=sources, max_pages=max_pages, use_cache=use_cache)
# # #         signal = self.analyze_reviews(ticker, ticker, reviews)

# # #         # Upload to S3 only
# # #         self._upload_raw_to_s3(ticker, reviews)
# # #         self._upload_output_to_s3(signal)

# # #         return signal

# # #     def collect_multiple(self, tickers, sources=None, max_pages=3, use_cache=True, delay=2.0):
# # #         results = {}
# # #         try:
# # #             for i, ticker in enumerate(tickers):
# # #                 try:
# # #                     signal = self.collect_and_analyze(ticker, sources=sources, max_pages=max_pages, use_cache=use_cache)
# # #                     results[ticker.upper()] = signal
# # #                 except Exception as e:
# # #                     logger.error(f"[{ticker}] FAILED: {e}")
# # #                 if i < len(tickers) - 1:
# # #                     logger.info(f"Waiting {delay}s before next ticker...")
# # #                     time.sleep(delay)
# # #         finally:
# # #             self.close_browser()
# # #         return results

# # #     def _save_signal(self, signal):
# # #         d = Path("results")
# # #         d.mkdir(parents=True, exist_ok=True)
# # #         p = d / f"{signal.ticker.lower()}_culture.json"
# # #         p.write_text(signal.to_json(), encoding="utf-8")
# # #         logger.info(f"[{signal.ticker}] Saved -> {p}")


# # # # =====================================================================
# # # # DISPLAY
# # # # =====================================================================

# # # def print_signal(signal):
# # #     reg = COMPANY_REGISTRY.get(signal.ticker, {})
# # #     name = reg.get("name", signal.ticker)
# # #     sector = reg.get("sector", "")
# # #     print(f"\n{'=' * 60}")
# # #     print(f"  CULTURE ANALYSIS -- {signal.ticker} ({name})")
# # #     if sector:
# # #         print(f"  Sector: {sector}")
# # #     print(f"{'=' * 60}")
# # #     print(f"  Overall Score:          {signal.overall_score}/100")
# # #     print(f"  Confidence:             {signal.confidence}")
# # #     print(f"  Reviews Analyzed:       {signal.review_count}")
# # #     print(f"  Source Breakdown:       {signal.source_breakdown}")
# # #     print(f"  Avg Rating:             {signal.avg_rating}/5.0")
# # #     print(f"  Current Employee Ratio: {signal.current_employee_ratio}")
# # #     print()
# # #     print(f"  Component Scores:       Weight   Score")
# # #     print(f"    Innovation:           0.30   {signal.innovation_score:>8}")
# # #     print(f"    Data-Driven:          0.25   {signal.data_driven_score:>8}")
# # #     print(f"    AI Awareness:         0.25   {signal.ai_awareness_score:>8}")
# # #     print(f"    Change Readiness:     0.20   {signal.change_readiness_score:>8}")
# # #     if signal.positive_keywords_found:
# # #         print(f"\n  (+) Keywords: {', '.join(signal.positive_keywords_found[:10])}")
# # #     if signal.negative_keywords_found:
# # #         print(f"  (-) Keywords: {', '.join(signal.negative_keywords_found[:10])}")


# # # # =====================================================================
# # # # MAIN
# # # # =====================================================================

# # # def main():
# # #     args = sys.argv[1:]
# # #     use_cache = "--no-cache" not in args

# # #     sources = None
# # #     for a in args:
# # #         if a.startswith("--sources="):
# # #             sources = [s.strip() for s in a.split("=", 1)[1].split(",")]

# # #     clean_args = [a for a in args if not a.startswith("-")]

# # #     if "--all" in args:
# # #         tickers = all_tickers()
# # #         glassdoor_active = sources is None or "glassdoor" in sources
# # #         est = len(tickers) * 3 if glassdoor_active else 0
# # #         print(f"\n  Running ALL {len(tickers)} tickers.")
# # #         if glassdoor_active:
# # #             print(f"   Estimated Glassdoor API calls: ~{est} (free tier = 500/month)")
# # #         print(f"   Tickers: {', '.join(tickers)}\n")
# # #         confirm = input("   Continue? [y/N]: ").strip().lower()
# # #         if confirm != "y":
# # #             print("Aborted.")
# # #             return
# # #     elif clean_args:
# # #         tickers = []
# # #         for t in clean_args:
# # #             try:
# # #                 tickers.append(validate_ticker(t))
# # #             except ValueError as e:
# # #                 print(f"ERROR: {e}")
# # #                 return
# # #     else:
# # #         print("\n" + "=" * 58)
# # #         print("  Multi-Source Culture Collector (CS3)")
# # #         print("=" * 58)
# # #         print()
# # #         print("  Usage:")
# # #         print("    python -m pipelines.culture_collector <TICKER> [TICKER ...]")
# # #         print("    python -m pipelines.culture_collector --all")
# # #         print()
# # #         print("  Options:")
# # #         print("    --no-cache                       Skip cached reviews")
# # #         print("    --sources=glassdoor,indeed,careerbliss  Pick sources")
# # #         print()
# # #         print("  Source Reliability:")
# # #         print("    Glassdoor    0.85  (via RapidAPI)")
# # #         print("    Indeed       0.80  (via Playwright)")
# # #         print("    CareerBliss  0.75  (via Playwright)")
# # #         print()
# # #         print("  Allowed tickers (13):")
# # #         for t in sorted(ALLOWED_TICKERS):
# # #             reg = COMPANY_REGISTRY[t]
# # #             print(f"    {t:<6} {reg['name']:<30} {reg['sector']}")
# # #         print()
# # #         print("  RapidAPI free tier = 500 requests/month.")
# # #         print("  Each ticker uses ~3 Glassdoor API calls.")
# # #         print("=" * 58)
# # #         return

# # #     glassdoor_active = sources is None or "glassdoor" in sources
# # #     if glassdoor_active:
# # #         est = len(tickers) * 3
# # #         print(f"\n  Glassdoor API calls: ~{est} (3 pages x {len(tickers)} tickers)")
# # #     print(f"  Tickers: {', '.join(tickers)}")
# # #     if sources:
# # #         print(f"  Sources: {', '.join(sources)}")
# # #     print()

# # #     collector = CultureCollector()
# # #     results = collector.collect_multiple(tickers, sources=sources, use_cache=use_cache, delay=2.0)

# # #     print("\n\n" + "#" * 60)
# # #     print(f"#  MULTI-SOURCE CULTURE ANALYSIS -- {len(results)} companies")
# # #     print("#" * 60)

# # #     for ticker, signal in results.items():
# # #         print_signal(signal)

# # #     if len(results) > 1:
# # #         print(f"\n{'=' * 62}")
# # #         print(
# # #             f"  {'Ticker':<6} {'Overall':>8} {'Innov':>7} "
# # #             f"{'Data':>7} {'AI':>7} {'Change':>7} {'#Rev':>5} {'Conf':>6}"
# # #         )
# # #         print(f"  {'-'*6} {'-'*8} {'-'*7} {'-'*7} {'-'*7} {'-'*7} {'-'*5} {'-'*6}")
# # #         for t, s in sorted(results.items(), key=lambda x: x[1].overall_score, reverse=True):
# # #             print(
# # #                 f"  {t:<6} {s.overall_score:>8} {s.innovation_score:>7} "
# # #                 f"{s.data_driven_score:>7} {s.ai_awareness_score:>7} "
# # #                 f"{s.change_readiness_score:>7} {s.review_count:>5} {s.confidence:>6}"
# # #             )
# # #         print(f"{'=' * 62}")


# # # if __name__ == "__main__":
# # #     main()

# # """
# # Culture Collector — Task 5.0c (CS3)
# # app/pipelines/glassdoor_collector.py

# # 3-source culture signal collector:
# #   1. Glassdoor   — via RapidAPI (real-time-glassdoor-data)
# #   2. Indeed      — via Playwright + BeautifulSoup
# #   3. CareerBliss — via Playwright + BeautifulSoup

# # Source reliability: Glassdoor 0.85, Indeed 0.80, CareerBliss 0.75
# # Scoring uses Decimal precision throughout.

# # Requirements:
# #     pip install playwright httpx beautifulsoup4 python-dotenv
# #     playwright install chromium

# # Usage (ticker REQUIRED to protect API quota):
# #     python -m pipelines.culture_collector NVDA
# #     python -m pipelines.culture_collector NVDA JPM WMT
# #     python -m pipelines.culture_collector --all
# #     python -m pipelines.culture_collector NVDA --no-cache
# #     python -m pipelines.culture_collector NVDA --sources=glassdoor,indeed,careerbliss

# # Tickers (5):
# #     NVDA, JPM, WMT, GE, DG
# # """

# # import json
# # import logging
# # import os
# # import re
# # import sys
# # import time
# # from dataclasses import dataclass, field, asdict
# # from datetime import datetime, timezone, timedelta
# # from decimal import Decimal, ROUND_HALF_UP
# # from pathlib import Path
# # from typing import Any, Dict, List, Optional

# # import httpx
# # from dotenv import load_dotenv

# # _THIS_FILE = Path(__file__).resolve()
# # _APP_DIR = _THIS_FILE.parent.parent
# # _PROJECT_ROOT = _APP_DIR.parent
# # for _p in [str(_PROJECT_ROOT), str(_APP_DIR)]:
# #     if _p not in sys.path:
# #         sys.path.insert(0, _p)

# # load_dotenv(_PROJECT_ROOT / ".env")

# # logging.basicConfig(level=logging.DEBUG, format="%(levelname)s | %(message)s")
# # logger = logging.getLogger(__name__)

# # logging.getLogger("httpx").setLevel(logging.WARNING)
# # logging.getLogger("httpcore").setLevel(logging.WARNING)
# # logging.getLogger("boto3").setLevel(logging.WARNING)
# # logging.getLogger("botocore").setLevel(logging.WARNING)
# # logging.getLogger("urllib3").setLevel(logging.WARNING)
# # logging.getLogger("s3transfer").setLevel(logging.WARNING)


# # # =====================================================================
# # # DATA MODELS
# # # =====================================================================

# # @dataclass
# # class CultureReview:
# #     review_id: str
# #     rating: float
# #     title: str
# #     pros: str
# #     cons: str
# #     advice_to_management: Optional[str] = None
# #     is_current_employee: bool = True
# #     job_title: str = ""
# #     review_date: Optional[datetime] = None
# #     source: str = "unknown"

# #     def __post_init__(self):
# #         if self.review_date is None:
# #             self.review_date = datetime.now(timezone.utc)


# # @dataclass
# # class CultureSignal:
# #     company_id: str
# #     ticker: str
# #     innovation_score: Decimal = Decimal("50.00")
# #     data_driven_score: Decimal = Decimal("0.00")
# #     change_readiness_score: Decimal = Decimal("50.00")
# #     ai_awareness_score: Decimal = Decimal("0.00")
# #     overall_score: Decimal = Decimal("25.00")
# #     review_count: int = 0
# #     avg_rating: Decimal = Decimal("0.00")
# #     current_employee_ratio: Decimal = Decimal("0.000")
# #     confidence: Decimal = Decimal("0.000")
# #     source_breakdown: Dict[str, int] = field(default_factory=dict)
# #     positive_keywords_found: List[str] = field(default_factory=list)
# #     negative_keywords_found: List[str] = field(default_factory=list)

# #     def to_json(self, indent=2):
# #         d = asdict(self)
# #         for k, v in d.items():
# #             if isinstance(v, Decimal):
# #                 d[k] = float(v)
# #         return json.dumps(d, indent=indent, default=str)


# # # =====================================================================
# # # COMPANY REGISTRY — 13 tickers
# # # =====================================================================

# # COMPANY_REGISTRY = {
# #     "NVDA": {
# #         "name": "NVIDIA", "sector": "Technology",
# #         "glassdoor_id": "NVIDIA",
# #         "indeed_slugs": ["NVIDIA"],
# #         "careerbliss_slug": "nvidia",
# #     },
# #     "JPM": {
# #         "name": "JPMorgan Chase", "sector": "Financial Services",
# #         "glassdoor_id": "JPMorgan-Chase",
# #         "indeed_slugs": ["JPMorgan-Chase", "jpmorgan-chase"],
# #         "careerbliss_slug": "jpmorgan-chase",
# #     },
# #     "WMT": {
# #         "name": "Walmart", "sector": "Consumer Retail",
# #         "glassdoor_id": "Walmart",
# #         "indeed_slugs": ["Walmart"],
# #         "careerbliss_slug": "walmart",
# #     },
# #     "GE": {
# #         "name": "GE Aerospace", "sector": "Industrials Manufacturing",
# #         "glassdoor_id": "GE-Aerospace",
# #         "indeed_slugs": ["GE-Aerospace", "General-Electric"],
# #         "careerbliss_slug": "ge-aerospace",
# #     },
# #     "DG": {
# #         "name": "Dollar General", "sector": "Consumer Retail",
# #         "glassdoor_id": "Dollar-General",
# #         "indeed_slugs": ["Dollar-General"],
# #         "careerbliss_slug": "dollar-general",
# #     },
# # }

# # ALLOWED_TICKERS = set(COMPANY_REGISTRY.keys())
# # VALID_SOURCES = {"glassdoor", "indeed", "careerbliss"}


# # def validate_ticker(ticker):
# #     t = ticker.upper()
# #     if t not in ALLOWED_TICKERS:
# #         raise ValueError(
# #             f"Unknown ticker '{t}'. Allowed: {', '.join(sorted(ALLOWED_TICKERS))}"
# #         )
# #     return t


# # def all_tickers():
# #     return sorted(ALLOWED_TICKERS)


# # # =====================================================================
# # # HELPERS
# # # =====================================================================

# # def _normalize_date(raw):
# #     if not raw:
# #         return None
# #     raw = raw.strip()
# #     iso = re.match(r"(\d{4}-\d{2}-\d{2})", raw)
# #     if iso:
# #         try:
# #             return datetime.strptime(iso.group(1), "%Y-%m-%d").replace(tzinfo=timezone.utc)
# #         except ValueError:
# #             pass
# #     for fmt in ("%B %d, %Y", "%b %d, %Y", "%d %B %Y", "%d %b %Y",
# #                 "%m/%d/%Y", "%d/%m/%Y", "%Y/%m/%d"):
# #         try:
# #             return datetime.strptime(raw, fmt).replace(tzinfo=timezone.utc)
# #         except ValueError:
# #             continue
# #     rel = re.match(r"(\d+)\s+(day|week|month|year)s?\s+ago", raw, re.I)
# #     if rel:
# #         num = int(rel.group(1))
# #         unit = rel.group(2).lower()
# #         days = {"day": 1, "week": 7, "month": 30, "year": 365}[unit]
# #         return datetime.now(timezone.utc) - timedelta(days=num * days)
# #     return None


# # # =====================================================================
# # # CULTURE COLLECTOR
# # # =====================================================================

# # class CultureCollector:

# #     # ── Keyword lists ───────────────────────────────────────

# #     INNOVATION_POSITIVE = [
# #         "innovative", "cutting-edge", "forward-thinking",
# #         "encourages new ideas", "experimental", "creative freedom",
# #         "startup mentality", "move fast", "disruptive",
# #         "innovation", "pioneering", "bleeding edge",
# #     ]
# #     INNOVATION_NEGATIVE = [
# #         "bureaucratic", "slow to change", "resistant",
# #         "outdated", "stuck in old ways", "red tape",
# #         "politics", "siloed", "hierarchical",
# #         "stagnant", "old-fashioned", "behind the times",
# #     ]
# #     DATA_DRIVEN_KEYWORDS = [
# #         "data-driven", "metrics", "evidence-based",
# #         "analytical", "kpis", "dashboards", "data culture",
# #         "measurement", "quantitative",
# #         "data informed", "analytics", "data-centric",
# #     ]
# #     AI_AWARENESS_KEYWORDS = [
# #         "ai", "artificial intelligence", "machine learning",
# #         "automation", "data science", "ml", "algorithms",
# #         "predictive", "neural network",
# #         "deep learning", "nlp", "llm", "generative ai",
# #         "chatbot", "computer vision",
# #     ]
# #     CHANGE_POSITIVE = [
# #         "agile", "adaptive", "fast-paced", "embraces change",
# #         "continuous improvement", "growth mindset",
# #         "evolving", "dynamic", "transforming",
# #     ]
# #     CHANGE_NEGATIVE = [
# #         "rigid", "traditional", "slow", "risk-averse",
# #         "change resistant", "old school",
# #         "inflexible", "set in their ways", "fear of change",
# #     ]

# #     # Keywords that need whole-word matching (short or common substrings)
# #     WHOLE_WORD_KEYWORDS = {
# #         # AI keywords - very short, appear inside many English words
# #         "ai", "ml", "nlp", "llm",
# #         # Change keywords - "slow" matches "slowly", "slow climb" etc.
# #         "slow",
# #         # Innovation keywords - "traditional" matches "non-traditional"
# #         "traditional",
# #         # Change keywords - "rigid" matches "frigid"
# #         "rigid",
# #         # "dynamic" is generic, ensure whole word
# #         "dynamic",
# #         # "agile" could appear in non-culture contexts
# #         "agile",
# #     }

# #     def _keyword_in_text(self, kw, text):
# #         """Check if keyword exists in text. Uses word boundary for short/ambiguous keywords."""
# #         if kw in self.WHOLE_WORD_KEYWORDS:
# #             return bool(re.search(r'\b' + re.escape(kw) + r'\b', text))
# #         return kw in text

# #     SOURCE_RELIABILITY = {
# #         "glassdoor":   Decimal("0.85"),
# #         "indeed":      Decimal("0.80"),
# #         "careerbliss": Decimal("0.75"),
# #         "unknown":     Decimal("0.70"),
# #     }

# #     RAPIDAPI_HOST = "real-time-glassdoor-data.p.rapidapi.com"
# #     RAPIDAPI_BASE = f"https://{RAPIDAPI_HOST}"

# #     def __init__(self, cache_dir="data/culture_cache"):
# #         self.cache_dir = Path(cache_dir)
# #         self.cache_dir.mkdir(parents=True, exist_ok=True)
# #         self._browser = None
# #         self._playwright = None

# #     # ── Browser management ──────────────────────────────────

# #     def _get_browser(self):
# #         if self._browser is None:
# #             from playwright.sync_api import sync_playwright
# #             self._playwright = sync_playwright().start()
# #             self._browser = self._playwright.chromium.launch(
# #                 headless=True,
# #                 args=[
# #                     "--disable-blink-features=AutomationControlled",
# #                     "--no-sandbox",
# #                     "--disable-dev-shm-usage",
# #                     "--disable-infobars",
# #                     "--window-size=1920,1080",
# #                 ],
# #             )
# #             logger.info("Playwright browser launched")
# #         return self._browser

# #     def _new_page(self, stealth=True):
# #         browser = self._get_browser()
# #         ctx = browser.new_context(
# #             user_agent=(
# #                 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
# #                 "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
# #             ),
# #             viewport={"width": 1920, "height": 1080},
# #             locale="en-US",
# #             timezone_id="America/New_York",
# #             extra_http_headers={
# #                 "Accept-Language": "en-US,en;q=0.9",
# #                 "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
# #                 "Sec-Fetch-Dest": "document",
# #                 "Sec-Fetch-Mode": "navigate",
# #                 "Sec-Fetch-Site": "none",
# #                 "Sec-Fetch-User": "?1",
# #                 "Upgrade-Insecure-Requests": "1",
# #             },
# #         )
# #         page = ctx.new_page()
# #         if stealth:
# #             page.add_init_script("""
# #                 Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
# #                 Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
# #                 Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
# #                 window.chrome = { runtime: {} };
# #                 const origQuery = window.navigator.permissions.query;
# #                 window.navigator.permissions.query = (p) =>
# #                     p.name === 'notifications'
# #                         ? Promise.resolve({ state: Notification.permission })
# #                         : origQuery(p);
# #             """)
# #         page.route(
# #             "**/*.{png,jpg,jpeg,gif,svg,webp,woff,woff2,ttf,mp4,webm}",
# #             lambda route: route.abort(),
# #         )
# #         return page

# #     def close_browser(self):
# #         if self._browser:
# #             self._browser.close()
# #             self._browser = None
# #         if self._playwright:
# #             self._playwright.stop()
# #             self._playwright = None
# #             logger.info("Playwright browser closed")

# #     # ── SOURCE 1: GLASSDOOR via RapidAPI ────────────────────

# #     def _get_api_key(self):
# #         key = os.getenv("RAPIDAPI_KEY", "")
# #         if not key:
# #             raise EnvironmentError(
# #                 "RAPIDAPI_KEY not set. Add it to your .env file.\n"
# #                 "Get a free key at: https://rapidapi.com/letscrape-6bRBa3QguO5/"
# #                 "api/real-time-glassdoor-data"
# #             )
# #         return key

# #     def _api_headers(self):
# #         return {
# #             "x-rapidapi-key": self._get_api_key(),
# #             "x-rapidapi-host": self.RAPIDAPI_HOST,
# #         }

# #     def fetch_glassdoor(self, ticker, max_pages=3, timeout=30.0):
# #         ticker = ticker.upper()
# #         reg = COMPANY_REGISTRY[ticker]
# #         company_id = reg["glassdoor_id"]
# #         reviews = []

# #         for page_num in range(1, max_pages + 1):
# #             params = {
# #                 "company_id": company_id,
# #                 "page": str(page_num),
# #                 "sort": "POPULAR",
# #                 "language": "en",
# #                 "only_current_employees": "false",
# #                 "extended_rating_data": "false",
# #                 "domain": "www.glassdoor.com",
# #             }
# #             url = f"{self.RAPIDAPI_BASE}/company-reviews"
# #             logger.info(f"[{ticker}][glassdoor] Fetching page {page_num}...")

# #             try:
# #                 resp = httpx.get(url, headers=self._api_headers(), params=params, timeout=timeout)
# #                 resp.raise_for_status()
# #                 raw_data = resp.json()
# #             except httpx.HTTPStatusError as e:
# #                 logger.error(f"[{ticker}][glassdoor] HTTP {e.response.status_code} on page {page_num}")
# #                 break
# #             except Exception as e:
# #                 logger.error(f"[{ticker}][glassdoor] Request failed: {e}")
# #                 break

# #             reviews_raw = raw_data.get("data", {}).get("reviews", [])
# #             total = raw_data.get("data", {}).get("review_count", "?")
# #             pages_total = raw_data.get("data", {}).get("page_count", "?")

# #             if not reviews_raw:
# #                 logger.info(f"[{ticker}][glassdoor] No more reviews at page {page_num}")
# #                 break

# #             for r in reviews_raw:
# #                 parsed = self._parse_glassdoor_review(ticker, r)
# #                 if parsed:
# #                     reviews.append(parsed)

# #             logger.info(
# #                 f"[{ticker}][glassdoor] Page {page_num}: {len(reviews_raw)} reviews "
# #                 f"(total available: {total}, pages: {pages_total})"
# #             )
# #             if page_num < max_pages:
# #                 time.sleep(0.5)

# #         logger.info(f"[{ticker}][glassdoor] Total fetched: {len(reviews)}")
# #         return reviews

# #     def _parse_glassdoor_review(self, ticker, raw):
# #         try:
# #             rid = f"glassdoor_{ticker}_{raw.get('review_id', 'unknown')}"
# #             rating = float(raw.get("rating", 3.0))
# #             title = raw.get("summary") or raw.get("title") or ""
# #             pros = raw.get("pros") or ""
# #             cons = raw.get("cons") or ""
# #             advice = raw.get("advice_to_management") or None
# #             job_title = raw.get("job_title") or ""
# #             is_current = bool(raw.get("is_current_employee", False))
# #             emp_status = raw.get("employment_status", "")
# #             if isinstance(emp_status, str) and emp_status.upper() == "REGULAR":
# #                 is_current = True
# #             review_date = None
# #             raw_date = raw.get("review_datetime") or None
# #             if raw_date and isinstance(raw_date, str):
# #                 review_date = _normalize_date(raw_date[:10])
# #             return CultureReview(
# #                 review_id=rid, rating=min(5.0, max(1.0, rating)),
# #                 title=title[:200], pros=pros[:2000], cons=cons[:2000],
# #                 advice_to_management=advice, is_current_employee=is_current,
# #                 job_title=job_title, review_date=review_date, source="glassdoor",
# #             )
# #         except Exception as e:
# #             logger.warning(f"[{ticker}][glassdoor] Parse error: {e}")
# #             return None

# #     # ── SOURCE 2: INDEED via Playwright + BeautifulSoup ─────

# #     def scrape_indeed(self, ticker, limit=50):
# #         from bs4 import BeautifulSoup
# #         ticker = ticker.upper()
# #         slugs = COMPANY_REGISTRY[ticker]["indeed_slugs"]
# #         reviews = []

# #         # Calculate pages needed (Indeed shows ~20 reviews per page)
# #         max_pages = max(1, (limit // 20) + 1)

# #         for slug in slugs:
# #             for page_num in range(max_pages):
# #                 start = page_num * 20
# #                 url = f"https://www.indeed.com/cmp/{slug}/reviews"
# #                 if page_num > 0:
# #                     url = f"{url}?start={start}"
# #                 logger.info(f"[{ticker}][indeed] Scraping page {page_num + 1}: {url}")
# #                 try:
# #                     page = self._new_page(stealth=True)
# #                     page.goto(url, wait_until="domcontentloaded", timeout=30000)
# #                     time.sleep(3)
# #                     if "blocked" in page.title().lower():
# #                         logger.warning(f"[{ticker}][indeed] Blocked: {page.title()}")
# #                         page.close()
# #                         break
# #                     try:
# #                         page.wait_for_selector(
# #                             '[data-testid*="review"], .cmp-ReviewsList, '
# #                             '[data-tn-component="reviewsList"]',
# #                             timeout=10000,
# #                         )
# #                     except Exception:
# #                         logger.warning(f"[{ticker}][indeed] Review container not found on page {page_num + 1}")
# #                         page.close()
# #                         break
# #                     for _ in range(3):
# #                         page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
# #                         time.sleep(1)
# #                     html = page.content()
# #                     page.close()

# #                     soup = BeautifulSoup(html, "html.parser")
# #                     cards = (
# #                         soup.find_all("div", {"data-testid": re.compile(r"review")})
# #                         or soup.find_all("div", class_=re.compile(r"cmp-Review(?!sList)"))
# #                         or soup.find_all("div", class_=re.compile(r"review", re.I))
# #                     )

# #                     if not cards:
# #                         logger.info(f"[{ticker}][indeed] No more reviews at page {page_num + 1}")
# #                         break

# #                     page_count = 0
# #                     for i, card in enumerate(cards):
# #                         text = card.get_text(separator=" ", strip=True)
# #                         if len(text) < 30:
# #                             continue
# #                         title_el = card.find(
# #                             ["h2", "h3", "a", "span"],
# #                             class_=re.compile(r"title|header", re.I),
# #                         )
# #                         title = title_el.get_text(strip=True) if title_el else text[:100]
# #                         rating = 3.0
# #                         star_el = card.find(attrs={"aria-label": re.compile(r"\d.*star", re.I)})
# #                         if star_el:
# #                             m = re.search(r"(\d+\.?\d*)", star_el.get("aria-label", ""))
# #                             if m:
# #                                 rating = float(m.group(1))
# #                         pros_text, cons_text = "", ""
# #                         for label in card.find_all(string=re.compile(r"^Pros?$", re.I)):
# #                             p = label.find_parent()
# #                             if p and p.find_next_sibling():
# #                                 pros_text = p.find_next_sibling().get_text(separator=" ", strip=True)
# #                         for label in card.find_all(string=re.compile(r"^Cons?$", re.I)):
# #                             p = label.find_parent()
# #                             if p and p.find_next_sibling():
# #                                 cons_text = p.find_next_sibling().get_text(separator=" ", strip=True)
# #                         if not pros_text and not cons_text:
# #                             pros_text = text
# #                         date_el = card.find("time") or card.find(class_=re.compile(r"date", re.I))
# #                         review_date = None
# #                         if date_el:
# #                             raw_d = date_el.get("datetime") or date_el.get("content") or date_el.get_text(strip=True)
# #                             review_date = _normalize_date(raw_d)
# #                         is_current = "current" in text.lower() and "former" not in text.lower()
# #                         global_idx = len(reviews)
# #                         reviews.append(CultureReview(
# #                             review_id=f"indeed_{ticker}_{global_idx}",
# #                             rating=min(5.0, max(1.0, rating)),
# #                             title=title[:200], pros=pros_text[:2000], cons=cons_text[:2000],
# #                             is_current_employee=is_current, review_date=review_date,
# #                             source="indeed",
# #                         ))
# #                         page_count += 1

# #                     logger.info(f"[{ticker}][indeed] Page {page_num + 1}: {page_count} reviews (total: {len(reviews)})")

# #                     if len(reviews) >= limit:
# #                         break

# #                     if page_num < max_pages - 1:
# #                         time.sleep(2)

# #                 except Exception as e:
# #                     logger.warning(f"[{ticker}][indeed] Error on page {page_num + 1} for slug '{slug}': {e}")
# #                     try:
# #                         page.close()
# #                     except Exception:
# #                         pass
# #                     break

# #             if reviews:
# #                 logger.info(f"[{ticker}][indeed] Extracted {len(reviews)} total reviews")
# #                 break
# #         return reviews[:limit]

# #     # ── SOURCE 3: CAREERBLISS via Playwright + BeautifulSoup ─

# #     def scrape_careerbliss(self, ticker, limit=50):
# #         from bs4 import BeautifulSoup
# #         ticker = ticker.upper()
# #         slug = COMPANY_REGISTRY[ticker]["careerbliss_slug"]
# #         reviews = []
# #         dq = chr(34)

# #         url = f"https://www.careerbliss.com/{slug}/reviews/"
# #         logger.info(f"[{ticker}][careerbliss] Scraping: {url}")

# #         try:
# #             page = self._new_page(stealth=True)
# #             page.goto(url, wait_until="domcontentloaded", timeout=30000)
# #             time.sleep(3)

# #             title_text = page.title().lower()
# #             if any(w in title_text for w in ["blocked", "denied", "captcha"]):
# #                 logger.warning(f"[{ticker}][careerbliss] Blocked: {page.title()}")
# #                 page.close()
# #                 return reviews

# #             for _ in range(5):
# #                 page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
# #                 time.sleep(1)

# #             for _ in range(3):
# #                 try:
# #                     more = page.query_selector(
# #                         'a:has-text("More Reviews"), a:has-text("Show More"), '
# #                         'button:has-text("More"), a.next'
# #                     )
# #                     if more and more.is_visible():
# #                         more.click()
# #                         time.sleep(2)
# #                     else:
# #                         break
# #                 except Exception:
# #                     break

# #             html = page.content()
# #             page.close()

# #             soup = BeautifulSoup(html, "html.parser")

# #             cards = (
# #                 soup.find_all("div", class_=re.compile(r"review", re.I))
# #                 or soup.find_all("li", class_=re.compile(r"review", re.I))
# #                 or soup.find_all("article")
# #             )

# #             seen = set()
# #             for i, card in enumerate(cards):
# #                 text = card.get_text(separator=" ", strip=True)
# #                 if len(text) < 30:
# #                     continue

# #                 key = text[:80].lower()
# #                 if key in seen:
# #                     continue
# #                 seen.add(key)

# #                 if any(bp in text.lower() for bp in [
# #                     "careerbliss", "share salary", "update your browser",
# #                     "search by job title", "browse salaries",
# #                 ]):
# #                     continue

# #                 # Extract review text
# #                 review_text = ""
# #                 for s in card.stripped_strings:
# #                     if s.startswith(dq) and s.endswith(dq) and len(s) > 30:
# #                         review_text = s.strip(dq)
# #                         break

# #                 if not review_text:
# #                     for el in card.find_all(["p", "span", "div"]):
# #                         t = el.get_text(separator=" ", strip=True)
# #                         if len(t) > len(review_text) and len(t) > 30:
# #                             if not re.match(r"^(Person|People|Work|Support|Rewards|Growth)", t):
# #                                 review_text = t

# #                 if not review_text or len(review_text) < 20:
# #                     review_text = text

# #                 # Extract rating
# #                 rating = 3.0
# #                 rating_el = card.find(attrs={"aria-label": re.compile(r"\d.*star", re.I)})
# #                 if rating_el:
# #                     m = re.search(r"(\d+\.?\d*)", rating_el.get("aria-label", ""))
# #                     if m:
# #                         rating = float(m.group(1))
# #                 else:
# #                     rating_match = re.search(r"(\d+\.?\d*)\s*(?:/|out of)\s*5", text)
# #                     if rating_match:
# #                         rating = float(rating_match.group(1))
# #                     else:
# #                         num_match = re.search(r"(?:rating|score)[:\s]*(\d+\.?\d*)", text, re.I)
# #                         if num_match:
# #                             val = float(num_match.group(1))
# #                             rating = val if val <= 5 else val / 20.0

# #                 # Extract job title
# #                 job_title = ""
# #                 job_el = card.find(class_=re.compile(r"job.?title|position|role", re.I))
# #                 if job_el:
# #                     job_title = job_el.get_text(strip=True)
# #                 else:
# #                     for tag in card.find_all(["strong", "b", "span"]):
# #                         t = tag.get_text(strip=True)
# #                         if 3 < len(t) < 60 and "review" not in t.lower():
# #                             job_title = t
# #                             break

# #                 # Extract date
# #                 review_date = None
# #                 date_el = card.find("time") or card.find(class_=re.compile(r"date", re.I))
# #                 if date_el:
# #                     raw_d = date_el.get("datetime") or date_el.get("content") or date_el.get_text(strip=True)
# #                     review_date = _normalize_date(raw_d)

# #                 reviews.append(CultureReview(
# #                     review_id=f"careerbliss_{ticker}_{i}",
# #                     rating=min(5.0, max(1.0, rating)),
# #                     title=review_text[:100], pros=review_text[:2000], cons="",
# #                     is_current_employee=True, job_title=job_title[:100],
# #                     review_date=review_date, source="careerbliss",
# #                 ))

# #             logger.info(f"[{ticker}][careerbliss] Extracted {len(reviews)} reviews")

# #         except Exception as e:
# #             logger.warning(f"[{ticker}][careerbliss] Error: {e}")
# #             try:
# #                 page.close()
# #             except Exception:
# #                 pass

# #         return reviews[:limit]

# #     # ── CACHING ─────────────────────────────────────────────

# #     def _cache_path(self, ticker, source):
# #         return self.cache_dir / f"{ticker.upper()}_{source}.json"

# #     def _save_cache(self, ticker, source, reviews):
# #         p = self._cache_path(ticker, source)
# #         try:
# #             data = []
# #             for r in reviews:
# #                 data.append({
# #                     "review_id": r.review_id, "rating": r.rating,
# #                     "title": r.title, "pros": r.pros, "cons": r.cons,
# #                     "advice_to_management": r.advice_to_management,
# #                     "is_current_employee": r.is_current_employee,
# #                     "job_title": r.job_title,
# #                     "review_date": r.review_date.isoformat() if r.review_date else None,
# #                     "source": r.source,
# #                 })
# #             p.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")
# #             logger.info(f"[{ticker}][{source}] Cached {len(reviews)} reviews -> {p}")
# #         except Exception as e:
# #             logger.warning(f"[{ticker}][{source}] Cache save failed: {e}")

# #     def _load_cache(self, ticker, source):
# #         p = self._cache_path(ticker, source)
# #         if not p.exists():
# #             return None
# #         try:
# #             data = json.loads(p.read_text(encoding="utf-8"))
# #             reviews = []
# #             for d in data:
# #                 rd = None
# #                 if d.get("review_date"):
# #                     try:
# #                         rd = datetime.fromisoformat(d["review_date"])
# #                     except (ValueError, TypeError):
# #                         rd = None
# #                 reviews.append(CultureReview(
# #                     review_id=d["review_id"], rating=d["rating"],
# #                     title=d["title"], pros=d["pros"], cons=d["cons"],
# #                     advice_to_management=d.get("advice_to_management"),
# #                     is_current_employee=d.get("is_current_employee", True),
# #                     job_title=d.get("job_title", ""),
# #                     review_date=rd, source=d.get("source", source),
# #                 ))
# #             logger.info(f"[{ticker}][{source}] Loaded {len(reviews)} from cache")
# #             return reviews
# #         except Exception as e:
# #             logger.warning(f"[{ticker}][{source}] Cache load failed: {e}")
# #             return None

# #     # ── MULTI-SOURCE FETCH ──────────────────────────────────

# #     # Review count targets
# #     MIN_REVIEWS = 100
# #     MAX_REVIEWS = 200

# #     def fetch_all_reviews(self, ticker, sources, max_pages=3, use_cache=True):
# #         ticker = ticker.upper()
# #         all_reviews = []
# #         num_sources = len(sources)

# #         # Distribute target evenly across sources, with headroom
# #         per_source_target = self.MAX_REVIEWS // max(num_sources, 1)

# #         for source in sources:
# #             if use_cache:
# #                 cached = self._load_cache(ticker, source)
# #                 if cached is not None:
# #                     all_reviews.extend(cached[:per_source_target])
# #                     continue
# #             revs = []
# #             try:
# #                 if source == "glassdoor":
# #                     # ~10 per page, so for 67 reviews need ~7 pages
# #                     pages_needed = max(max_pages, (per_source_target // 10) + 1)
# #                     revs = self.fetch_glassdoor(ticker, max_pages=pages_needed)
# #                 elif source == "indeed":
# #                     revs = self.scrape_indeed(ticker, limit=per_source_target)
# #                 elif source == "careerbliss":
# #                     revs = self.scrape_careerbliss(ticker, limit=per_source_target)
# #                 else:
# #                     logger.warning(f"[{ticker}] Unknown source: {source}")
# #                     continue
# #             except Exception as e:
# #                 logger.error(f"[{ticker}][{source}] FAILED: {e}")
# #             if revs:
# #                 self._save_cache(ticker, source, revs)
# #             all_reviews.extend(revs[:per_source_target])

# #         total = len(all_reviews)

# #         # Cap at MAX_REVIEWS
# #         if total > self.MAX_REVIEWS:
# #             logger.info(f"[{ticker}] Capping reviews from {total} to {self.MAX_REVIEWS}")
# #             all_reviews = all_reviews[:self.MAX_REVIEWS]

# #         # Warn if below MIN_REVIEWS
# #         if total < self.MIN_REVIEWS:
# #             logger.warning(
# #                 f"[{ticker}] Only {total} reviews collected (minimum target: {self.MIN_REVIEWS}). "
# #                 f"Score confidence may be lower."
# #             )

# #         logger.info(
# #             f"[{ticker}] Total reviews collected: {len(all_reviews)} "
# #             f"(target: {self.MIN_REVIEWS}-{self.MAX_REVIEWS})"
# #         )
# #         return all_reviews

# #     # ── SCORING ─────────────────────────────────────────────

# #     def analyze_reviews(self, company_id, ticker, reviews):
# #         if not reviews:
# #             logger.warning(f"[{ticker}] No reviews to analyze")
# #             return CultureSignal(company_id=company_id, ticker=ticker)

# #         inn_pos = inn_neg = Decimal("0")
# #         dd = ai_m = Decimal("0")
# #         ch_pos = ch_neg = Decimal("0")
# #         total_w = Decimal("0")
# #         rating_sum = 0.0
# #         current_count = 0
# #         pos_kw = []
# #         neg_kw = []
# #         src_counts = {}
# #         now = datetime.now(timezone.utc)

# #         for idx, r in enumerate(reviews):
# #             text = f"{r.pros} {r.cons}".lower()
# #             if r.advice_to_management:
# #                 text += f" {r.advice_to_management}".lower()

# #             days_old = (now - r.review_date).days if r.review_date else -1
# #             rec_w = Decimal("1.0") if days_old < 730 else Decimal("0.5")
# #             emp_w = Decimal("1.2") if r.is_current_employee else Decimal("1.0")
# #             src_w = self.SOURCE_RELIABILITY.get(r.source, Decimal("0.70"))
# #             w = rec_w * emp_w * src_w
# #             total_w += w
# #             rating_sum += r.rating
# #             if r.is_current_employee:
# #                 current_count += 1
# #             src_counts[r.source] = src_counts.get(r.source, 0) + 1

# #             review_hits = []
# #             for kw in self.INNOVATION_POSITIVE:
# #                 if self._keyword_in_text(kw, text):
# #                     inn_pos += w
# #                     review_hits.append(f"+innov:{kw}")
# #                     if kw not in pos_kw:
# #                         pos_kw.append(kw)
# #             for kw in self.INNOVATION_NEGATIVE:
# #                 if self._keyword_in_text(kw, text):
# #                     inn_neg += w
# #                     review_hits.append(f"-innov:{kw}")
# #                     if kw not in neg_kw:
# #                         neg_kw.append(kw)
# #             for kw in self.DATA_DRIVEN_KEYWORDS:
# #                 if self._keyword_in_text(kw, text):
# #                     dd += w
# #                     review_hits.append(f"+data:{kw}")
# #             for kw in self.AI_AWARENESS_KEYWORDS:
# #                 if self._keyword_in_text(kw, text):
# #                     ai_m += w
# #                     review_hits.append(f"+ai:{kw}")
# #             for kw in self.CHANGE_POSITIVE:
# #                 if self._keyword_in_text(kw, text):
# #                     ch_pos += w
# #                     review_hits.append(f"+change:{kw}")
# #                     if kw not in pos_kw:
# #                         pos_kw.append(kw)
# #             for kw in self.CHANGE_NEGATIVE:
# #                 if self._keyword_in_text(kw, text):
# #                     ch_neg += w
# #                     review_hits.append(f"-change:{kw}")
# #                     if kw not in neg_kw:
# #                         neg_kw.append(kw)

# #             current_tag = "current" if r.is_current_employee else "former"
# #             hits_str = ", ".join(review_hits) if review_hits else "(no keyword hits)"
# #             logger.debug(
# #                 f"[{ticker}] Review #{idx+1} [{r.source}] "
# #                 f"rating={r.rating} {current_tag} "
# #                 f"days_old={days_old} "
# #                 f"rec_w={rec_w} emp_w={emp_w} src_w={src_w} w={w} | "
# #                 f"{hits_str}"
# #             )

# #         if total_w > 0:
# #             inn_s = (inn_pos - inn_neg) / total_w * 50 + 50
# #             dd_s = dd / total_w * 100
# #             ai_s = ai_m / total_w * 100
# #             ch_s = (ch_pos - ch_neg) / total_w * 50 + 50
# #         else:
# #             inn_s = Decimal("50")
# #             dd_s = Decimal("0")
# #             ai_s = Decimal("0")
# #             ch_s = Decimal("50")

# #         c = lambda v: max(Decimal("0"), min(Decimal("100"), v))
# #         inn_s, dd_s, ai_s, ch_s = c(inn_s), c(dd_s), c(ai_s), c(ch_s)

# #         overall = (
# #             Decimal("0.30") * inn_s + Decimal("0.25") * dd_s
# #             + Decimal("0.25") * ai_s + Decimal("0.20") * ch_s
# #         ).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)

# #         conf = min(Decimal("0.5") + Decimal(str(len(reviews))) / 200, Decimal("0.90"))
# #         source_bonus = min(Decimal(str(len(src_counts))) * Decimal("0.03"), Decimal("0.10"))
# #         conf = min(conf + source_bonus, Decimal("0.95"))

# #         avg_rating = Decimal(str(round(rating_sum / len(reviews), 2)))
# #         current_ratio = Decimal(str(round(current_count / len(reviews), 3)))

# #         logger.info(f"[{ticker}] {'=' * 44}")
# #         logger.info(f"[{ticker}]   SCORING SUMMARY")
# #         logger.info(f"[{ticker}] {'=' * 44}")
# #         logger.info(f"[{ticker}]   Reviews analyzed:       {len(reviews)}")
# #         logger.info(f"[{ticker}]   Sources:                {src_counts}")
# #         logger.info(f"[{ticker}]   Total weight:           {total_w}")
# #         logger.info(f"[{ticker}]   Avg rating (raw):       {avg_rating}/5.0")
# #         logger.info(f"[{ticker}]   Current employees:      {current_count}/{len(reviews)} ({current_ratio})")
# #         logger.info(f"[{ticker}]   -- Raw accumulators --")
# #         logger.info(f"[{ticker}]     innov_pos={inn_pos}  innov_neg={inn_neg}  net={inn_pos - inn_neg}")
# #         logger.info(f"[{ticker}]     data_mentions={dd}")
# #         logger.info(f"[{ticker}]     ai_mentions={ai_m}")
# #         logger.info(f"[{ticker}]     change_pos={ch_pos}  change_neg={ch_neg}  net={ch_pos - ch_neg}")
# #         logger.info(f"[{ticker}]   -- Component scores --")
# #         logger.info(f"[{ticker}]     Innovation:       {inn_s.quantize(Decimal('0.01'))}  (x0.30 -> {(Decimal('0.30') * inn_s).quantize(Decimal('0.01'))})")
# #         logger.info(f"[{ticker}]     Data-Driven:      {dd_s.quantize(Decimal('0.01'))}  (x0.25 -> {(Decimal('0.25') * dd_s).quantize(Decimal('0.01'))})")
# #         logger.info(f"[{ticker}]     AI Awareness:     {ai_s.quantize(Decimal('0.01'))}  (x0.25 -> {(Decimal('0.25') * ai_s).quantize(Decimal('0.01'))})")
# #         logger.info(f"[{ticker}]     Change Readiness: {ch_s.quantize(Decimal('0.01'))}  (x0.20 -> {(Decimal('0.20') * ch_s).quantize(Decimal('0.01'))})")
# #         logger.info(f"[{ticker}]   -- Final --")
# #         logger.info(f"[{ticker}]     OVERALL SCORE:    {overall}/100")
# #         logger.info(f"[{ticker}]     Confidence:       {conf.quantize(Decimal('0.001'))} (base + {source_bonus} source bonus)")
# #         if pos_kw:
# #             logger.info(f"[{ticker}]     (+) Keywords: {', '.join(pos_kw)}")
# #         if neg_kw:
# #             logger.info(f"[{ticker}]     (-) Keywords: {', '.join(neg_kw)}")
# #         logger.info(f"[{ticker}] {'=' * 44}")

# #         return CultureSignal(
# #             company_id=company_id, ticker=ticker,
# #             innovation_score=inn_s.quantize(Decimal("0.01")),
# #             data_driven_score=dd_s.quantize(Decimal("0.01")),
# #             change_readiness_score=ch_s.quantize(Decimal("0.01")),
# #             ai_awareness_score=ai_s.quantize(Decimal("0.01")),
# #             overall_score=overall, review_count=len(reviews),
# #             avg_rating=avg_rating, current_employee_ratio=current_ratio,
# #             confidence=conf.quantize(Decimal("0.001")),
# #             source_breakdown=src_counts,
# #             positive_keywords_found=pos_kw, negative_keywords_found=neg_kw,
# #         )

# #     # ── S3 UPLOAD ─────────────────────────────────────────────

# #     def _get_s3_service(self):
# #         """Initialize S3 client directly using .env credentials."""
# #         if not hasattr(self, '_s3_client'):
# #             try:
# #                 import boto3
# #                 bucket = os.getenv("S3_BUCKET", "")
# #                 key_id = os.getenv("AWS_ACCESS_KEY_ID", "")
# #                 secret = os.getenv("AWS_SECRET_ACCESS_KEY", "")
# #                 region = os.getenv("AWS_REGION", "us-east-1")

# #                 if not bucket or not key_id or not secret:
# #                     logger.warning(
# #                         "S3 not configured. Set S3_BUCKET, AWS_ACCESS_KEY_ID, "
# #                         "AWS_SECRET_ACCESS_KEY in .env"
# #                     )
# #                     self._s3_client = None
# #                     self._s3_bucket = None
# #                     return None

# #                 self._s3_client = boto3.client(
# #                     's3',
# #                     aws_access_key_id=key_id,
# #                     aws_secret_access_key=secret,
# #                     region_name=region,
# #                 )
# #                 self._s3_bucket = bucket
# #                 logger.info(f"S3 initialized: bucket={bucket}, region={region}")
# #             except Exception as e:
# #                 logger.error(f"S3 initialization failed: {e}")
# #                 self._s3_client = None
# #                 self._s3_bucket = None
# #         return self._s3_client

# #     def _upload_raw_to_s3(self, ticker, reviews):
# #         """Upload raw reviews to S3: glassdoor_signals/raw/{TICKER}_raw.json"""
# #         client = self._get_s3_service()
# #         if not client:
# #             return None
# #         ticker = ticker.upper()
# #         raw_data = []
# #         for r in reviews:
# #             raw_data.append({
# #                 "review_id": r.review_id, "rating": r.rating,
# #                 "title": r.title, "pros": r.pros, "cons": r.cons,
# #                 "advice_to_management": r.advice_to_management,
# #                 "is_current_employee": r.is_current_employee,
# #                 "job_title": r.job_title,
# #                 "review_date": r.review_date.isoformat() if r.review_date else None,
# #                 "source": r.source,
# #             })
# #         s3_key = f"glassdoor_signals/raw/{ticker}_raw.json"
# #         payload = json.dumps(
# #             {"ticker": ticker, "review_count": len(raw_data), "reviews": raw_data},
# #             indent=2, default=str,
# #         )
# #         try:
# #             client.put_object(
# #                 Bucket=self._s3_bucket, Key=s3_key,
# #                 Body=payload.encode("utf-8"),
# #                 ContentType="application/json",
# #             )
# #             logger.info(f"[{ticker}] Uploaded {len(raw_data)} raw reviews to S3: {s3_key}")
# #             return s3_key
# #         except Exception as e:
# #             logger.error(f"[{ticker}] S3 raw upload failed: {e}")
# #             return None

# #     def _upload_output_to_s3(self, signal):
# #         """Upload scored output to S3: glassdoor_signals/output/{TICKER}_culture.json"""
# #         client = self._get_s3_service()
# #         if not client:
# #             return None
# #         ticker = signal.ticker.upper()
# #         output_data = asdict(signal)
# #         for k, v in output_data.items():
# #             if isinstance(v, Decimal):
# #                 output_data[k] = float(v)
# #         s3_key = f"glassdoor_signals/output/{ticker}_culture.json"
# #         payload = json.dumps(output_data, indent=2, default=str)
# #         try:
# #             client.put_object(
# #                 Bucket=self._s3_bucket, Key=s3_key,
# #                 Body=payload.encode("utf-8"),
# #                 ContentType="application/json",
# #             )
# #             logger.info(f"[{ticker}] Uploaded culture signal to S3: {s3_key}")
# #             return s3_key
# #         except Exception as e:
# #             logger.error(f"[{ticker}] S3 output upload failed: {e}")
# #             return None

# #     # ── MAIN ENTRY POINTS ──────────────────────────────────

# #     def collect_and_analyze(self, ticker, sources=None, max_pages=3, use_cache=True):
# #         ticker = validate_ticker(ticker)
# #         if sources is None:
# #             sources = ["glassdoor", "indeed", "careerbliss"]
# #         sources = [s for s in sources if s in VALID_SOURCES]
# #         reg = COMPANY_REGISTRY[ticker]
# #         logger.info(f"{'=' * 55}")
# #         logger.info(f"CULTURE COLLECTION: {ticker} ({reg['name']})")
# #         logger.info(f"   Sector: {reg['sector']}")
# #         logger.info(f"   Sources: {', '.join(sources)}")
# #         logger.info(f"{'=' * 55}")

# #         reviews = self.fetch_all_reviews(ticker, sources=sources, max_pages=max_pages, use_cache=use_cache)
# #         signal = self.analyze_reviews(ticker, ticker, reviews)

# #         # Upload to S3 only
# #         self._upload_raw_to_s3(ticker, reviews)
# #         self._upload_output_to_s3(signal)

# #         return signal

# #     def collect_multiple(self, tickers, sources=None, max_pages=3, use_cache=True, delay=2.0):
# #         results = {}
# #         try:
# #             for i, ticker in enumerate(tickers):
# #                 try:
# #                     signal = self.collect_and_analyze(ticker, sources=sources, max_pages=max_pages, use_cache=use_cache)
# #                     results[ticker.upper()] = signal
# #                 except Exception as e:
# #                     logger.error(f"[{ticker}] FAILED: {e}")
# #                 if i < len(tickers) - 1:
# #                     logger.info(f"Waiting {delay}s before next ticker...")
# #                     time.sleep(delay)
# #         finally:
# #             self.close_browser()
# #         return results

# #     def _save_signal(self, signal):
# #         d = Path("results")
# #         d.mkdir(parents=True, exist_ok=True)
# #         p = d / f"{signal.ticker.lower()}_culture.json"
# #         p.write_text(signal.to_json(), encoding="utf-8")
# #         logger.info(f"[{signal.ticker}] Saved -> {p}")


# # # =====================================================================
# # # DISPLAY
# # # =====================================================================

# # def print_signal(signal):
# #     reg = COMPANY_REGISTRY.get(signal.ticker, {})
# #     name = reg.get("name", signal.ticker)
# #     sector = reg.get("sector", "")
# #     print(f"\n{'=' * 60}")
# #     print(f"  CULTURE ANALYSIS -- {signal.ticker} ({name})")
# #     if sector:
# #         print(f"  Sector: {sector}")
# #     print(f"{'=' * 60}")
# #     print(f"  Overall Score:          {signal.overall_score}/100")
# #     print(f"  Confidence:             {signal.confidence}")
# #     print(f"  Reviews Analyzed:       {signal.review_count}")
# #     print(f"  Source Breakdown:       {signal.source_breakdown}")
# #     print(f"  Avg Rating:             {signal.avg_rating}/5.0")
# #     print(f"  Current Employee Ratio: {signal.current_employee_ratio}")
# #     print()
# #     print(f"  Component Scores:       Weight   Score")
# #     print(f"    Innovation:           0.30   {signal.innovation_score:>8}")
# #     print(f"    Data-Driven:          0.25   {signal.data_driven_score:>8}")
# #     print(f"    AI Awareness:         0.25   {signal.ai_awareness_score:>8}")
# #     print(f"    Change Readiness:     0.20   {signal.change_readiness_score:>8}")
# #     if signal.positive_keywords_found:
# #         print(f"\n  (+) Keywords: {', '.join(signal.positive_keywords_found[:10])}")
# #     if signal.negative_keywords_found:
# #         print(f"  (-) Keywords: {', '.join(signal.negative_keywords_found[:10])}")


# # # =====================================================================
# # # MAIN
# # # =====================================================================

# # def main():
# #     args = sys.argv[1:]
# #     use_cache = "--no-cache" not in args

# #     sources = None
# #     for a in args:
# #         if a.startswith("--sources="):
# #             sources = [s.strip() for s in a.split("=", 1)[1].split(",")]

# #     clean_args = [a for a in args if not a.startswith("-")]

# #     if "--all" in args:
# #         tickers = all_tickers()
# #         glassdoor_active = sources is None or "glassdoor" in sources
# #         est = len(tickers) * 3 if glassdoor_active else 0
# #         print(f"\n  Running ALL {len(tickers)} tickers.")
# #         if glassdoor_active:
# #             print(f"   Estimated Glassdoor API calls: ~{est} (free tier = 500/month)")
# #         print(f"   Tickers: {', '.join(tickers)}\n")
# #         confirm = input("   Continue? [y/N]: ").strip().lower()
# #         if confirm != "y":
# #             print("Aborted.")
# #             return
# #     elif clean_args:
# #         tickers = []
# #         for t in clean_args:
# #             try:
# #                 tickers.append(validate_ticker(t))
# #             except ValueError as e:
# #                 print(f"ERROR: {e}")
# #                 return
# #     else:
# #         print("\n" + "=" * 58)
# #         print("  Multi-Source Culture Collector (CS3)")
# #         print("=" * 58)
# #         print()
# #         print("  Usage:")
# #         print("    python -m pipelines.culture_collector <TICKER> [TICKER ...]")
# #         print("    python -m pipelines.culture_collector --all")
# #         print()
# #         print("  Options:")
# #         print("    --no-cache                       Skip cached reviews")
# #         print("    --sources=glassdoor,indeed,careerbliss  Pick sources")
# #         print()
# #         print("  Source Reliability:")
# #         print("    Glassdoor    0.85  (via RapidAPI)")
# #         print("    Indeed       0.80  (via Playwright)")
# #         print("    CareerBliss  0.75  (via Playwright)")
# #         print()
# #         print("  Allowed tickers (13):")
# #         for t in sorted(ALLOWED_TICKERS):
# #             reg = COMPANY_REGISTRY[t]
# #             print(f"    {t:<6} {reg['name']:<30} {reg['sector']}")
# #         print()
# #         print("  RapidAPI free tier = 500 requests/month.")
# #         print("  Each ticker uses ~3 Glassdoor API calls.")
# #         print("=" * 58)
# #         return

# #     glassdoor_active = sources is None or "glassdoor" in sources
# #     if glassdoor_active:
# #         est = len(tickers) * 3
# #         print(f"\n  Glassdoor API calls: ~{est} (3 pages x {len(tickers)} tickers)")
# #     print(f"  Tickers: {', '.join(tickers)}")
# #     if sources:
# #         print(f"  Sources: {', '.join(sources)}")
# #     print()

# #     collector = CultureCollector()
# #     results = collector.collect_multiple(tickers, sources=sources, use_cache=use_cache, delay=2.0)

# #     print("\n\n" + "#" * 60)
# #     print(f"#  MULTI-SOURCE CULTURE ANALYSIS -- {len(results)} companies")
# #     print("#" * 60)

# #     for ticker, signal in results.items():
# #         print_signal(signal)

# #     if len(results) > 1:
# #         print(f"\n{'=' * 62}")
# #         print(
# #             f"  {'Ticker':<6} {'Overall':>8} {'Innov':>7} "
# #             f"{'Data':>7} {'AI':>7} {'Change':>7} {'#Rev':>5} {'Conf':>6}"
# #         )
# #         print(f"  {'-'*6} {'-'*8} {'-'*7} {'-'*7} {'-'*7} {'-'*7} {'-'*5} {'-'*6}")
# #         for t, s in sorted(results.items(), key=lambda x: x[1].overall_score, reverse=True):
# #             print(
# #                 f"  {t:<6} {s.overall_score:>8} {s.innovation_score:>7} "
# #                 f"{s.data_driven_score:>7} {s.ai_awareness_score:>7} "
# #                 f"{s.change_readiness_score:>7} {s.review_count:>5} {s.confidence:>6}"
# #             )
# #         print(f"{'=' * 62}")


# # if __name__ == "__main__":
# #     main()


# import json
# import logging
# import os
# import re
# import sys
# import time
# from dataclasses import dataclass, field, asdict
# from datetime import datetime, timezone, timedelta
# from decimal import Decimal, ROUND_HALF_UP
# from pathlib import Path
# from typing import Any, Dict, List, Optional

# import httpx
# from dotenv import load_dotenv

# _THIS_FILE = Path(__file__).resolve()
# _APP_DIR = _THIS_FILE.parent.parent
# _PROJECT_ROOT = _APP_DIR.parent
# for _p in [str(_PROJECT_ROOT), str(_APP_DIR)]:
#     if _p not in sys.path:
#         sys.path.insert(0, _p)

# load_dotenv(_PROJECT_ROOT / ".env")

# logging.basicConfig(level=logging.DEBUG, format="%(levelname)s | %(message)s")
# logger = logging.getLogger(__name__)

# logging.getLogger("httpx").setLevel(logging.WARNING)
# logging.getLogger("httpcore").setLevel(logging.WARNING)
# logging.getLogger("boto3").setLevel(logging.WARNING)
# logging.getLogger("botocore").setLevel(logging.WARNING)
# logging.getLogger("urllib3").setLevel(logging.WARNING)
# logging.getLogger("s3transfer").setLevel(logging.WARNING)


# # =====================================================================
# # DATA MODELS
# # =====================================================================

# @dataclass
# class CultureReview:
#     review_id: str
#     rating: float
#     title: str
#     pros: str
#     cons: str
#     advice_to_management: Optional[str] = None
#     is_current_employee: bool = True
#     job_title: str = ""
#     review_date: Optional[datetime] = None
#     source: str = "unknown"

#     def __post_init__(self):
#         if self.review_date is None:
#             self.review_date = datetime.now(timezone.utc)


# @dataclass
# class CultureSignal:
#     company_id: str
#     ticker: str
#     innovation_score: Decimal = Decimal("50.00")
#     data_driven_score: Decimal = Decimal("0.00")
#     change_readiness_score: Decimal = Decimal("50.00")
#     ai_awareness_score: Decimal = Decimal("0.00")
#     overall_score: Decimal = Decimal("25.00")
#     review_count: int = 0
#     avg_rating: Decimal = Decimal("0.00")
#     current_employee_ratio: Decimal = Decimal("0.000")
#     confidence: Decimal = Decimal("0.000")
#     source_breakdown: Dict[str, int] = field(default_factory=dict)
#     positive_keywords_found: List[str] = field(default_factory=list)
#     negative_keywords_found: List[str] = field(default_factory=list)

#     def to_json(self, indent=2):
#         d = asdict(self)
#         for k, v in d.items():
#             if isinstance(v, Decimal):
#                 d[k] = float(v)
#         return json.dumps(d, indent=indent, default=str)


# # =====================================================================
# # COMPANY REGISTRY — 13 tickers
# # =====================================================================

# COMPANY_REGISTRY = {
#     "NVDA": {
#         "name": "NVIDIA", "sector": "Technology",
#         "glassdoor_id": "NVIDIA",
#         "indeed_slugs": ["NVIDIA"],
#         "careerbliss_slug": "nvidia",
#     },
#     "JPM": {
#         "name": "JPMorgan Chase", "sector": "Financial Services",
#         "glassdoor_id": "JPMorgan-Chase",
#         "indeed_slugs": ["JPMorgan-Chase", "jpmorgan-chase"],
#         "careerbliss_slug": "jpmorgan-chase",
#     },
#     "WMT": {
#         "name": "Walmart", "sector": "Consumer Retail",
#         "glassdoor_id": "Walmart",
#         "indeed_slugs": ["Walmart"],
#         "careerbliss_slug": "walmart",
#     },
#     "GE": {
#         "name": "GE Aerospace", "sector": "Industrials Manufacturing",
#         "glassdoor_id": "GE-Aerospace",
#         "indeed_slugs": ["GE-Aerospace", "General-Electric"],
#         "careerbliss_slug": "ge-aerospace",
#     },
#     "DG": {
#         "name": "Dollar General", "sector": "Consumer Retail",
#         "glassdoor_id": "Dollar-General",
#         "indeed_slugs": ["Dollar-General"],
#         "careerbliss_slug": "dollar-general",
#     },
# }

# ALLOWED_TICKERS = set(COMPANY_REGISTRY.keys())
# VALID_SOURCES = {"glassdoor", "indeed", "careerbliss"}


# def validate_ticker(ticker):
#     t = ticker.upper()
#     if t not in ALLOWED_TICKERS:
#         raise ValueError(
#             f"Unknown ticker '{t}'. Allowed: {', '.join(sorted(ALLOWED_TICKERS))}"
#         )
#     return t


# def all_tickers():
#     return sorted(ALLOWED_TICKERS)


# # =====================================================================
# # HELPERS
# # =====================================================================

# def _normalize_date(raw):
#     if not raw:
#         return None
#     raw = raw.strip()
#     iso = re.match(r"(\d{4}-\d{2}-\d{2})", raw)
#     if iso:
#         try:
#             return datetime.strptime(iso.group(1), "%Y-%m-%d").replace(tzinfo=timezone.utc)
#         except ValueError:
#             pass
#     for fmt in ("%B %d, %Y", "%b %d, %Y", "%d %B %Y", "%d %b %Y",
#                 "%m/%d/%Y", "%d/%m/%Y", "%Y/%m/%d"):
#         try:
#             return datetime.strptime(raw, fmt).replace(tzinfo=timezone.utc)
#         except ValueError:
#             continue
#     rel = re.match(r"(\d+)\s+(day|week|month|year)s?\s+ago", raw, re.I)
#     if rel:
#         num = int(rel.group(1))
#         unit = rel.group(2).lower()
#         days = {"day": 1, "week": 7, "month": 30, "year": 365}[unit]
#         return datetime.now(timezone.utc) - timedelta(days=num * days)
#     return None


# # =====================================================================
# # CULTURE COLLECTOR
# # =====================================================================

# class CultureCollector:

#     # ── Keyword lists ───────────────────────────────────────

#     INNOVATION_POSITIVE = [
#         "innovative", "cutting-edge", "forward-thinking",
#         "encourages new ideas", "experimental", "creative freedom",
#         "startup mentality", "move fast", "disruptive",
#         "innovation", "pioneering", "bleeding edge",
#     ]
#     INNOVATION_NEGATIVE = [
#         "bureaucratic", "slow to change", "resistant",
#         "outdated", "stuck in old ways", "red tape",
#         "politics", "siloed", "hierarchical",
#         "stagnant", "old-fashioned", "behind the times",
#     ]
#     DATA_DRIVEN_KEYWORDS = [
#         "data-driven", "metrics", "evidence-based",
#         "analytical", "kpis", "dashboards", "data culture",
#         "measurement", "quantitative",
#         "data informed", "analytics", "data-centric",
#     ]
#     AI_AWARENESS_KEYWORDS = [
#         "ai", "artificial intelligence", "machine learning",
#         "automation", "data science", "ml", "algorithms",
#         "predictive", "neural network",
#         "deep learning", "nlp", "llm", "generative ai",
#         "chatbot", "computer vision",
#     ]
#     CHANGE_POSITIVE = [
#         "agile", "adaptive", "fast-paced", "embraces change",
#         "continuous improvement", "growth mindset",
#         "evolving", "dynamic", "transforming",
#     ]
#     CHANGE_NEGATIVE = [
#         "rigid", "traditional", "slow", "risk-averse",
#         "change resistant", "old school",
#         "inflexible", "set in their ways", "fear of change",
#     ]

#     # Keywords that need whole-word matching (short or common substrings)
#     WHOLE_WORD_KEYWORDS = [
#         # AI keywords - very short, appear inside many English words
#         "ai", "ml", "nlp", "llm",
#         # Change keywords - "slow" matches "slowly", "slow climb" etc.
#         "slow",
#         # Innovation keywords - "traditional" matches "non-traditional"
#         "traditional",
#         # Change keywords - "rigid" matches "frigid"
#         "rigid",
#         # "dynamic" is generic, ensure whole word
#         "dynamic",
#         # "agile" could appear in non-culture contexts
#         "agile",
#     ]

#     # ── Context-exclusion patterns for ambiguous keywords ───
#     # When these patterns appear near the keyword, the match is REJECTED
#     # because the keyword is used in a non-culture context.
#     KEYWORD_CONTEXT_EXCLUSIONS = {
#         "slow": [
#             r"slow\s+climb",
#             r"slow\s+(?:career|promotion|advancement|growth|process|hiring|recruiting|interview)",
#             r"(?:career|promotion|advancement|growth|process|hiring|recruiting|interview)\s+(?:is|are|was|were|seems?|feels?)\s+slow",
#             r"slow\s+(?:to\s+)?(?:promote|hire|respond|reply|get\s+back)",
#         ],
#         "traditional": [
#             r"traditional\s+(?:benefits|hours|schedule|shift|role)",
#         ],
#         "politics": [
#             r"(?:office|internal|team)\s+politics",
#         ],
#         "automation": [
#             r"(?:test|testing)\s+automation",
#             r"automation\s+(?:test|engineer|qa)",
#         ],
#     }

#     # ── Noise detection patterns for Indeed page dumps ──────
#     INDEED_NOISE_INDICATORS = [
#         "slide 1 of",
#         "slide 2 of",
#         "see more jobs",
#         "selecting an option will update the page",
#         "report review copy link",
#         "show more report review",
#         "page 1 of 3",
#         "days ago slide",
#         "an hour",  # salary ranges like "$15 - $28 an hour"
#     ]
#     # Threshold: if this many noise indicators found, it's a page dump
#     INDEED_NOISE_THRESHOLD = 3

#     # Maximum text length for a single legitimate review (chars)
#     MAX_REVIEW_TEXT_LENGTH = 2000

#     def _keyword_in_text(self, kw, text):
#         """Check if keyword exists in text. Uses word boundary for short/ambiguous keywords."""
#         if kw in self.WHOLE_WORD_KEYWORDS:
#             return bool(re.search(r'\b' + re.escape(kw) + r'\b', text))
#         return kw in text

#     def _keyword_in_context(self, kw, text):
#         """
#         Check if keyword exists in text AND is used in a culture-relevant context.

#         Returns True only if:
#         1. The keyword is found in the text
#         2. None of the context-exclusion patterns match

#         This prevents false positives like "slow climb" counting as
#         change-resistance when it's about career progression.
#         """
#         if not self._keyword_in_text(kw, text):
#             return False

#         exclusions = self.KEYWORD_CONTEXT_EXCLUSIONS.get(kw)
#         if not exclusions:
#             return True

#         for pattern in exclusions:
#             if re.search(pattern, text, re.IGNORECASE):
#                 logger.debug(f"  Context exclusion: '{kw}' rejected by pattern '{pattern}'")
#                 return False

#         return True

#     def _is_indeed_page_dump(self, review):
#         """
#         Detect if an Indeed review is actually a page dump (navigation + multiple reviews).

#         Returns True if the review text looks like scraped page chrome rather than
#         an individual employee review.
#         """
#         text = f"{review.pros} {review.cons}".lower()

#         # Check 1: noise indicator count
#         noise_count = sum(1 for ind in self.INDEED_NOISE_INDICATORS if ind in text)
#         if noise_count >= self.INDEED_NOISE_THRESHOLD:
#             return True

#         # Check 2: text is excessively long for a single review
#         if len(text) > self.MAX_REVIEW_TEXT_LENGTH:
#             # Long text with multiple date patterns = concatenated reviews
#             date_pattern = re.findall(
#                 r'(?:january|february|march|april|may|june|july|august|'
#                 r'september|october|november|december)\s+\d{1,2},\s+\d{4}',
#                 text
#             )
#             if len(date_pattern) >= 3:
#                 return True

#         # Check 3: contains job listing elements
#         job_listing_signals = [
#             r'\$\d+\s*-\s*\$\d+\s*an?\s*hour',
#             r'\d+\s*days?\s*ago\s*slide',
#             r'see more jobs',
#         ]
#         listing_count = sum(1 for p in job_listing_signals if re.search(p, text))
#         if listing_count >= 2:
#             return True

#         return False

#     def _deduplicate_reviews(self, reviews):
#         """
#         Remove duplicate reviews based on text similarity.

#         Uses the first 150 chars of normalized (pros+cons) as a fingerprint.
#         Keeps the first occurrence (typically the better-structured one).
#         """
#         seen = set()
#         unique = []
#         for r in reviews:
#             # Build fingerprint from review content
#             content = f"{r.pros} {r.cons}".lower().strip()
#             # Normalize whitespace
#             content = re.sub(r'\s+', ' ', content)
#             # Use first 150 chars as fingerprint
#             fingerprint = content[:150]

#             if fingerprint in seen:
#                 logger.debug(f"  Dedup: skipping duplicate review {r.review_id}")
#                 continue
#             seen.add(fingerprint)
#             unique.append(r)

#         removed = len(reviews) - len(unique)
#         if removed > 0:
#             logger.info(f"  Deduplication removed {removed} duplicate reviews "
#                         f"({len(reviews)} -> {len(unique)})")
#         return unique

#     SOURCE_RELIABILITY = {
#         "glassdoor":   Decimal("0.85"),
#         "indeed":      Decimal("0.80"),
#         "careerbliss": Decimal("0.75"),
#         "unknown":     Decimal("0.70"),
#     }

#     RAPIDAPI_HOST = "real-time-glassdoor-data.p.rapidapi.com"
#     RAPIDAPI_BASE = f"https://{RAPIDAPI_HOST}"

#     def __init__(self, cache_dir="data/culture_cache"):
#         self.cache_dir = Path(cache_dir)
#         self.cache_dir.mkdir(parents=True, exist_ok=True)
#         self._browser = None
#         self._playwright = None

#     # ── Browser management ──────────────────────────────────

#     def _get_browser(self):
#         if self._browser is None:
#             from playwright.sync_api import sync_playwright
#             self._playwright = sync_playwright().start()
#             self._browser = self._playwright.chromium.launch(
#                 headless=True,
#                 args=[
#                     "--disable-blink-features=AutomationControlled",
#                     "--no-sandbox",
#                     "--disable-dev-shm-usage",
#                     "--disable-infobars",
#                     "--window-size=1920,1080",
#                 ],
#             )
#             logger.info("Playwright browser launched")
#         return self._browser

#     def _new_page(self, stealth=True):
#         browser = self._get_browser()
#         ctx = browser.new_context(
#             user_agent=(
#                 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
#                 "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
#             ),
#             viewport={"width": 1920, "height": 1080},
#             locale="en-US",
#             timezone_id="America/New_York",
#             extra_http_headers={
#                 "Accept-Language": "en-US,en;q=0.9",
#                 "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
#                 "Sec-Fetch-Dest": "document",
#                 "Sec-Fetch-Mode": "navigate",
#                 "Sec-Fetch-Site": "none",
#                 "Sec-Fetch-User": "?1",
#                 "Upgrade-Insecure-Requests": "1",
#             },
#         )
#         page = ctx.new_page()
#         if stealth:
#             page.add_init_script("""
#                 Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
#                 Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
#                 Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
#                 window.chrome = { runtime: {} };
#                 const origQuery = window.navigator.permissions.query;
#                 window.navigator.permissions.query = (p) =>
#                     p.name === 'notifications'
#                         ? Promise.resolve({ state: Notification.permission })
#                         : origQuery(p);
#             """)
#         page.route(
#             "**/*.{png,jpg,jpeg,gif,svg,webp,woff,woff2,ttf,mp4,webm}",
#             lambda route: route.abort(),
#         )
#         return page

#     def close_browser(self):
#         if self._browser:
#             self._browser.close()
#             self._browser = None
#         if self._playwright:
#             self._playwright.stop()
#             self._playwright = None
#             logger.info("Playwright browser closed")

#     # ── SOURCE 1: GLASSDOOR via RapidAPI ────────────────────

#     def _get_api_key(self):
#         key = os.getenv("RAPIDAPI_KEY", "")
#         if not key:
#             raise EnvironmentError(
#                 "RAPIDAPI_KEY not set. Add it to your .env file.\n"
#                 "Get a free key at: https://rapidapi.com/letscrape-6bRBa3QguO5/"
#                 "api/real-time-glassdoor-data"
#             )
#         return key

#     def _api_headers(self):
#         return {
#             "x-rapidapi-key": self._get_api_key(),
#             "x-rapidapi-host": self.RAPIDAPI_HOST,
#         }

#     def fetch_glassdoor(self, ticker, max_pages=3, timeout=30.0):
#         ticker = ticker.upper()
#         reg = COMPANY_REGISTRY[ticker]
#         company_id = reg["glassdoor_id"]
#         reviews = []

#         for page_num in range(1, max_pages + 1):
#             params = {
#                 "company_id": company_id,
#                 "page": str(page_num),
#                 "sort": "POPULAR",
#                 "language": "en",
#                 "only_current_employees": "false",
#                 "extended_rating_data": "false",
#                 "domain": "www.glassdoor.com",
#             }
#             url = f"{self.RAPIDAPI_BASE}/company-reviews"
#             logger.info(f"[{ticker}][glassdoor] Fetching page {page_num}...")

#             try:
#                 resp = httpx.get(url, headers=self._api_headers(), params=params, timeout=timeout)
#                 resp.raise_for_status()
#                 raw_data = resp.json()
#             except httpx.HTTPStatusError as e:
#                 logger.error(f"[{ticker}][glassdoor] HTTP {e.response.status_code} on page {page_num}")
#                 break
#             except Exception as e:
#                 logger.error(f"[{ticker}][glassdoor] Request failed: {e}")
#                 break

#             reviews_raw = raw_data.get("data", {}).get("reviews", [])
#             total = raw_data.get("data", {}).get("review_count", "?")
#             pages_total = raw_data.get("data", {}).get("page_count", "?")

#             if not reviews_raw:
#                 logger.info(f"[{ticker}][glassdoor] No more reviews at page {page_num}")
#                 break

#             for r in reviews_raw:
#                 parsed = self._parse_glassdoor_review(ticker, r)
#                 if parsed:
#                     reviews.append(parsed)

#             logger.info(
#                 f"[{ticker}][glassdoor] Page {page_num}: {len(reviews_raw)} reviews "
#                 f"(total available: {total}, pages: {pages_total})"
#             )
#             if page_num < max_pages:
#                 time.sleep(0.5)

#         logger.info(f"[{ticker}][glassdoor] Total fetched: {len(reviews)}")
#         return reviews

#     def _parse_glassdoor_review(self, ticker, raw):
#         try:
#             rid = f"glassdoor_{ticker}_{raw.get('review_id', 'unknown')}"
#             rating = float(raw.get("rating", 3.0))
#             title = raw.get("summary") or raw.get("title") or ""
#             pros = raw.get("pros") or ""
#             cons = raw.get("cons") or ""
#             advice = raw.get("advice_to_management") or None
#             job_title = raw.get("job_title") or ""
#             is_current = bool(raw.get("is_current_employee", False))
#             emp_status = raw.get("employment_status", "")
#             if isinstance(emp_status, str) and emp_status.upper() == "REGULAR":
#                 is_current = True
#             review_date = None
#             raw_date = raw.get("review_datetime") or None
#             if raw_date and isinstance(raw_date, str):
#                 review_date = _normalize_date(raw_date[:10])
#             return CultureReview(
#                 review_id=rid, rating=min(5.0, max(1.0, rating)),
#                 title=title[:200], pros=pros[:2000], cons=cons[:2000],
#                 advice_to_management=advice, is_current_employee=is_current,
#                 job_title=job_title, review_date=review_date, source="glassdoor",
#             )
#         except Exception as e:
#             logger.warning(f"[{ticker}][glassdoor] Parse error: {e}")
#             return None

#     # ── SOURCE 2: INDEED via Playwright + BeautifulSoup ─────

#     def scrape_indeed(self, ticker, limit=50):
#         from bs4 import BeautifulSoup
#         ticker = ticker.upper()
#         slugs = COMPANY_REGISTRY[ticker]["indeed_slugs"]
#         reviews = []

#         # Calculate pages needed (Indeed shows ~20 reviews per page)
#         max_pages = max(1, (limit // 20) + 1)

#         for slug in slugs:
#             for page_num in range(max_pages):
#                 start = page_num * 20
#                 url = f"https://www.indeed.com/cmp/{slug}/reviews"
#                 if page_num > 0:
#                     url = f"{url}?start={start}"
#                 logger.info(f"[{ticker}][indeed] Scraping page {page_num + 1}: {url}")
#                 try:
#                     page = self._new_page(stealth=True)
#                     page.goto(url, wait_until="domcontentloaded", timeout=30000)
#                     time.sleep(3)
#                     if "blocked" in page.title().lower():
#                         logger.warning(f"[{ticker}][indeed] Blocked: {page.title()}")
#                         page.close()
#                         break
#                     try:
#                         page.wait_for_selector(
#                             '[data-testid*="review"], .cmp-ReviewsList, '
#                             '[data-tn-component="reviewsList"]',
#                             timeout=10000,
#                         )
#                     except Exception:
#                         logger.warning(f"[{ticker}][indeed] Review container not found on page {page_num + 1}")
#                         page.close()
#                         break
#                     for _ in range(3):
#                         page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
#                         time.sleep(1)
#                     html = page.content()
#                     page.close()

#                     soup = BeautifulSoup(html, "html.parser")
#                     cards = (
#                         soup.find_all("div", {"data-testid": re.compile(r"review")})
#                         or soup.find_all("div", class_=re.compile(r"cmp-Review(?!sList)"))
#                         or soup.find_all("div", class_=re.compile(r"review", re.I))
#                     )

#                     if not cards:
#                         logger.info(f"[{ticker}][indeed] No more reviews at page {page_num + 1}")
#                         break

#                     page_count = 0
#                     for i, card in enumerate(cards):
#                         text = card.get_text(separator=" ", strip=True)
#                         if len(text) < 30:
#                             continue
#                         title_el = card.find(
#                             ["h2", "h3", "a", "span"],
#                             class_=re.compile(r"title|header", re.I),
#                         )
#                         title = title_el.get_text(strip=True) if title_el else text[:100]
#                         rating = 3.0
#                         star_el = card.find(attrs={"aria-label": re.compile(r"\d.*star", re.I)})
#                         if star_el:
#                             m = re.search(r"(\d+\.?\d*)", star_el.get("aria-label", ""))
#                             if m:
#                                 rating = float(m.group(1))
#                         pros_text, cons_text = "", ""
#                         for label in card.find_all(string=re.compile(r"^Pros?$", re.I)):
#                             p = label.find_parent()
#                             if p and p.find_next_sibling():
#                                 pros_text = p.find_next_sibling().get_text(separator=" ", strip=True)
#                         for label in card.find_all(string=re.compile(r"^Cons?$", re.I)):
#                             p = label.find_parent()
#                             if p and p.find_next_sibling():
#                                 cons_text = p.find_next_sibling().get_text(separator=" ", strip=True)
#                         if not pros_text and not cons_text:
#                             pros_text = text
#                         date_el = card.find("time") or card.find(class_=re.compile(r"date", re.I))
#                         review_date = None
#                         if date_el:
#                             raw_d = date_el.get("datetime") or date_el.get("content") or date_el.get_text(strip=True)
#                             review_date = _normalize_date(raw_d)
#                         is_current = "current" in text.lower() and "former" not in text.lower()
#                         global_idx = len(reviews)
#                         reviews.append(CultureReview(
#                             review_id=f"indeed_{ticker}_{global_idx}",
#                             rating=min(5.0, max(1.0, rating)),
#                             title=title[:200], pros=pros_text[:2000], cons=cons_text[:2000],
#                             is_current_employee=is_current, review_date=review_date,
#                             source="indeed",
#                         ))
#                         page_count += 1

#                     logger.info(f"[{ticker}][indeed] Page {page_num + 1}: {page_count} reviews (total: {len(reviews)})")

#                     if len(reviews) >= limit:
#                         break

#                     if page_num < max_pages - 1:
#                         time.sleep(2)

#                 except Exception as e:
#                     logger.warning(f"[{ticker}][indeed] Error on page {page_num + 1} for slug '{slug}': {e}")
#                     try:
#                         page.close()
#                     except Exception:
#                         pass
#                     break

#             if reviews:
#                 logger.info(f"[{ticker}][indeed] Extracted {len(reviews)} total reviews")
#                 break
#         return reviews[:limit]

#     # ── SOURCE 3: CAREERBLISS via Playwright + BeautifulSoup ─

#     def scrape_careerbliss(self, ticker, limit=50):
#         from bs4 import BeautifulSoup
#         ticker = ticker.upper()
#         slug = COMPANY_REGISTRY[ticker]["careerbliss_slug"]
#         reviews = []
#         dq = chr(34)

#         url = f"https://www.careerbliss.com/{slug}/reviews/"
#         logger.info(f"[{ticker}][careerbliss] Scraping: {url}")

#         try:
#             page = self._new_page(stealth=True)
#             page.goto(url, wait_until="domcontentloaded", timeout=30000)
#             time.sleep(3)

#             title_text = page.title().lower()
#             if any(w in title_text for w in ["blocked", "denied", "captcha"]):
#                 logger.warning(f"[{ticker}][careerbliss] Blocked: {page.title()}")
#                 page.close()
#                 return reviews

#             for _ in range(5):
#                 page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
#                 time.sleep(1)

#             for _ in range(3):
#                 try:
#                     more = page.query_selector(
#                         'a:has-text("More Reviews"), a:has-text("Show More"), '
#                         'button:has-text("More"), a.next'
#                     )
#                     if more and more.is_visible():
#                         more.click()
#                         time.sleep(2)
#                     else:
#                         break
#                 except Exception:
#                     break

#             html = page.content()
#             page.close()

#             soup = BeautifulSoup(html, "html.parser")

#             cards = (
#                 soup.find_all("div", class_=re.compile(r"review", re.I))
#                 or soup.find_all("li", class_=re.compile(r"review", re.I))
#                 or soup.find_all("article")
#             )

#             seen = set()
#             for i, card in enumerate(cards):
#                 text = card.get_text(separator=" ", strip=True)
#                 if len(text) < 30:
#                     continue

#                 key = text[:80].lower()
#                 if key in seen:
#                     continue
#                 seen.add(key)

#                 if any(bp in text.lower() for bp in [
#                     "careerbliss", "share salary", "update your browser",
#                     "search by job title", "browse salaries",
#                 ]):
#                     continue

#                 # Extract review text
#                 review_text = ""
#                 for s in card.stripped_strings:
#                     if s.startswith(dq) and s.endswith(dq) and len(s) > 30:
#                         review_text = s.strip(dq)
#                         break

#                 if not review_text:
#                     for el in card.find_all(["p", "span", "div"]):
#                         t = el.get_text(separator=" ", strip=True)
#                         if len(t) > len(review_text) and len(t) > 30:
#                             if not re.match(r"^(Person|People|Work|Support|Rewards|Growth)", t):
#                                 review_text = t

#                 if not review_text or len(review_text) < 20:
#                     review_text = text

#                 # Extract rating
#                 rating = 3.0
#                 rating_el = card.find(attrs={"aria-label": re.compile(r"\d.*star", re.I)})
#                 if rating_el:
#                     m = re.search(r"(\d+\.?\d*)", rating_el.get("aria-label", ""))
#                     if m:
#                         rating = float(m.group(1))
#                 else:
#                     rating_match = re.search(r"(\d+\.?\d*)\s*(?:/|out of)\s*5", text)
#                     if rating_match:
#                         rating = float(rating_match.group(1))
#                     else:
#                         num_match = re.search(r"(?:rating|score)[:\s]*(\d+\.?\d*)", text, re.I)
#                         if num_match:
#                             val = float(num_match.group(1))
#                             rating = val if val <= 5 else val / 20.0

#                 # Extract job title
#                 job_title = ""
#                 job_el = card.find(class_=re.compile(r"job.?title|position|role", re.I))
#                 if job_el:
#                     job_title = job_el.get_text(strip=True)
#                 else:
#                     for tag in card.find_all(["strong", "b", "span"]):
#                         t = tag.get_text(strip=True)
#                         if 3 < len(t) < 60 and "review" not in t.lower():
#                             job_title = t
#                             break

#                 # Extract date
#                 review_date = None
#                 date_el = card.find("time") or card.find(class_=re.compile(r"date", re.I))
#                 if date_el:
#                     raw_d = date_el.get("datetime") or date_el.get("content") or date_el.get_text(strip=True)
#                     review_date = _normalize_date(raw_d)

#                 reviews.append(CultureReview(
#                     review_id=f"careerbliss_{ticker}_{i}",
#                     rating=min(5.0, max(1.0, rating)),
#                     title=review_text[:100], pros=review_text[:2000], cons="",
#                     is_current_employee=True, job_title=job_title[:100],
#                     review_date=review_date, source="careerbliss",
#                 ))

#             logger.info(f"[{ticker}][careerbliss] Extracted {len(reviews)} reviews")

#         except Exception as e:
#             logger.warning(f"[{ticker}][careerbliss] Error: {e}")
#             try:
#                 page.close()
#             except Exception:
#                 pass

#         return reviews[:limit]

#     # ── CACHING ─────────────────────────────────────────────

#     def _cache_path(self, ticker, source):
#         return self.cache_dir / f"{ticker.upper()}_{source}.json"

#     def _save_cache(self, ticker, source, reviews):
#         p = self._cache_path(ticker, source)
#         try:
#             data = []
#             for r in reviews:
#                 data.append({
#                     "review_id": r.review_id, "rating": r.rating,
#                     "title": r.title, "pros": r.pros, "cons": r.cons,
#                     "advice_to_management": r.advice_to_management,
#                     "is_current_employee": r.is_current_employee,
#                     "job_title": r.job_title,
#                     "review_date": r.review_date.isoformat() if r.review_date else None,
#                     "source": r.source,
#                 })
#             p.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")
#             logger.info(f"[{ticker}][{source}] Cached {len(reviews)} reviews -> {p}")
#         except Exception as e:
#             logger.warning(f"[{ticker}][{source}] Cache save failed: {e}")

#     def _load_cache(self, ticker, source):
#         p = self._cache_path(ticker, source)
#         if not p.exists():
#             return None
#         try:
#             data = json.loads(p.read_text(encoding="utf-8"))
#             reviews = []
#             for d in data:
#                 rd = None
#                 if d.get("review_date"):
#                     try:
#                         rd = datetime.fromisoformat(d["review_date"])
#                     except (ValueError, TypeError):
#                         rd = None
#                 reviews.append(CultureReview(
#                     review_id=d["review_id"], rating=d["rating"],
#                     title=d["title"], pros=d["pros"], cons=d["cons"],
#                     advice_to_management=d.get("advice_to_management"),
#                     is_current_employee=d.get("is_current_employee", True),
#                     job_title=d.get("job_title", ""),
#                     review_date=rd, source=d.get("source", source),
#                 ))
#             logger.info(f"[{ticker}][{source}] Loaded {len(reviews)} from cache")
#             return reviews
#         except Exception as e:
#             logger.warning(f"[{ticker}][{source}] Cache load failed: {e}")
#             return None

#     # ── MULTI-SOURCE FETCH ──────────────────────────────────

#     # Review count targets
#     MIN_REVIEWS = 100
#     MAX_REVIEWS = 200

#     def fetch_all_reviews(self, ticker, sources, max_pages=3, use_cache=True):
#         ticker = ticker.upper()
#         all_reviews = []
#         num_sources = len(sources)

#         # Distribute target evenly across sources, with headroom
#         per_source_target = self.MAX_REVIEWS // max(num_sources, 1)

#         for source in sources:
#             if use_cache:
#                 cached = self._load_cache(ticker, source)
#                 if cached is not None:
#                     all_reviews.extend(cached[:per_source_target])
#                     continue
#             revs = []
#             try:
#                 if source == "glassdoor":
#                     # ~10 per page, so for 67 reviews need ~7 pages
#                     pages_needed = max(max_pages, (per_source_target // 10) + 1)
#                     revs = self.fetch_glassdoor(ticker, max_pages=pages_needed)
#                 elif source == "indeed":
#                     revs = self.scrape_indeed(ticker, limit=per_source_target)
#                 elif source == "careerbliss":
#                     revs = self.scrape_careerbliss(ticker, limit=per_source_target)
#                 else:
#                     logger.warning(f"[{ticker}] Unknown source: {source}")
#                     continue
#             except Exception as e:
#                 logger.error(f"[{ticker}][{source}] FAILED: {e}")
#             if revs:
#                 self._save_cache(ticker, source, revs)
#             all_reviews.extend(revs[:per_source_target])

#         total = len(all_reviews)

#         # Cap at MAX_REVIEWS
#         if total > self.MAX_REVIEWS:
#             logger.info(f"[{ticker}] Capping reviews from {total} to {self.MAX_REVIEWS}")
#             all_reviews = all_reviews[:self.MAX_REVIEWS]

#         # Warn if below MIN_REVIEWS
#         if total < self.MIN_REVIEWS:
#             logger.warning(
#                 f"[{ticker}] Only {total} reviews collected (minimum target: {self.MIN_REVIEWS}). "
#                 f"Score confidence may be lower."
#             )

#         logger.info(
#             f"[{ticker}] Total reviews collected: {len(all_reviews)} "
#             f"(target: {self.MIN_REVIEWS}-{self.MAX_REVIEWS})"
#         )
#         return all_reviews

#     # ── SCORING ─────────────────────────────────────────────

#     def analyze_reviews(self, company_id, ticker, reviews):
#         if not reviews:
#             logger.warning(f"[{ticker}] No reviews to analyze")
#             return CultureSignal(company_id=company_id, ticker=ticker)

#         # ── Pre-processing: deduplicate and filter noise ────
#         original_count = len(reviews)
#         reviews = self._deduplicate_reviews(reviews)

#         # Detect and flag Indeed page dumps
#         page_dump_ids = set()
#         for r in reviews:
#             if r.source == "indeed" and self._is_indeed_page_dump(r):
#                 page_dump_ids.add(r.review_id)

#         if page_dump_ids:
#             logger.info(f"[{ticker}] Detected {len(page_dump_ids)} Indeed page-dump "
#                         f"reviews (will be excluded from scoring)")

#         # Filter out page dumps entirely — they are noise, not reviews
#         reviews = [r for r in reviews if r.review_id not in page_dump_ids]

#         logger.info(f"[{ticker}] Reviews after cleaning: {len(reviews)} "
#                     f"(original: {original_count}, "
#                     f"deduped+filtered: {original_count - len(reviews)} removed)")

#         if not reviews:
#             logger.warning(f"[{ticker}] No reviews remaining after cleaning")
#             return CultureSignal(company_id=company_id, ticker=ticker)

#         # ── Main scoring loop ───────────────────────────────
#         # FIX: Use binary per-review presence (not per-keyword accumulation)
#         # Each review contributes its weight AT MOST ONCE per category,
#         # regardless of how many keywords from that category match.
#         # This follows CS3 spec: mentions / total_reviews * scale

#         inn_pos = inn_neg = Decimal("0")
#         dd = ai_m = Decimal("0")
#         ch_pos = ch_neg = Decimal("0")
#         total_w = Decimal("0")
#         rating_sum = 0.0
#         current_count = 0
#         pos_kw = []
#         neg_kw = []
#         src_counts = {}
#         now = datetime.now(timezone.utc)

#         for idx, r in enumerate(reviews):
#             text = f"{r.pros} {r.cons}".lower()
#             if r.advice_to_management:
#                 text += f" {r.advice_to_management}".lower()

#             # FIX: Also include job title in search text for AI/tech signals
#             # Employees at AI companies often have AI-relevant job titles
#             # that reflect the company's AI culture even when review text
#             # discusses WLB/pay/management instead.
#             job_title_lower = r.job_title.lower() if r.job_title else ""

#             days_old = (now - r.review_date).days if r.review_date else -1
#             rec_w = Decimal("1.0") if days_old < 730 else Decimal("0.5")
#             emp_w = Decimal("1.2") if r.is_current_employee else Decimal("1.0")
#             src_w = self.SOURCE_RELIABILITY.get(r.source, Decimal("0.70"))
#             w = rec_w * emp_w * src_w
#             total_w += w
#             rating_sum += r.rating
#             if r.is_current_employee:
#                 current_count += 1
#             src_counts[r.source] = src_counts.get(r.source, 0) + 1

#             review_hits = []

#             # ── Innovation Positive (binary per review) ─────
#             inn_pos_hit = False
#             for kw in self.INNOVATION_POSITIVE:
#                 if self._keyword_in_text(kw, text):
#                     review_hits.append(f"+innov:{kw}")
#                     if kw not in pos_kw:
#                         pos_kw.append(kw)
#                     inn_pos_hit = True
#             if inn_pos_hit:
#                 inn_pos += w

#             # ── Innovation Negative (binary, with context check) ─
#             inn_neg_hit = False
#             for kw in self.INNOVATION_NEGATIVE:
#                 if self._keyword_in_context(kw, text):
#                     review_hits.append(f"-innov:{kw}")
#                     if kw not in neg_kw:
#                         neg_kw.append(kw)
#                     inn_neg_hit = True
#             if inn_neg_hit:
#                 inn_neg += w

#             # ── Data-Driven (binary per review) ─────────────
#             dd_hit = False
#             for kw in self.DATA_DRIVEN_KEYWORDS:
#                 if self._keyword_in_text(kw, text):
#                     review_hits.append(f"+data:{kw}")
#                     dd_hit = True
#             if dd_hit:
#                 dd += w

#             # ── AI Awareness (binary per review) ────────────
#             # FIX: Search both review text AND job title
#             ai_hit = False
#             for kw in self.AI_AWARENESS_KEYWORDS:
#                 if self._keyword_in_context(kw, text):
#                     review_hits.append(f"+ai:{kw}")
#                     ai_hit = True
#                 elif self._keyword_in_text(kw, job_title_lower):
#                     review_hits.append(f"+ai(title):{kw}")
#                     ai_hit = True
#             if ai_hit:
#                 ai_m += w

#             # ── Change Positive (binary per review) ─────────
#             ch_pos_hit = False
#             for kw in self.CHANGE_POSITIVE:
#                 if self._keyword_in_text(kw, text):
#                     review_hits.append(f"+change:{kw}")
#                     if kw not in pos_kw:
#                         pos_kw.append(kw)
#                     ch_pos_hit = True
#             if ch_pos_hit:
#                 ch_pos += w

#             # ── Change Negative (binary, with context check) ─
#             ch_neg_hit = False
#             for kw in self.CHANGE_NEGATIVE:
#                 if self._keyword_in_context(kw, text):
#                     review_hits.append(f"-change:{kw}")
#                     if kw not in neg_kw:
#                         neg_kw.append(kw)
#                     ch_neg_hit = True
#             if ch_neg_hit:
#                 ch_neg += w

#             current_tag = "current" if r.is_current_employee else "former"
#             hits_str = ", ".join(review_hits) if review_hits else "(no keyword hits)"
#             logger.debug(
#                 f"[{ticker}] Review #{idx+1} [{r.source}] "
#                 f"rating={r.rating} {current_tag} "
#                 f"days_old={days_old} "
#                 f"rec_w={rec_w} emp_w={emp_w} src_w={src_w} w={w} | "
#                 f"{hits_str}"
#             )

#         # ── Calculate component scores ──────────────────────
#         # CS3 spec formulas:
#         #   innovation_score = (pos_mentions - neg_mentions) / total_reviews * 50 + 50
#         #   data_driven_score = data_mentions / total_reviews * 100
#         #   ai_awareness_score = ai_mentions / total_reviews * 100
#         #   change_readiness = (pos_change - neg_change) / total * 50 + 50
#         #   Overall = 0.30*innovation + 0.25*data_driven + 0.25*ai_awareness + 0.20*change

#         if total_w > 0:
#             inn_s = (inn_pos - inn_neg) / total_w * 50 + 50
#             dd_s = dd / total_w * 100
#             ai_s = ai_m / total_w * 100
#             ch_s = (ch_pos - ch_neg) / total_w * 50 + 50
#         else:
#             inn_s = Decimal("50")
#             dd_s = Decimal("0")
#             ai_s = Decimal("0")
#             ch_s = Decimal("50")

#         c = lambda v: max(Decimal("0"), min(Decimal("100"), v))
#         inn_s, dd_s, ai_s, ch_s = c(inn_s), c(dd_s), c(ai_s), c(ch_s)

#         overall = (
#             Decimal("0.30") * inn_s + Decimal("0.25") * dd_s
#             + Decimal("0.25") * ai_s + Decimal("0.20") * ch_s
#         ).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)

#         conf = min(Decimal("0.5") + Decimal(str(len(reviews))) / 200, Decimal("0.90"))
#         source_bonus = min(Decimal(str(len(src_counts))) * Decimal("0.03"), Decimal("0.10"))
#         conf = min(conf + source_bonus, Decimal("0.95"))

#         avg_rating = Decimal(str(round(rating_sum / len(reviews), 2)))
#         current_ratio = Decimal(str(round(current_count / len(reviews), 3)))

#         logger.info(f"[{ticker}] {'=' * 44}")
#         logger.info(f"[{ticker}]   SCORING SUMMARY")
#         logger.info(f"[{ticker}] {'=' * 44}")
#         logger.info(f"[{ticker}]   Reviews analyzed:       {len(reviews)}")
#         logger.info(f"[{ticker}]   Sources:                {src_counts}")
#         logger.info(f"[{ticker}]   Total weight:           {total_w}")
#         logger.info(f"[{ticker}]   Avg rating (raw):       {avg_rating}/5.0")
#         logger.info(f"[{ticker}]   Current employees:      {current_count}/{len(reviews)} ({current_ratio})")
#         logger.info(f"[{ticker}]   -- Raw accumulators --")
#         logger.info(f"[{ticker}]     innov_pos={inn_pos}  innov_neg={inn_neg}  net={inn_pos - inn_neg}")
#         logger.info(f"[{ticker}]     data_mentions={dd}")
#         logger.info(f"[{ticker}]     ai_mentions={ai_m}")
#         logger.info(f"[{ticker}]     change_pos={ch_pos}  change_neg={ch_neg}  net={ch_pos - ch_neg}")
#         logger.info(f"[{ticker}]   -- Component scores --")
#         logger.info(f"[{ticker}]     Innovation:       {inn_s.quantize(Decimal('0.01'))}  (x0.30 -> {(Decimal('0.30') * inn_s).quantize(Decimal('0.01'))})")
#         logger.info(f"[{ticker}]     Data-Driven:      {dd_s.quantize(Decimal('0.01'))}  (x0.25 -> {(Decimal('0.25') * dd_s).quantize(Decimal('0.01'))})")
#         logger.info(f"[{ticker}]     AI Awareness:     {ai_s.quantize(Decimal('0.01'))}  (x0.25 -> {(Decimal('0.25') * ai_s).quantize(Decimal('0.01'))})")
#         logger.info(f"[{ticker}]     Change Readiness: {ch_s.quantize(Decimal('0.01'))}  (x0.20 -> {(Decimal('0.20') * ch_s).quantize(Decimal('0.01'))})")
#         logger.info(f"[{ticker}]   -- Final --")
#         logger.info(f"[{ticker}]     OVERALL SCORE:    {overall}/100")
#         logger.info(f"[{ticker}]     Confidence:       {conf.quantize(Decimal('0.001'))} (base + {source_bonus} source bonus)")
#         if pos_kw:
#             logger.info(f"[{ticker}]     (+) Keywords: {', '.join(pos_kw)}")
#         if neg_kw:
#             logger.info(f"[{ticker}]     (-) Keywords: {', '.join(neg_kw)}")
#         logger.info(f"[{ticker}] {'=' * 44}")

#         return CultureSignal(
#             company_id=company_id, ticker=ticker,
#             innovation_score=inn_s.quantize(Decimal("0.01")),
#             data_driven_score=dd_s.quantize(Decimal("0.01")),
#             change_readiness_score=ch_s.quantize(Decimal("0.01")),
#             ai_awareness_score=ai_s.quantize(Decimal("0.01")),
#             overall_score=overall, review_count=len(reviews),
#             avg_rating=avg_rating, current_employee_ratio=current_ratio,
#             confidence=conf.quantize(Decimal("0.001")),
#             source_breakdown=src_counts,
#             positive_keywords_found=pos_kw, negative_keywords_found=neg_kw,
#         )

#     # ── S3 UPLOAD ─────────────────────────────────────────────

#     def _get_s3_service(self):
#         """Initialize S3 client directly using .env credentials."""
#         if not hasattr(self, '_s3_client'):
#             try:
#                 import boto3
#                 bucket = os.getenv("S3_BUCKET", "")
#                 key_id = os.getenv("AWS_ACCESS_KEY_ID", "")
#                 secret = os.getenv("AWS_SECRET_ACCESS_KEY", "")
#                 region = os.getenv("AWS_REGION", "us-east-1")

#                 if not bucket or not key_id or not secret:
#                     logger.warning(
#                         "S3 not configured. Set S3_BUCKET, AWS_ACCESS_KEY_ID, "
#                         "AWS_SECRET_ACCESS_KEY in .env"
#                     )
#                     self._s3_client = None
#                     self._s3_bucket = None
#                     return None

#                 self._s3_client = boto3.client(
#                     's3',
#                     aws_access_key_id=key_id,
#                     aws_secret_access_key=secret,
#                     region_name=region,
#                 )
#                 self._s3_bucket = bucket
#                 logger.info(f"S3 initialized: bucket={bucket}, region={region}")
#             except Exception as e:
#                 logger.error(f"S3 initialization failed: {e}")
#                 self._s3_client = None
#                 self._s3_bucket = None
#         return self._s3_client

#     def _upload_raw_to_s3(self, ticker, reviews):
#         """Upload raw reviews to S3: glassdoor_signals/raw/{TICKER}_raw.json"""
#         client = self._get_s3_service()
#         if not client:
#             return None
#         ticker = ticker.upper()
#         raw_data = []
#         for r in reviews:
#             raw_data.append({
#                 "review_id": r.review_id, "rating": r.rating,
#                 "title": r.title, "pros": r.pros, "cons": r.cons,
#                 "advice_to_management": r.advice_to_management,
#                 "is_current_employee": r.is_current_employee,
#                 "job_title": r.job_title,
#                 "review_date": r.review_date.isoformat() if r.review_date else None,
#                 "source": r.source,
#             })
#         s3_key = f"glassdoor_signals/raw/{ticker}_raw.json"
#         payload = json.dumps(
#             {"ticker": ticker, "review_count": len(raw_data), "reviews": raw_data},
#             indent=2, default=str,
#         )
#         try:
#             client.put_object(
#                 Bucket=self._s3_bucket, Key=s3_key,
#                 Body=payload.encode("utf-8"),
#                 ContentType="application/json",
#             )
#             logger.info(f"[{ticker}] Uploaded {len(raw_data)} raw reviews to S3: {s3_key}")
#             return s3_key
#         except Exception as e:
#             logger.error(f"[{ticker}] S3 raw upload failed: {e}")
#             return None

#     def _upload_output_to_s3(self, signal):
#         """Upload scored output to S3: glassdoor_signals/output/{TICKER}_culture.json"""
#         client = self._get_s3_service()
#         if not client:
#             return None
#         ticker = signal.ticker.upper()
#         output_data = asdict(signal)
#         for k, v in output_data.items():
#             if isinstance(v, Decimal):
#                 output_data[k] = float(v)
#         s3_key = f"glassdoor_signals/output/{ticker}_culture.json"
#         payload = json.dumps(output_data, indent=2, default=str)
#         try:
#             client.put_object(
#                 Bucket=self._s3_bucket, Key=s3_key,
#                 Body=payload.encode("utf-8"),
#                 ContentType="application/json",
#             )
#             logger.info(f"[{ticker}] Uploaded culture signal to S3: {s3_key}")
#             return s3_key
#         except Exception as e:
#             logger.error(f"[{ticker}] S3 output upload failed: {e}")
#             return None

#     # ── MAIN ENTRY POINTS ──────────────────────────────────

#     def collect_and_analyze(self, ticker, sources=None, max_pages=3, use_cache=True):
#         ticker = validate_ticker(ticker)
#         if sources is None:
#             sources = ["glassdoor", "indeed", "careerbliss"]
#         sources = [s for s in sources if s in VALID_SOURCES]
#         reg = COMPANY_REGISTRY[ticker]
#         logger.info(f"{'=' * 55}")
#         logger.info(f"CULTURE COLLECTION: {ticker} ({reg['name']})")
#         logger.info(f"   Sector: {reg['sector']}")
#         logger.info(f"   Sources: {', '.join(sources)}")
#         logger.info(f"{'=' * 55}")

#         reviews = self.fetch_all_reviews(ticker, sources=sources, max_pages=max_pages, use_cache=use_cache)
#         signal = self.analyze_reviews(ticker, ticker, reviews)

#         # Upload to S3 only
#         self._upload_raw_to_s3(ticker, reviews)
#         self._upload_output_to_s3(signal)

#         return signal

#     def collect_multiple(self, tickers, sources=None, max_pages=3, use_cache=True, delay=2.0):
#         results = {}
#         try:
#             for i, ticker in enumerate(tickers):
#                 try:
#                     signal = self.collect_and_analyze(ticker, sources=sources, max_pages=max_pages, use_cache=use_cache)
#                     results[ticker.upper()] = signal
#                 except Exception as e:
#                     logger.error(f"[{ticker}] FAILED: {e}")
#                 if i < len(tickers) - 1:
#                     logger.info(f"Waiting {delay}s before next ticker...")
#                     time.sleep(delay)
#         finally:
#             self.close_browser()
#         return results

#     def _save_signal(self, signal):
#         d = Path("results")
#         d.mkdir(parents=True, exist_ok=True)
#         p = d / f"{signal.ticker.lower()}_culture.json"
#         p.write_text(signal.to_json(), encoding="utf-8")
#         logger.info(f"[{signal.ticker}] Saved -> {p}")


# # =====================================================================
# # DISPLAY
# # =====================================================================

# def print_signal(signal):
#     reg = COMPANY_REGISTRY.get(signal.ticker, {})
#     name = reg.get("name", signal.ticker)
#     sector = reg.get("sector", "")
#     print(f"\n{'=' * 60}")
#     print(f"  CULTURE ANALYSIS -- {signal.ticker} ({name})")
#     if sector:
#         print(f"  Sector: {sector}")
#     print(f"{'=' * 60}")
#     print(f"  Overall Score:          {signal.overall_score}/100")
#     print(f"  Confidence:             {signal.confidence}")
#     print(f"  Reviews Analyzed:       {signal.review_count}")
#     print(f"  Source Breakdown:       {signal.source_breakdown}")
#     print(f"  Avg Rating:             {signal.avg_rating}/5.0")
#     print(f"  Current Employee Ratio: {signal.current_employee_ratio}")
#     print()
#     print(f"  Component Scores:       Weight   Score")
#     print(f"    Innovation:           0.30   {signal.innovation_score:>8}")
#     print(f"    Data-Driven:          0.25   {signal.data_driven_score:>8}")
#     print(f"    AI Awareness:         0.25   {signal.ai_awareness_score:>8}")
#     print(f"    Change Readiness:     0.20   {signal.change_readiness_score:>8}")
#     if signal.positive_keywords_found:
#         print(f"\n  (+) Keywords: {', '.join(signal.positive_keywords_found[:10])}")
#     if signal.negative_keywords_found:
#         print(f"  (-) Keywords: {', '.join(signal.negative_keywords_found[:10])}")


# # =====================================================================
# # MAIN
# # =====================================================================

# def main():
#     args = sys.argv[1:]
#     use_cache = "--no-cache" not in args

#     sources = None
#     for a in args:
#         if a.startswith("--sources="):
#             sources = [s.strip() for s in a.split("=", 1)[1].split(",")]

#     clean_args = [a for a in args if not a.startswith("-")]

#     if "--all" in args:
#         tickers = all_tickers()
#         glassdoor_active = sources is None or "glassdoor" in sources
#         est = len(tickers) * 3 if glassdoor_active else 0
#         print(f"\n  Running ALL {len(tickers)} tickers.")
#         if glassdoor_active:
#             print(f"   Estimated Glassdoor API calls: ~{est} (free tier = 500/month)")
#         print(f"   Tickers: {', '.join(tickers)}\n")
#         confirm = input("   Continue? [y/N]: ").strip().lower()
#         if confirm != "y":
#             print("Aborted.")
#             return
#     elif clean_args:
#         tickers = []
#         for t in clean_args:
#             try:
#                 tickers.append(validate_ticker(t))
#             except ValueError as e:
#                 print(f"ERROR: {e}")
#                 return
#     else:
#         print("\n" + "=" * 58)
#         print("  Multi-Source Culture Collector (CS3)")
#         print("=" * 58)
#         print()
#         print("  Usage:")
#         print("    python -m pipelines.culture_collector <TICKER> [TICKER ...]")
#         print("    python -m pipelines.culture_collector --all")
#         print()
#         print("  Options:")
#         print("    --no-cache                       Skip cached reviews")
#         print("    --sources=glassdoor,indeed,careerbliss  Pick sources")
#         print()
#         print("  Source Reliability:")
#         print("    Glassdoor    0.85  (via RapidAPI)")
#         print("    Indeed       0.80  (via Playwright)")
#         print("    CareerBliss  0.75  (via Playwright)")
#         print()
#         print("  Allowed tickers (13):")
#         for t in sorted(ALLOWED_TICKERS):
#             reg = COMPANY_REGISTRY[t]
#             print(f"    {t:<6} {reg['name']:<30} {reg['sector']}")
#         print()
#         print("  RapidAPI free tier = 500 requests/month.")
#         print("  Each ticker uses ~3 Glassdoor API calls.")
#         print("=" * 58)
#         return

#     glassdoor_active = sources is None or "glassdoor" in sources
#     if glassdoor_active:
#         est = len(tickers) * 3
#         print(f"\n  Glassdoor API calls: ~{est} (3 pages x {len(tickers)} tickers)")
#     print(f"  Tickers: {', '.join(tickers)}")
#     if sources:
#         print(f"  Sources: {', '.join(sources)}")
#     print()

#     collector = CultureCollector()
#     results = collector.collect_multiple(tickers, sources=sources, use_cache=use_cache, delay=2.0)

#     print("\n\n" + "#" * 60)
#     print(f"#  MULTI-SOURCE CULTURE ANALYSIS -- {len(results)} companies")
#     print("#" * 60)

#     for ticker, signal in results.items():
#         print_signal(signal)

#     if len(results) > 1:
#         print(f"\n{'=' * 62}")
#         print(
#             f"  {'Ticker':<6} {'Overall':>8} {'Innov':>7} "
#             f"{'Data':>7} {'AI':>7} {'Change':>7} {'#Rev':>5} {'Conf':>6}"
#         )
#         print(f"  {'-'*6} {'-'*8} {'-'*7} {'-'*7} {'-'*7} {'-'*7} {'-'*5} {'-'*6}")
#         for t, s in sorted(results.items(), key=lambda x: x[1].overall_score, reverse=True):
#             print(
#                 f"  {t:<6} {s.overall_score:>8} {s.innovation_score:>7} "
#                 f"{s.data_driven_score:>7} {s.ai_awareness_score:>7} "
#                 f"{s.change_readiness_score:>7} {s.review_count:>5} {s.confidence:>6}"
#             )
#         print(f"{'=' * 62}")


# if __name__ == "__main__":
#     main()



# app/pipelines/culture_collector.py
"""
Multi-Source Culture Collector (CS3)

What this version changes (per your request):
- NO hard cap on number of reviews collected (no MAX_REVIEWS slicing).
- NO per-source target slicing.
- Glassdoor/Indeed/CareerBliss now collect "as much as possible" up to a
  configurable max page/click guardrail (to avoid bans/quota burn).
- CLI flags added to control depth:
    --gd-pages=N
    --indeed-pages=N
    --cb-clicks=N
    --no-cache
    --sources=glassdoor,indeed,careerbliss
    --all

IMPORTANT:
- Glassdoor uses RapidAPI: raising gd-pages can burn your quota fast.
- Indeed/CareerBliss scraping too deep can trigger blocking/captcha.
"""

import json
import logging
import os
import re
import sys
import time
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone, timedelta
from decimal import Decimal, ROUND_HALF_UP
from pathlib import Path
from typing import Any, Dict, List, Optional

import httpx
from dotenv import load_dotenv

# ---------------------------------------------------------------------
# PATH + ENV
# ---------------------------------------------------------------------
_THIS_FILE = Path(__file__).resolve()
_APP_DIR = _THIS_FILE.parent.parent
_PROJECT_ROOT = _APP_DIR.parent
for _p in [str(_PROJECT_ROOT), str(_APP_DIR)]:
    if _p not in sys.path:
        sys.path.insert(0, _p)

load_dotenv(_PROJECT_ROOT / ".env")

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger(__name__)

logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("boto3").setLevel(logging.WARNING)
logging.getLogger("botocore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("s3transfer").setLevel(logging.WARNING)


# =====================================================================
# DATA MODELS
# =====================================================================

@dataclass
class CultureReview:
    review_id: str
    rating: float
    title: str
    pros: str
    cons: str
    advice_to_management: Optional[str] = None
    is_current_employee: bool = True
    job_title: str = ""
    review_date: Optional[datetime] = None
    source: str = "unknown"

    def __post_init__(self):
        if self.review_date is None:
            self.review_date = datetime.now(timezone.utc)


@dataclass
class CultureSignal:
    company_id: str
    ticker: str
    innovation_score: Decimal = Decimal("50.00")
    data_driven_score: Decimal = Decimal("0.00")
    change_readiness_score: Decimal = Decimal("50.00")
    ai_awareness_score: Decimal = Decimal("0.00")
    overall_score: Decimal = Decimal("25.00")
    review_count: int = 0
    avg_rating: Decimal = Decimal("0.00")
    current_employee_ratio: Decimal = Decimal("0.000")
    confidence: Decimal = Decimal("0.000")
    source_breakdown: Dict[str, int] = field(default_factory=dict)
    positive_keywords_found: List[str] = field(default_factory=list)
    negative_keywords_found: List[str] = field(default_factory=list)

    def to_json(self, indent=2) -> str:
        d = asdict(self)
        for k, v in d.items():
            if isinstance(v, Decimal):
                d[k] = float(v)
        return json.dumps(d, indent=indent, default=str)


# =====================================================================
# COMPANY REGISTRY (your current 5)
# =====================================================================

COMPANY_REGISTRY: Dict[str, Dict[str, Any]] = {
    "NVDA": {
        "name": "NVIDIA", "sector": "Technology",
        "glassdoor_id": "NVIDIA",
        "indeed_slugs": ["NVIDIA"],
        "careerbliss_slug": "nvidia",
    },
    "JPM": {
        "name": "JPMorgan Chase", "sector": "Financial Services",
        "glassdoor_id": "JPMorgan-Chase",
        "indeed_slugs": ["JPMorgan-Chase", "jpmorgan-chase"],
        "careerbliss_slug": "jpmorgan-chase",
    },
    "WMT": {
        "name": "Walmart", "sector": "Consumer Retail",
        "glassdoor_id": "Walmart",
        "indeed_slugs": ["Walmart"],
        "careerbliss_slug": "walmart",
    },
    "GE": {
        "name": "GE Aerospace", "sector": "Industrials Manufacturing",
        "glassdoor_id": "GE-Aerospace",
        "indeed_slugs": ["GE-Aerospace", "General-Electric"],
        "careerbliss_slug": "ge-aerospace",
    },
    "DG": {
        "name": "Dollar General", "sector": "Consumer Retail",
        "glassdoor_id": "Dollar-General",
        "indeed_slugs": ["Dollar-General"],
        "careerbliss_slug": "dollar-general",
    },
}

ALLOWED_TICKERS = set(COMPANY_REGISTRY.keys())
VALID_SOURCES = {"glassdoor", "indeed", "careerbliss"}


def validate_ticker(ticker: str) -> str:
    t = ticker.upper()
    if t not in ALLOWED_TICKERS:
        raise ValueError(f"Unknown ticker '{t}'. Allowed: {', '.join(sorted(ALLOWED_TICKERS))}")
    return t


def all_tickers() -> List[str]:
    return sorted(ALLOWED_TICKERS)


# =====================================================================
# HELPERS
# =====================================================================

def _normalize_date(raw: Optional[str]) -> Optional[datetime]:
    if not raw:
        return None
    raw = raw.strip()

    iso = re.match(r"(\d{4}-\d{2}-\d{2})", raw)
    if iso:
        try:
            return datetime.strptime(iso.group(1), "%Y-%m-%d").replace(tzinfo=timezone.utc)
        except ValueError:
            pass

    for fmt in ("%B %d, %Y", "%b %d, %Y", "%d %B %Y", "%d %b %Y",
                "%m/%d/%Y", "%d/%m/%Y", "%Y/%m/%d"):
        try:
            return datetime.strptime(raw, fmt).replace(tzinfo=timezone.utc)
        except ValueError:
            continue

    rel = re.match(r"(\d+)\s+(day|week|month|year)s?\s+ago", raw, re.I)
    if rel:
        num = int(rel.group(1))
        unit = rel.group(2).lower()
        days = {"day": 1, "week": 7, "month": 30, "year": 365}[unit]
        return datetime.now(timezone.utc) - timedelta(days=num * days)

    return None


# =====================================================================
# CULTURE COLLECTOR
# =====================================================================

class CultureCollector:
    # -----------------------------------------------------------------
    # CONFIG: remove hard caps, keep safety guardrails (adjust via CLI)
    # -----------------------------------------------------------------
    MAX_REVIEWS_TOTAL: Optional[int] = None
    MAX_REVIEWS_PER_SOURCE: Optional[int] = None

    DEFAULT_MAX_GLASSDOOR_PAGES = 30     # RapidAPI quota guardrail
    DEFAULT_MAX_INDEED_PAGES = 25        # anti-block guardrail
    DEFAULT_MAX_CAREERBLISS_CLICKS = 15  # anti-block guardrail

    SOURCE_RELIABILITY = {
        "glassdoor":   Decimal("0.85"),
        "indeed":      Decimal("0.80"),
        "careerbliss": Decimal("0.75"),
        "unknown":     Decimal("0.70"),
    }

    RAPIDAPI_HOST = "real-time-glassdoor-data.p.rapidapi.com"
    RAPIDAPI_BASE = f"https://{RAPIDAPI_HOST}"

    # ---------------------- Keywords ----------------------
    INNOVATION_POSITIVE = [
        "innovative", "cutting-edge", "forward-thinking",
        "encourages new ideas", "experimental", "creative freedom",
        "startup mentality", "move fast", "disruptive",
        "innovation", "pioneering", "bleeding edge",
    ]
    INNOVATION_NEGATIVE = [
        "bureaucratic", "slow to change", "resistant",
        "outdated", "stuck in old ways", "red tape",
        "politics", "siloed", "hierarchical",
        "stagnant", "old-fashioned", "behind the times",
    ]
    DATA_DRIVEN_KEYWORDS = [
        "data-driven", "metrics", "evidence-based",
        "analytical", "kpis", "dashboards", "data culture",
        "measurement", "quantitative",
        "data informed", "analytics", "data-centric",
    ]
    AI_AWARENESS_KEYWORDS = [
        "ai", "artificial intelligence", "machine learning",
        "automation", "data science", "ml", "algorithms",
        "predictive", "neural network",
        "deep learning", "nlp", "llm", "generative ai",
        "chatbot", "computer vision",
    ]
    CHANGE_POSITIVE = [
        "agile", "adaptive", "fast-paced", "embraces change",
        "continuous improvement", "growth mindset",
        "evolving", "dynamic", "transforming",
    ]
    CHANGE_NEGATIVE = [
        "rigid", "traditional", "slow", "risk-averse",
        "change resistant", "old school",
        "inflexible", "set in their ways", "fear of change",
    ]

    WHOLE_WORD_KEYWORDS = [
        "ai", "ml", "nlp", "llm",
        "slow", "traditional", "rigid", "dynamic", "agile",
    ]

    KEYWORD_CONTEXT_EXCLUSIONS = {
        "slow": [
            r"slow\s+climb",
            r"slow\s+(?:career|promotion|advancement|growth|process|hiring|recruiting|interview)",
            r"(?:career|promotion|advancement|growth|process|hiring|recruiting|interview)\s+(?:is|are|was|were|seems?|feels?)\s+slow",
            r"slow\s+(?:to\s+)?(?:promote|hire|respond|reply|get\s+back)",
        ],
        "traditional": [
            r"traditional\s+(?:benefits|hours|schedule|shift|role)",
        ],
        "politics": [
            r"(?:office|internal|team)\s+politics",
        ],
        "automation": [
            r"(?:test|testing)\s+automation",
            r"automation\s+(?:test|engineer|qa)",
        ],
    }

    INDEED_NOISE_INDICATORS = [
        "slide 1 of",
        "slide 2 of",
        "see more jobs",
        "selecting an option will update the page",
        "report review copy link",
        "show more report review",
        "page 1 of 3",
        "days ago slide",
        "an hour",
    ]
    INDEED_NOISE_THRESHOLD = 3
    MAX_REVIEW_TEXT_LENGTH = 2000

    # ---------------------- Init ----------------------
    def __init__(self, cache_dir="data/culture_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._browser = None
        self._playwright = None

    # -----------------------------------------------------------------
    # Browser (Playwright) management
    # -----------------------------------------------------------------
    def _get_browser(self):
        if self._browser is None:
            from playwright.sync_api import sync_playwright
            self._playwright = sync_playwright().start()
            self._browser = self._playwright.chromium.launch(
                headless=True,
                args=[
                    "--disable-blink-features=AutomationControlled",
                    "--no-sandbox",
                    "--disable-dev-shm-usage",
                    "--disable-infobars",
                    "--window-size=1920,1080",
                ],
            )
            logger.info("Playwright browser launched")
        return self._browser

    def _new_page(self, stealth=True):
        browser = self._get_browser()
        ctx = browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
            ),
            viewport={"width": 1920, "height": 1080},
            locale="en-US",
            timezone_id="America/New_York",
            extra_http_headers={
                "Accept-Language": "en-US,en;q=0.9",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Upgrade-Insecure-Requests": "1",
            },
        )
        page = ctx.new_page()
        if stealth:
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
                Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
                window.chrome = { runtime: {} };
                const origQuery = window.navigator.permissions.query;
                window.navigator.permissions.query = (p) =>
                    p.name === 'notifications'
                        ? Promise.resolve({ state: Notification.permission })
                        : origQuery(p);
            """)
        page.route("**/*.{png,jpg,jpeg,gif,svg,webp,woff,woff2,ttf,mp4,webm}", lambda route: route.abort())
        return page

    def close_browser(self):
        if self._browser:
            self._browser.close()
            self._browser = None
        if self._playwright:
            self._playwright.stop()
            self._playwright = None
            logger.info("Playwright browser closed")

    # -----------------------------------------------------------------
    # Keyword helpers
    # -----------------------------------------------------------------
    def _keyword_in_text(self, kw: str, text: str) -> bool:
        if kw in self.WHOLE_WORD_KEYWORDS:
            return bool(re.search(r"\b" + re.escape(kw) + r"\b", text))
        return kw in text

    def _keyword_in_context(self, kw: str, text: str) -> bool:
        if not self._keyword_in_text(kw, text):
            return False
        exclusions = self.KEYWORD_CONTEXT_EXCLUSIONS.get(kw)
        if not exclusions:
            return True
        for pattern in exclusions:
            if re.search(pattern, text, re.IGNORECASE):
                return False
        return True

    def _is_indeed_page_dump(self, review: CultureReview) -> bool:
        text = f"{review.pros} {review.cons}".lower()
        noise_count = sum(1 for ind in self.INDEED_NOISE_INDICATORS if ind in text)
        if noise_count >= self.INDEED_NOISE_THRESHOLD:
            return True
        if len(text) > self.MAX_REVIEW_TEXT_LENGTH:
            date_pattern = re.findall(
                r"(?:january|february|march|april|may|june|july|august|"
                r"september|october|november|december)\s+\d{1,2},\s+\d{4}",
                text,
            )
            if len(date_pattern) >= 3:
                return True
        job_listing_signals = [
            r"\$\d+\s*-\s*\$\d+\s*an?\s*hour",
            r"\d+\s*days?\s*ago\s*slide",
            r"see more jobs",
        ]
        listing_count = sum(1 for p in job_listing_signals if re.search(p, text))
        if listing_count >= 2:
            return True
        return False

    def _deduplicate_reviews(self, reviews: List[CultureReview]) -> List[CultureReview]:
        seen = set()
        unique = []
        for r in reviews:
            content = f"{r.pros} {r.cons}".lower().strip()
            content = re.sub(r"\s+", " ", content)
            fingerprint = content[:150]
            if fingerprint in seen:
                continue
            seen.add(fingerprint)
            unique.append(r)
        removed = len(reviews) - len(unique)
        if removed > 0:
            logger.info(f"  Dedup removed {removed} duplicate reviews ({len(reviews)} -> {len(unique)})")
        return unique

    # -----------------------------------------------------------------
    # Glassdoor (RapidAPI)
    # -----------------------------------------------------------------
    def _get_api_key(self) -> str:
        key = os.getenv("RAPIDAPI_KEY", "")
        if not key:
            raise EnvironmentError("RAPIDAPI_KEY not set in .env")
        return key

    def _api_headers(self) -> Dict[str, str]:
        return {
            "x-rapidapi-key": self._get_api_key(),
            "x-rapidapi-host": self.RAPIDAPI_HOST,
        }

    def fetch_glassdoor(self, ticker: str, max_pages: int, timeout: float = 30.0) -> List[CultureReview]:
        ticker = ticker.upper()
        reg = COMPANY_REGISTRY[ticker]
        company_id = reg["glassdoor_id"]
        reviews: List[CultureReview] = []

        for page_num in range(1, max_pages + 1):
            params = {
                "company_id": company_id,
                "page": str(page_num),
                "sort": "POPULAR",
                "language": "en",
                "only_current_employees": "false",
                "extended_rating_data": "false",
                "domain": "www.glassdoor.com",
            }
            url = f"{self.RAPIDAPI_BASE}/company-reviews"
            logger.info(f"[{ticker}][glassdoor] Fetching page {page_num}...")

            try:
                resp = httpx.get(url, headers=self._api_headers(), params=params, timeout=timeout)
                resp.raise_for_status()
                raw_data = resp.json()
            except httpx.HTTPStatusError as e:
                logger.error(f"[{ticker}][glassdoor] HTTP {e.response.status_code} on page {page_num}")
                break
            except Exception as e:
                logger.error(f"[{ticker}][glassdoor] Request failed: {e}")
                break

            reviews_raw = raw_data.get("data", {}).get("reviews", [])
            if not reviews_raw:
                logger.info(f"[{ticker}][glassdoor] No more reviews at page {page_num}")
                break

            for r in reviews_raw:
                parsed = self._parse_glassdoor_review(ticker, r)
                if parsed:
                    reviews.append(parsed)

            # polite delay
            time.sleep(0.35)

        logger.info(f"[{ticker}][glassdoor] Total fetched: {len(reviews)}")
        return reviews

    def _parse_glassdoor_review(self, ticker: str, raw: Dict[str, Any]) -> Optional[CultureReview]:
        try:
            rid = f"glassdoor_{ticker}_{raw.get('review_id', 'unknown')}"
            rating = float(raw.get("rating", 3.0))
            title = raw.get("summary") or raw.get("title") or ""
            pros = raw.get("pros") or ""
            cons = raw.get("cons") or ""
            advice = raw.get("advice_to_management") or None
            job_title = raw.get("job_title") or ""
            is_current = bool(raw.get("is_current_employee", False))
            emp_status = raw.get("employment_status", "")
            if isinstance(emp_status, str) and emp_status.upper() == "REGULAR":
                is_current = True
            review_date = None
            raw_date = raw.get("review_datetime") or None
            if raw_date and isinstance(raw_date, str):
                review_date = _normalize_date(raw_date[:10])

            return CultureReview(
                review_id=rid,
                rating=min(5.0, max(1.0, rating)),
                title=title[:200],
                pros=pros[:2000],
                cons=cons[:2000],
                advice_to_management=advice,
                is_current_employee=is_current,
                job_title=job_title[:200],
                review_date=review_date,
                source="glassdoor",
            )
        except Exception as e:
            logger.warning(f"[{ticker}][glassdoor] Parse error: {e}")
            return None

    # -----------------------------------------------------------------
    # Indeed (Playwright + BeautifulSoup) - deeper pagination
    # -----------------------------------------------------------------
    def scrape_indeed(self, ticker: str, max_pages: int = 25) -> List[CultureReview]:
        from bs4 import BeautifulSoup

        ticker = ticker.upper()
        slugs = COMPANY_REGISTRY[ticker]["indeed_slugs"]
        reviews: List[CultureReview] = []

        for slug in slugs:
            for page_num in range(max_pages):
                start = page_num * 20
                url = f"https://www.indeed.com/cmp/{slug}/reviews"
                if page_num > 0:
                    url = f"{url}?start={start}"

                logger.info(f"[{ticker}][indeed] Scraping page {page_num + 1}/{max_pages}: {url}")

                page = None
                try:
                    page = self._new_page(stealth=True)
                    page.goto(url, wait_until="domcontentloaded", timeout=30000)
                    time.sleep(3)

                    title = page.title().lower()
                    if any(w in title for w in ["blocked", "captcha", "access denied"]):
                        logger.warning(f"[{ticker}][indeed] Blocked: {page.title()}")
                        page.close()
                        break

                    try:
                        page.wait_for_selector(
                            '[data-testid*="review"], .cmp-ReviewsList, [data-tn-component="reviewsList"]',
                            timeout=10000,
                        )
                    except Exception:
                        logger.info(f"[{ticker}][indeed] No review container on page {page_num + 1}")
                        page.close()
                        break

                    for _ in range(3):
                        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                        time.sleep(1)

                    html = page.content()
                    page.close()

                    soup = BeautifulSoup(html, "html.parser")
                    cards = (
                        soup.find_all("div", {"data-testid": re.compile(r"review")})
                        or soup.find_all("div", class_=re.compile(r"cmp-Review(?!sList)"))
                        or soup.find_all("div", class_=re.compile(r"review", re.I))
                    )
                    if not cards:
                        logger.info(f"[{ticker}][indeed] No cards on page {page_num + 1} -> stopping")
                        break

                    page_added = 0
                    for card in cards:
                        text = card.get_text(separator=" ", strip=True)
                        if len(text) < 30:
                            continue

                        title_el = card.find(["h2", "h3", "a", "span"], class_=re.compile(r"title|header", re.I))
                        title_text = title_el.get_text(strip=True) if title_el else text[:100]

                        rating = 3.0
                        star_el = card.find(attrs={"aria-label": re.compile(r"\d.*star", re.I)})
                        if star_el:
                            m = re.search(r"(\d+\.?\d*)", star_el.get("aria-label", ""))
                            if m:
                                rating = float(m.group(1))

                        pros_text, cons_text = "", ""
                        for label in card.find_all(string=re.compile(r"^Pros?$", re.I)):
                            p = label.find_parent()
                            if p and p.find_next_sibling():
                                pros_text = p.find_next_sibling().get_text(separator=" ", strip=True)
                        for label in card.find_all(string=re.compile(r"^Cons?$", re.I)):
                            p = label.find_parent()
                            if p and p.find_next_sibling():
                                cons_text = p.find_next_sibling().get_text(separator=" ", strip=True)

                        if not pros_text and not cons_text:
                            pros_text = text

                        date_el = card.find("time") or card.find(class_=re.compile(r"date", re.I))
                        review_date = None
                        if date_el:
                            raw_d = date_el.get("datetime") or date_el.get("content") or date_el.get_text(strip=True)
                            review_date = _normalize_date(raw_d)

                        is_current = "current" in text.lower() and "former" not in text.lower()

                        global_idx = len(reviews)
                        reviews.append(
                            CultureReview(
                                review_id=f"indeed_{ticker}_{global_idx}",
                                rating=min(5.0, max(1.0, rating)),
                                title=title_text[:200],
                                pros=pros_text[:2000],
                                cons=cons_text[:2000],
                                is_current_employee=is_current,
                                review_date=review_date,
                                source="indeed",
                            )
                        )
                        page_added += 1

                    logger.info(f"[{ticker}][indeed] Page {page_num + 1}: +{page_added} (total {len(reviews)})")

                    time.sleep(1.5)

                except Exception as e:
                    logger.warning(f"[{ticker}][indeed] Error page {page_num + 1} for slug '{slug}': {e}")
                    try:
                        if page:
                            page.close()
                    except Exception:
                        pass
                    break

            if reviews:
                logger.info(f"[{ticker}][indeed] Extracted {len(reviews)} reviews total")
                break

        return reviews

    # -----------------------------------------------------------------
    # CareerBliss (Playwright + BeautifulSoup) - deeper "More Reviews"
    # -----------------------------------------------------------------
    def scrape_careerbliss(self, ticker: str, max_clicks: int = 15) -> List[CultureReview]:
        from bs4 import BeautifulSoup

        ticker = ticker.upper()
        slug = COMPANY_REGISTRY[ticker]["careerbliss_slug"]
        reviews: List[CultureReview] = []
        dq = chr(34)

        url = f"https://www.careerbliss.com/{slug}/reviews/"
        logger.info(f"[{ticker}][careerbliss] Scraping: {url}")

        page = None
        try:
            page = self._new_page(stealth=True)
            page.goto(url, wait_until="domcontentloaded", timeout=30000)
            time.sleep(3)

            title_text = page.title().lower()
            if any(w in title_text for w in ["blocked", "denied", "captcha"]):
                logger.warning(f"[{ticker}][careerbliss] Blocked: {page.title()}")
                page.close()
                return reviews

            for _ in range(4):
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(1)

            # Click "More Reviews" repeatedly
            for i in range(max_clicks):
                try:
                    more = page.query_selector(
                        'a:has-text("More Reviews"), a:has-text("Show More"), '
                        'button:has-text("More"), a.next'
                    )
                    if more and more.is_visible():
                        more.click()
                        logger.info(f"[{ticker}][careerbliss] Clicked more ({i+1}/{max_clicks})")
                        time.sleep(2)
                        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                        time.sleep(1)
                    else:
                        break
                except Exception:
                    break

            html = page.content()
            page.close()

            soup = BeautifulSoup(html, "html.parser")
            cards = (
                soup.find_all("div", class_=re.compile(r"review", re.I))
                or soup.find_all("li", class_=re.compile(r"review", re.I))
                or soup.find_all("article")
            )

            seen = set()
            for i, card in enumerate(cards):
                text = card.get_text(separator=" ", strip=True)
                if len(text) < 30:
                    continue

                key = text[:80].lower()
                if key in seen:
                    continue
                seen.add(key)

                if any(bp in text.lower() for bp in [
                    "careerbliss", "share salary", "update your browser",
                    "search by job title", "browse salaries",
                ]):
                    continue

                review_text = ""
                for s in card.stripped_strings:
                    if s.startswith(dq) and s.endswith(dq) and len(s) > 30:
                        review_text = s.strip(dq)
                        break
                if not review_text:
                    for el in card.find_all(["p", "span", "div"]):
                        t = el.get_text(separator=" ", strip=True)
                        if len(t) > len(review_text) and len(t) > 30:
                            review_text = t
                if not review_text or len(review_text) < 20:
                    review_text = text

                rating = 3.0
                rating_el = card.find(attrs={"aria-label": re.compile(r"\d.*star", re.I)})
                if rating_el:
                    m = re.search(r"(\d+\.?\d*)", rating_el.get("aria-label", ""))
                    if m:
                        rating = float(m.group(1))
                else:
                    rating_match = re.search(r"(\d+\.?\d*)\s*(?:/|out of)\s*5", text)
                    if rating_match:
                        rating = float(rating_match.group(1))

                job_title = ""
                job_el = card.find(class_=re.compile(r"job.?title|position|role", re.I))
                if job_el:
                    job_title = job_el.get_text(strip=True)

                review_date = None
                date_el = card.find("time") or card.find(class_=re.compile(r"date", re.I))
                if date_el:
                    raw_d = date_el.get("datetime") or date_el.get("content") or date_el.get_text(strip=True)
                    review_date = _normalize_date(raw_d)

                reviews.append(
                    CultureReview(
                        review_id=f"careerbliss_{ticker}_{i}",
                        rating=min(5.0, max(1.0, rating)),
                        title=review_text[:100],
                        pros=review_text[:2000],
                        cons="",
                        is_current_employee=True,
                        job_title=job_title[:100],
                        review_date=review_date,
                        source="careerbliss",
                    )
                )

            logger.info(f"[{ticker}][careerbliss] Extracted {len(reviews)} reviews")

        except Exception as e:
            logger.warning(f"[{ticker}][careerbliss] Error: {e}")
            try:
                if page:
                    page.close()
            except Exception:
                pass

        return reviews

    # -----------------------------------------------------------------
    # Caching
    # -----------------------------------------------------------------
    def _cache_path(self, ticker: str, source: str) -> Path:
        return self.cache_dir / f"{ticker.upper()}_{source}.json"

    def _save_cache(self, ticker: str, source: str, reviews: List[CultureReview]) -> None:
        p = self._cache_path(ticker, source)
        try:
            data = []
            for r in reviews:
                data.append({
                    "review_id": r.review_id,
                    "rating": r.rating,
                    "title": r.title,
                    "pros": r.pros,
                    "cons": r.cons,
                    "advice_to_management": r.advice_to_management,
                    "is_current_employee": r.is_current_employee,
                    "job_title": r.job_title,
                    "review_date": r.review_date.isoformat() if r.review_date else None,
                    "source": r.source,
                })
            p.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")
            logger.info(f"[{ticker}][{source}] Cached {len(reviews)} reviews -> {p}")
        except Exception as e:
            logger.warning(f"[{ticker}][{source}] Cache save failed: {e}")

    def _load_cache(self, ticker: str, source: str) -> Optional[List[CultureReview]]:
        p = self._cache_path(ticker, source)
        if not p.exists():
            return None
        try:
            data = json.loads(p.read_text(encoding="utf-8"))
            reviews: List[CultureReview] = []
            for d in data:
                rd = None
                if d.get("review_date"):
                    try:
                        rd = datetime.fromisoformat(d["review_date"])
                    except (ValueError, TypeError):
                        rd = None
                reviews.append(
                    CultureReview(
                        review_id=d["review_id"],
                        rating=d["rating"],
                        title=d["title"],
                        pros=d["pros"],
                        cons=d["cons"],
                        advice_to_management=d.get("advice_to_management"),
                        is_current_employee=d.get("is_current_employee", True),
                        job_title=d.get("job_title", ""),
                        review_date=rd,
                        source=d.get("source", source),
                    )
                )
            logger.info(f"[{ticker}][{source}] Loaded {len(reviews)} from cache")
            return reviews
        except Exception as e:
            logger.warning(f"[{ticker}][{source}] Cache load failed: {e}")
            return None

    # -----------------------------------------------------------------
    # Multi-source fetch (NO slicing)
    # -----------------------------------------------------------------
    def fetch_all_reviews(
        self,
        ticker: str,
        sources: List[str],
        max_pages_glassdoor: Optional[int] = None,
        max_pages_indeed: Optional[int] = None,
        max_clicks_careerbliss: Optional[int] = None,
        use_cache: bool = True,
    ) -> List[CultureReview]:
        """
        Collect as many reviews as possible from all sources, up to guardrails.
        """
        ticker = ticker.upper()

        max_pages_glassdoor = max_pages_glassdoor or self.DEFAULT_MAX_GLASSDOOR_PAGES
        max_pages_indeed = max_pages_indeed or self.DEFAULT_MAX_INDEED_PAGES
        max_clicks_careerbliss = max_clicks_careerbliss or self.DEFAULT_MAX_CAREERBLISS_CLICKS

        all_reviews: List[CultureReview] = []

        for source in sources:
            if use_cache:
                cached = self._load_cache(ticker, source)
                if cached is not None:
                    all_reviews.extend(cached)
                    continue

            revs: List[CultureReview] = []
            try:
                if source == "glassdoor":
                    revs = self.fetch_glassdoor(ticker, max_pages=max_pages_glassdoor)
                elif source == "indeed":
                    revs = self.scrape_indeed(ticker, max_pages=max_pages_indeed)
                elif source == "careerbliss":
                    revs = self.scrape_careerbliss(ticker, max_clicks=max_clicks_careerbliss)
                else:
                    logger.warning(f"[{ticker}] Unknown source: {source}")
                    continue
            except Exception as e:
                logger.error(f"[{ticker}][{source}] FAILED: {e}")

            if revs:
                self._save_cache(ticker, source, revs)
                all_reviews.extend(revs)

        if self.MAX_REVIEWS_TOTAL is not None and len(all_reviews) > self.MAX_REVIEWS_TOTAL:
            all_reviews = all_reviews[: self.MAX_REVIEWS_TOTAL]

        logger.info(f"[{ticker}] Total reviews collected: {len(all_reviews)}")
        return all_reviews

    # -----------------------------------------------------------------
    # Scoring (unchanged logic; just uses more reviews now)
    # -----------------------------------------------------------------
    def analyze_reviews(self, company_id: str, ticker: str, reviews: List[CultureReview]) -> CultureSignal:
        if not reviews:
            logger.warning(f"[{ticker}] No reviews to analyze")
            return CultureSignal(company_id=company_id, ticker=ticker)

        original_count = len(reviews)
        reviews = self._deduplicate_reviews(reviews)

        page_dump_ids = set()
        for r in reviews:
            if r.source == "indeed" and self._is_indeed_page_dump(r):
                page_dump_ids.add(r.review_id)

        if page_dump_ids:
            logger.info(f"[{ticker}] Detected {len(page_dump_ids)} Indeed page-dump reviews (excluded)")

        reviews = [r for r in reviews if r.review_id not in page_dump_ids]

        logger.info(f"[{ticker}] Reviews after cleaning: {len(reviews)} (original {original_count})")

        if not reviews:
            logger.warning(f"[{ticker}] No reviews remaining after cleaning")
            return CultureSignal(company_id=company_id, ticker=ticker)

        inn_pos = inn_neg = Decimal("0")
        dd = ai_m = Decimal("0")
        ch_pos = ch_neg = Decimal("0")
        total_w = Decimal("0")
        rating_sum = 0.0
        current_count = 0
        pos_kw: List[str] = []
        neg_kw: List[str] = []
        src_counts: Dict[str, int] = {}
        now = datetime.now(timezone.utc)

        for idx, r in enumerate(reviews):
            text = f"{r.pros} {r.cons}".lower()
            if r.advice_to_management:
                text += f" {r.advice_to_management}".lower()
            job_title_lower = r.job_title.lower() if r.job_title else ""

            days_old = (now - r.review_date).days if r.review_date else -1
            rec_w = Decimal("1.0") if days_old < 730 else Decimal("0.5")
            emp_w = Decimal("1.2") if r.is_current_employee else Decimal("1.0")
            src_w = self.SOURCE_RELIABILITY.get(r.source, Decimal("0.70"))
            w = rec_w * emp_w * src_w
            total_w += w

            rating_sum += r.rating
            if r.is_current_employee:
                current_count += 1
            src_counts[r.source] = src_counts.get(r.source, 0) + 1

            # Innovation Positive (binary per review)
            inn_pos_hit = False
            for kw in self.INNOVATION_POSITIVE:
                if self._keyword_in_text(kw, text):
                    if kw not in pos_kw:
                        pos_kw.append(kw)
                    inn_pos_hit = True
            if inn_pos_hit:
                inn_pos += w

            # Innovation Negative (binary, context)
            inn_neg_hit = False
            for kw in self.INNOVATION_NEGATIVE:
                if self._keyword_in_context(kw, text):
                    if kw not in neg_kw:
                        neg_kw.append(kw)
                    inn_neg_hit = True
            if inn_neg_hit:
                inn_neg += w

            # Data-driven (binary per review)
            dd_hit = False
            for kw in self.DATA_DRIVEN_KEYWORDS:
                if self._keyword_in_text(kw, text):
                    dd_hit = True
            if dd_hit:
                dd += w

            # AI awareness (binary per review; text OR job title)
            ai_hit = False
            for kw in self.AI_AWARENESS_KEYWORDS:
                if self._keyword_in_context(kw, text):
                    ai_hit = True
                elif self._keyword_in_text(kw, job_title_lower):
                    ai_hit = True
            if ai_hit:
                ai_m += w

            # Change Positive (binary)
            ch_pos_hit = False
            for kw in self.CHANGE_POSITIVE:
                if self._keyword_in_text(kw, text):
                    if kw not in pos_kw:
                        pos_kw.append(kw)
                    ch_pos_hit = True
            if ch_pos_hit:
                ch_pos += w

            # Change Negative (binary, context)
            ch_neg_hit = False
            for kw in self.CHANGE_NEGATIVE:
                if self._keyword_in_context(kw, text):
                    if kw not in neg_kw:
                        neg_kw.append(kw)
                    ch_neg_hit = True
            if ch_neg_hit:
                ch_neg += w

            if idx < 3:
                logger.debug(f"[{ticker}] sample review weight={w} source={r.source} current={r.is_current_employee}")

        if total_w > 0:
            inn_s = (inn_pos - inn_neg) / total_w * 50 + 50
            dd_s = dd / total_w * 100
            ai_s = ai_m / total_w * 100
            ch_s = (ch_pos - ch_neg) / total_w * 50 + 50
        else:
            inn_s = Decimal("50")
            dd_s = Decimal("0")
            ai_s = Decimal("0")
            ch_s = Decimal("50")

        clamp = lambda v: max(Decimal("0"), min(Decimal("100"), v))
        inn_s, dd_s, ai_s, ch_s = clamp(inn_s), clamp(dd_s), clamp(ai_s), clamp(ch_s)

        overall = (
            Decimal("0.30") * inn_s
            + Decimal("0.25") * dd_s
            + Decimal("0.25") * ai_s
            + Decimal("0.20") * ch_s
        ).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)

        # Confidence: grows with review count + small source diversity bonus
        conf = min(Decimal("0.5") + Decimal(str(len(reviews))) / 200, Decimal("0.90"))
        source_bonus = min(Decimal(str(len(src_counts))) * Decimal("0.03"), Decimal("0.10"))
        conf = min(conf + source_bonus, Decimal("0.95"))

        avg_rating = Decimal(str(round(rating_sum / len(reviews), 2)))
        current_ratio = Decimal(str(round(current_count / len(reviews), 3)))

        logger.info(f"[{ticker}] Reviews analyzed={len(reviews)} sources={src_counts} total_w={total_w}")
        logger.info(f"[{ticker}] Scores: inn={inn_s:.2f} dd={dd_s:.2f} ai={ai_s:.2f} ch={ch_s:.2f} overall={overall}")

        return CultureSignal(
            company_id=company_id,
            ticker=ticker,
            innovation_score=inn_s.quantize(Decimal("0.01")),
            data_driven_score=dd_s.quantize(Decimal("0.01")),
            change_readiness_score=ch_s.quantize(Decimal("0.01")),
            ai_awareness_score=ai_s.quantize(Decimal("0.01")),
            overall_score=overall,
            review_count=len(reviews),
            avg_rating=avg_rating,
            current_employee_ratio=current_ratio,
            confidence=conf.quantize(Decimal("0.001")),
            source_breakdown=src_counts,
            positive_keywords_found=pos_kw,
            negative_keywords_found=neg_kw,
        )

    # -----------------------------------------------------------------
    # S3 upload (unchanged)
    # -----------------------------------------------------------------
    def _get_s3_service(self):
        if not hasattr(self, "_s3_client"):
            try:
                import boto3
                bucket = os.getenv("S3_BUCKET", "")
                key_id = os.getenv("AWS_ACCESS_KEY_ID", "")
                secret = os.getenv("AWS_SECRET_ACCESS_KEY", "")
                region = os.getenv("AWS_REGION", "us-east-1")

                if not bucket or not key_id or not secret:
                    logger.warning("S3 not configured. Set S3_BUCKET, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY in .env")
                    self._s3_client = None
                    self._s3_bucket = None
                    return None

                self._s3_client = boto3.client(
                    "s3",
                    aws_access_key_id=key_id,
                    aws_secret_access_key=secret,
                    region_name=region,
                )
                self._s3_bucket = bucket
                logger.info(f"S3 initialized: bucket={bucket}, region={region}")
            except Exception as e:
                logger.error(f"S3 initialization failed: {e}")
                self._s3_client = None
                self._s3_bucket = None
        return self._s3_client

    def _upload_raw_to_s3(self, ticker: str, reviews: List[CultureReview]):
        client = self._get_s3_service()
        if not client:
            return None
        ticker = ticker.upper()
        raw_data = []
        for r in reviews:
            raw_data.append({
                "review_id": r.review_id,
                "rating": r.rating,
                "title": r.title,
                "pros": r.pros,
                "cons": r.cons,
                "advice_to_management": r.advice_to_management,
                "is_current_employee": r.is_current_employee,
                "job_title": r.job_title,
                "review_date": r.review_date.isoformat() if r.review_date else None,
                "source": r.source,
            })
        s3_key = f"glassdoor_signals/raw/{ticker}_raw.json"
        payload = json.dumps({"ticker": ticker, "review_count": len(raw_data), "reviews": raw_data}, indent=2, default=str)
        try:
            client.put_object(
                Bucket=self._s3_bucket,
                Key=s3_key,
                Body=payload.encode("utf-8"),
                ContentType="application/json",
            )
            logger.info(f"[{ticker}] Uploaded {len(raw_data)} raw reviews to S3: {s3_key}")
            return s3_key
        except Exception as e:
            logger.error(f"[{ticker}] S3 raw upload failed: {e}")
            return None

    def _upload_output_to_s3(self, signal: CultureSignal):
        client = self._get_s3_service()
        if not client:
            return None
        ticker = signal.ticker.upper()
        output_data = asdict(signal)
        for k, v in output_data.items():
            if isinstance(v, Decimal):
                output_data[k] = float(v)
        s3_key = f"glassdoor_signals/output/{ticker}_culture.json"
        payload = json.dumps(output_data, indent=2, default=str)
        try:
            client.put_object(
                Bucket=self._s3_bucket,
                Key=s3_key,
                Body=payload.encode("utf-8"),
                ContentType="application/json",
            )
            logger.info(f"[{ticker}] Uploaded culture signal to S3: {s3_key}")
            return s3_key
        except Exception as e:
            logger.error(f"[{ticker}] S3 output upload failed: {e}")
            return None

    # -----------------------------------------------------------------
    # Entry points
    # -----------------------------------------------------------------
    def collect_and_analyze(
        self,
        ticker: str,
        sources: Optional[List[str]] = None,
        use_cache: bool = True,
        gd_pages: Optional[int] = None,
        indeed_pages: Optional[int] = None,
        cb_clicks: Optional[int] = None,
    ) -> CultureSignal:
        ticker = validate_ticker(ticker)
        if sources is None:
            sources = ["glassdoor", "indeed", "careerbliss"]
        sources = [s for s in sources if s in VALID_SOURCES]

        reg = COMPANY_REGISTRY[ticker]
        logger.info(f"{'=' * 55}")
        logger.info(f"CULTURE COLLECTION: {ticker} ({reg['name']})")
        logger.info(f"   Sector:  {reg['sector']}")
        logger.info(f"   Sources: {', '.join(sources)}")
        logger.info(f"   Depth:   gd_pages={gd_pages or self.DEFAULT_MAX_GLASSDOOR_PAGES}, "
                    f"indeed_pages={indeed_pages or self.DEFAULT_MAX_INDEED_PAGES}, "
                    f"cb_clicks={cb_clicks or self.DEFAULT_MAX_CAREERBLISS_CLICKS}")
        logger.info(f"{'=' * 55}")

        reviews = self.fetch_all_reviews(
            ticker,
            sources=sources,
            max_pages_glassdoor=gd_pages,
            max_pages_indeed=indeed_pages,
            max_clicks_careerbliss=cb_clicks,
            use_cache=use_cache,
        )

        signal = self.analyze_reviews(ticker, ticker, reviews)

        # Upload to S3 only (as your previous code does)
        self._upload_raw_to_s3(ticker, reviews)
        self._upload_output_to_s3(signal)

        return signal

    def collect_multiple(
        self,
        tickers: List[str],
        sources: Optional[List[str]] = None,
        use_cache: bool = True,
        gd_pages: Optional[int] = None,
        indeed_pages: Optional[int] = None,
        cb_clicks: Optional[int] = None,
        delay: float = 2.0,
    ) -> Dict[str, CultureSignal]:
        results: Dict[str, CultureSignal] = {}
        try:
            for i, ticker in enumerate(tickers):
                try:
                    signal = self.collect_and_analyze(
                        ticker,
                        sources=sources,
                        use_cache=use_cache,
                        gd_pages=gd_pages,
                        indeed_pages=indeed_pages,
                        cb_clicks=cb_clicks,
                    )
                    results[ticker.upper()] = signal
                except Exception as e:
                    logger.error(f"[{ticker}] FAILED: {e}")
                if i < len(tickers) - 1:
                    logger.info(f"Waiting {delay}s before next ticker...")
                    time.sleep(delay)
        finally:
            self.close_browser()
        return results


# =====================================================================
# DISPLAY
# =====================================================================

def print_signal(signal: CultureSignal):
    reg = COMPANY_REGISTRY.get(signal.ticker, {})
    name = reg.get("name", signal.ticker)
    sector = reg.get("sector", "")
    print(f"\n{'=' * 60}")
    print(f"  CULTURE ANALYSIS -- {signal.ticker} ({name})")
    if sector:
        print(f"  Sector: {sector}")
    print(f"{'=' * 60}")
    print(f"  Overall Score:          {signal.overall_score}/100")
    print(f"  Confidence:             {signal.confidence}")
    print(f"  Reviews Analyzed:       {signal.review_count}")
    print(f"  Source Breakdown:       {signal.source_breakdown}")
    print(f"  Avg Rating:             {signal.avg_rating}/5.0")
    print(f"  Current Employee Ratio: {signal.current_employee_ratio}")
    print()
    print(f"  Component Scores:       Weight   Score")
    print(f"    Innovation:           0.30   {signal.innovation_score:>8}")
    print(f"    Data-Driven:          0.25   {signal.data_driven_score:>8}")
    print(f"    AI Awareness:         0.25   {signal.ai_awareness_score:>8}")
    print(f"    Change Readiness:     0.20   {signal.change_readiness_score:>8}")
    if signal.positive_keywords_found:
        print(f"\n  (+) Keywords: {', '.join(signal.positive_keywords_found[:10])}")
    if signal.negative_keywords_found:
        print(f"  (-) Keywords: {', '.join(signal.negative_keywords_found[:10])}")


# =====================================================================
# MAIN / CLI
# =====================================================================

def _parse_int_flag(args: List[str], prefix: str) -> Optional[int]:
    for a in args:
        if a.startswith(prefix):
            try:
                return int(a.split("=", 1)[1])
            except Exception:
                raise ValueError(f"Bad flag {a}. Expected {prefix}<int>")
    return None


def main():
    args = sys.argv[1:]
    use_cache = "--no-cache" not in args

    sources: Optional[List[str]] = None
    for a in args:
        if a.startswith("--sources="):
            sources = [s.strip() for s in a.split("=", 1)[1].split(",") if s.strip()]

    gd_pages = _parse_int_flag(args, "--gd-pages=")
    indeed_pages = _parse_int_flag(args, "--indeed-pages=")
    cb_clicks = _parse_int_flag(args, "--cb-clicks=")

    # Clean args = tickers
    tickers = []
    for a in args:
        if a.startswith("--"):
            continue
        if a.startswith("-"):
            continue
        tickers.append(a)

    if "--all" in args:
        tickers = all_tickers()
    elif not tickers:
        print("\n" + "=" * 58)
        print("  Multi-Source Culture Collector (CS3)")
        print("=" * 58)
        print()
        print("Usage:")
        print("  python app/pipelines/culture_collector.py NVDA JPM")
        print("  python app/pipelines/culture_collector.py --all")
        print()
        print("Options:")
        print("  --no-cache")
        print("  --sources=glassdoor,indeed,careerbliss")
        print("  --gd-pages=30         (RapidAPI usage guardrail)")
        print("  --indeed-pages=25     (scrape depth guardrail)")
        print("  --cb-clicks=15        (More Reviews clicks)")
        print()
        print("Allowed tickers:")
        for t in sorted(ALLOWED_TICKERS):
            reg = COMPANY_REGISTRY[t]
            print(f"  {t:<6} {reg['name']:<30} {reg['sector']}")
        print()
        return

    tickers = [validate_ticker(t) for t in tickers]

    collector = CultureCollector()
    results = collector.collect_multiple(
        tickers,
        sources=sources,
        use_cache=use_cache,
        gd_pages=gd_pages,
        indeed_pages=indeed_pages,
        cb_clicks=cb_clicks,
        delay=2.0,
    )

    print("\n\n" + "#" * 60)
    print(f"#  MULTI-SOURCE CULTURE ANALYSIS -- {len(results)} companies")
    print("#" * 60)

    for _, signal in results.items():
        print_signal(signal)

    if len(results) > 1:
        print(f"\n{'=' * 62}")
        print(f"  {'Ticker':<6} {'Overall':>8} {'Innov':>7} {'Data':>7} {'AI':>7} {'Change':>7} {'#Rev':>5} {'Conf':>6}")
        print(f"  {'-'*6} {'-'*8} {'-'*7} {'-'*7} {'-'*7} {'-'*7} {'-'*5} {'-'*6}")
        for t, s in sorted(results.items(), key=lambda x: x[1].overall_score, reverse=True):
            print(f"  {t:<6} {s.overall_score:>8} {s.innovation_score:>7} {s.data_driven_score:>7} {s.ai_awareness_score:>7} {s.change_readiness_score:>7} {s.review_count:>5} {s.confidence:>6}")
        print(f"{'=' * 62}")


if __name__ == "__main__":
    main()
