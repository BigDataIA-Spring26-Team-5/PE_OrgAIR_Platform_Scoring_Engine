# #app/pipelines/job_signals.py
# from __future__ import annotations
# import asyncio
# import json
# import logging
# import re
# from collections import defaultdict
# from datetime import datetime
# from pathlib import Path
# from typing import List, Optional, Dict, Any
# from app.config import (
#     settings,
#     get_company_search_name,
#     get_company_aliases,
#     get_search_name_by_official,
#     get_aliases_by_official,
#     COMPANY_NAME_MAPPINGS
# )
# from app.models.signal import JobPosting
# from app.pipelines.keywords import AI_KEYWORDS, AI_TECHSTACK_KEYWORDS, TOP_AI_TOOLS
# from app.pipelines.pipeline2_state import Pipeline2State
# from app.pipelines.utils import clean_nan, safe_filename
# from app.pipelines.tech_signals import (
#     TechStackCollector, TechnologyDetection,
#     calculate_techstack_score, create_external_signal_from_techstack,
#     log_techstack_results
# )

# # RapidFuzz for fuzzy company name matching
# from rapidfuzz import fuzz

# logger = logging.getLogger(__name__)


# def step1_init_job_collection(state: Pipeline2State) -> Pipeline2State:
#     """Initialize job collection step."""
#     state.mark_started()
    
#     # Create output directory
#     Path(state.output_dir).mkdir(parents=True, exist_ok=True)
    
#     logger.info("-" * 40)
#     logger.info("üìÅ [1/5] INITIALIZING JOB COLLECTION")
#     logger.info(f"   Output directory: {state.output_dir}")
#     return state

# def is_company_match_fuzzy(
#     job_company: str,
#     target_company: str,
#     threshold: float = 75.0,
#     ticker: Optional[str] = None
# ) -> bool:
#     """
#     Use fuzzy matching to determine if job company matches target company.

#     Args:
#         job_company: Company name from job posting
#         target_company: Target company we're searching for
#         threshold: Similarity score threshold (0-100)
#         ticker: Optional ticker to use predefined aliases for matching

#     Returns:
#         True if companies match with sufficient similarity
#     """
#     if not job_company or not target_company:
#         return False

#     # Clean the job company string
#     job_clean = str(job_company).strip().lower()

#     # Build list of valid names to match against
#     valid_names = [target_company]

#     # Add aliases from config if ticker is provided
#     if ticker:
#         aliases = get_company_aliases(ticker)
#         valid_names.extend(aliases)
#     else:
#         # Try to find aliases by official name
#         aliases = get_aliases_by_official(target_company)
#         if aliases:
#             valid_names.extend(aliases)

#     # Remove duplicates and clean
#     valid_names_clean = list(set(name.strip().lower() for name in valid_names if name))

#     # Check for exact match first
#     if job_clean in valid_names_clean:
#         return True

#     # Fuzzy match against all valid names
#     for valid_name in valid_names_clean:
#         scores = [
#             fuzz.token_sort_ratio(job_clean, valid_name),  # Handles word order
#             fuzz.partial_ratio(job_clean, valid_name),     # Handles substrings
#             fuzz.ratio(job_clean, valid_name),             # Simple ratio
#         ]
#         if max(scores) >= threshold:
#             return True

#     return False

# async def step2_fetch_job_postings(
#     state: Pipeline2State,
#     *,
#     sites: Optional[List[str]] = None,
#     results_wanted: Optional[int] = None,
#     hours_old: Optional[int] = None,
# ) -> Pipeline2State:
    
#     logger.info("-" * 40)
#     logger.info("üîç [2/5] FETCHING JOB POSTINGS")
    
#     # Use config defaults if not provided
#     if sites is None:
#         sites = settings.JOBSPY_DEFAULT_SITES
#     if results_wanted is None:
#         results_wanted = settings.JOBSPY_RESULTS_WANTED
#     if hours_old is None:
#         hours_old = settings.JOBSPY_HOURS_OLD
    
#     logger.info(f"   Sites: {', '.join(sites)}")
#     logger.info(f"   Max results: {results_wanted}")
#     logger.info(f"   Hours old: {hours_old}")
#     logger.info(f"   Fuzzy match threshold: {settings.JOBSPY_FUZZY_MATCH_THRESHOLD}%")
    
#     # Try to import jobspy
#     try:
#         from jobspy import scrape_jobs
#     except ImportError as e:
#         error_msg = "python-jobspy not installed. Run: pip install python-jobspy"
#         logger.error(f"   ‚ùå {error_msg}")
#         state.add_error("job_fetch", "import", error_msg)
#         raise ImportError(error_msg) from e
    
#     # Initialize tech stack collector
#     tech_collector = TechStackCollector()
    
#     for company in state.companies:
#         company_id = company.get("id", "")
#         company_name = company.get("name", "")
#         ticker = company.get("ticker", "").upper()

#         if not company_name:
#             continue

#         # Get search name from mappings (falls back to company_name if not mapped)
#         search_name = None
#         if ticker:
#             search_name = get_company_search_name(ticker)
#         if not search_name:
#             search_name = get_search_name_by_official(company_name)
#         if not search_name:
#             search_name = company_name  # Fallback to original name

#         # Rate limiting
#         await asyncio.sleep(max(state.request_delay, settings.JOBSPY_REQUEST_DELAY))

#         try:
#             logger.info(f"   üì• Scraping: {company_name} (search: '{search_name}')...")

#             # Scrape jobs - search by mapped search name
#             jobs_df = scrape_jobs(
#                 site_name=sites,
#                 search_term=search_name,
#                 results_wanted=results_wanted,
#                 hours_old=hours_old,
#                 country_indeed="USA",
#                 linkedin_fetch_description=True,  # Get full descriptions
#             )
            
#             postings = []
#             filtered_count = 0
#             total_raw = 0
            
#             if jobs_df is not None and not jobs_df.empty:
#                 total_raw = len(jobs_df)
                
#                 # Log a few samples for debugging
#                 sample_companies = jobs_df['company'].head(3).tolist()
#                 logger.debug(f"      Sample company names from scraped jobs: {sample_companies}")
                
#                 for _, row in jobs_df.iterrows():
#                     # Get the ACTUAL company name from the job posting
#                     job_company = str(row.get("company", "")) if clean_nan(row.get("company")) else ""
#                     source = str(row.get("site", "unknown"))
                    
#                     # Use fuzzy matching with aliases instead of strict matching
#                     if not is_company_match_fuzzy(
#                         job_company,
#                         search_name,
#                         threshold=settings.JOBSPY_FUZZY_MATCH_THRESHOLD,
#                         ticker=ticker
#                     ):
#                         filtered_count += 1
                        
#                         # Log first few filtered items for debugging
#                         if filtered_count <= 3:
#                             logger.debug(f"      Filtered: '{job_company}' vs '{company_name}'")
#                         continue
                    
#                     # Create JobPosting instance
#                     posting = JobPosting(
#                         company_id=company_id,
#                         company_name=job_company,
#                         title=str(row.get("title", "")),
#                         description=str(row.get("description", "")),
#                         location=str(row.get("location", "")) if clean_nan(row.get("location")) else None,
#                         posted_date=clean_nan(row.get("date_posted")),
#                         source=source,
#                         url=str(row.get("job_url", "")) if clean_nan(row.get("job_url")) else None,
#                     )
                    
#                     # Detect technologies using TechStackCollector
#                     description_text = posting.description or ""
#                     tech_detections = tech_collector.detect_technologies_from_text(
#                         f"{posting.title} {description_text}"
#                     )
                    
#                     # Convert to dict for storage
#                     posting_dict = posting.model_dump()
#                     posting_dict["tech_detections"] = [
#                         {
#                             "name": t.name,
#                             "category": t.category,
#                             "is_ai_related": t.is_ai_related,
#                             "confidence": t.confidence
#                         }
#                         for t in tech_detections
#                     ]
                    
#                     postings.append(posting_dict)
            
#             state.job_postings.extend(postings)
#             state.summary["job_postings_collected"] += len(postings)
            
#             logger.info(f"      ‚Ä¢ Raw results: {total_raw}")
#             logger.info(f"      ‚Ä¢ Matched jobs: {len(postings)} (filtered {filtered_count} unrelated)")
            
#             # If we filtered everything out, log warning
#             if total_raw > 0 and len(postings) == 0:
#                 logger.warning(f"      ‚ö†Ô∏è  No jobs matched after filtering. Try lowering JOBSPY_FUZZY_MATCH_THRESHOLD")
                
#         except Exception as e:
#             state.add_error("job_fetch", company_id, str(e))
#             logger.error(f"      ‚ùå Error: {e}")
    
#     logger.info(f"   ‚úÖ Total collected: {len(state.job_postings)} job postings")
#     return state


# def _has_keyword(text: str, keyword: str) -> bool:
#     """
#     Check if keyword exists in text with word boundary awareness.
#     Handles short keywords like 'ai', 'ml' that could match parts of words.
#     """
#     # For very short keywords (2-3 chars), use word boundary matching
#     if len(keyword) <= 3:
#         # Match as whole word or with common separators
#         pattern = r'(?:^|[\s,\-_/\(\)])' + re.escape(keyword) + r'(?:$|[\s,\-_/\(\)])'
#         return bool(re.search(pattern, text, re.IGNORECASE))
#     else:
#         # For longer keywords, simple substring match is fine
#         return keyword in text


# def step3_classify_ai_jobs(state: Pipeline2State) -> Pipeline2State:
#     """
#     Classify job postings as AI-related using AI_KEYWORDS.
#     """
#     logger.info("-" * 40)
#     logger.info("ü§ñ [3/5] CLASSIFYING AI-RELATED JOBS")
    
#     for posting in state.job_postings:
#         title = posting.get('title', '')
#         description = posting.get('description', '') or ''
        
#         # Check if we have a real description
#         has_description = description and description.lower() not in ('none', 'nan', '')
        
#         # Combine title and description for searching
#         text = f"{title} {description}".lower()
#         title_lower = title.lower()
        
#         # Find matching AI keywords
#         ai_keywords_found = []
#         for keyword in AI_KEYWORDS:
#             if _has_keyword(text, keyword):
#                 ai_keywords_found.append(keyword)
        
#         # Find matching tech stack keywords
#         techstack_found = []
#         for keyword in AI_TECHSTACK_KEYWORDS:
#             if _has_keyword(text, keyword):
#                 techstack_found.append(keyword)
        
#         posting["ai_keywords_found"] = ai_keywords_found
#         posting["techstack_keywords_found"] = techstack_found
        
#         # Determine if AI role based on keyword count
#         if has_description:
#             posting["is_ai_role"] = len(ai_keywords_found) >= settings.JOBSPY_AI_KEYWORDS_THRESHOLD_WITH_DESC
#         else:
#             posting["is_ai_role"] = len(ai_keywords_found) >= settings.JOBSPY_AI_KEYWORDS_THRESHOLD_NO_DESC
        
#         # Calculate AI score (0-JOBSPY_MAX_SCORE)
#         posting["ai_score"] = min(settings.JOBSPY_MAX_SCORE, len(ai_keywords_found) * settings.JOBSPY_AI_SCORE_MULTIPLIER)
    
#     ai_count = sum(1 for p in state.job_postings if p.get("is_ai_role"))
#     total_count = len(state.job_postings)
    
#     logger.info(f"   ‚Ä¢ Total jobs analyzed: {total_count}")
#     logger.info(f"   ‚Ä¢ AI-related jobs: {ai_count}")
#     if total_count > 0:
#         logger.info(f"   ‚Ä¢ AI job ratio: {(ai_count/total_count*100):.1f}%")
    
#     return state


# def calculate_job_score(jobs: List[Dict[str, Any]]) -> Dict[str, Any]:
#     """
#     Calculate job market score for AI hiring signals.

#     Uses pre-classified AI roles from step3 (is_ai_role flag) for consistency.

#     Scoring Algorithm (0-100):
#     - AI Ratio Score: (ai_jobs / total_jobs) * 40 (max 40 points)
#     - AI Volume Score: ai_jobs * 2 (max 30 points, need 15 AI jobs)
#     - Skill Diversity: (unique_ai_keywords / 15) * 30 (max 30 points)
#     """
#     total_jobs = len(jobs)

#     # Use pre-classified AI roles from step3
#     ai_jobs = sum(1 for job in jobs if job.get('is_ai_role', False))

#     # Collect unique AI keywords found across all jobs
#     all_ai_keywords = set()
#     for job in jobs:
#         all_ai_keywords.update(job.get('ai_keywords_found', []))

#     # Calculate AI ratio
#     ai_ratio = ai_jobs / total_jobs if total_jobs > 0 else 0

#     # Calculate score components (updated - harder to max out)
#     ratio_score = min(ai_ratio * 100, 40)  # 40% AI jobs = max 40 points
#     volume_score = min(ai_jobs * 2, 30)     # 15 AI jobs = max 30 points
#     diversity_score = min(len(all_ai_keywords) / 15, 1) * 30  # 15 keywords = max 30 points

#     final_score = ratio_score + volume_score + diversity_score

#     # Calculate confidence based on sample size
#     if total_jobs >= 100:
#         confidence = 0.95
#     elif total_jobs >= 50:
#         confidence = 0.85
#     elif total_jobs >= 20:
#         confidence = 0.70
#     else:
#         confidence = 0.50

#     return {
#         "score": round(final_score, 1),
#         "ai_jobs": ai_jobs,
#         "total_jobs": total_jobs,
#         "ai_ratio": round(ai_ratio, 3),
#         "ai_keywords": list(all_ai_keywords),
#         "score_breakdown": {
#             "ratio_score": round(ratio_score, 1),
#             "volume_score": round(volume_score, 1),
#             "diversity_score": round(diversity_score, 1)
#         },
#         "confidence": confidence
#     }


# def step4_score_job_market(state: Pipeline2State) -> Pipeline2State:
#     """
#     Calculate job market score for each company using the main scoring algorithm.
#     """
#     logger.info("-" * 40)
#     logger.info("üìä [4/5] SCORING JOB MARKET")

#     # Build lookup for target company names
#     company_name_lookup = {c.get("id"): c.get("name", c.get("id")) for c in state.companies}

#     # Group all jobs by TARGET company_id (from our database, not job site)
#     company_jobs = defaultdict(list)
#     for posting in state.job_postings:
#         company_jobs[posting["company_id"]].append(posting)

#     for company_id, jobs in company_jobs.items():
#         if not jobs:
#             state.job_market_scores[company_id] = 0.0
#             continue

#         # Use main scoring algorithm
#         analysis = calculate_job_score(jobs)

#         final_score = analysis["score"]
#         state.job_market_scores[company_id] = round(final_score, 2)

#         # Store analysis for later use
#         state.job_market_analyses[company_id] = analysis

#         # Use TARGET company name from state, not job posting company name
#         company_name = company_name_lookup.get(company_id, company_id)
#         breakdown = analysis["score_breakdown"]
#         logger.info(
#             f"   ‚Ä¢ {company_name}: {final_score:.1f}/100 "
#             f"(ratio={breakdown['ratio_score']:.1f}, volume={breakdown['volume_score']:.1f}, diversity={breakdown['diversity_score']:.1f})"
#         )

#     logger.info(f"   ‚úÖ Scored {len(state.job_market_scores)} companies")
#     return state


# def step4b_score_techstack(state: Pipeline2State) -> Pipeline2State:
#     """
#     Calculate techstack score for each company based on AI tools presence.

#     Uses both original scoring and TechStackCollector analysis.
#     """
#     logger.info("-" * 40)
#     logger.info("üîß [4b/5] SCORING TECH STACK")

#     # Build lookup for target company names
#     company_name_lookup = {c.get("id"): c.get("name", c.get("id")) for c in state.companies}

#     tech_collector = TechStackCollector()
#     company_jobs = defaultdict(list)

#     for posting in state.job_postings:
#         company_jobs[posting["company_id"]].append(posting)
    
#     for company_id, jobs in company_jobs.items():
#         if not jobs:
#             state.techstack_scores[company_id] = 0.0
#             state.company_techstacks[company_id] = []
#             continue
        
#         # Aggregate ALL unique techstack keywords from all jobs
#         all_techstack_keywords = set()
#         for job in jobs:
#             all_techstack_keywords.update(job.get("techstack_keywords_found", []))
        
#         # Aggregate technology detections
#         all_tech_detections = []
#         for job in jobs:
#             tech_detections = job.get("tech_detections", [])
#             for tech in tech_detections:
#                 all_tech_detections.append(
#                     TechnologyDetection(
#                         name=tech["name"],
#                         category=tech["category"],
#                         is_ai_related=tech["is_ai_related"],
#                         confidence=tech["confidence"]
#                     )
#                 )
        
#         # Store unique techstack for this company
#         state.company_techstacks[company_id] = sorted(list(all_techstack_keywords))
        
#         # Calculate original techstack score
#         original_analysis = calculate_techstack_score(all_techstack_keywords, all_tech_detections)
#         original_score = original_analysis["score"]
        
#         # TechStackCollector scoring
#         collector_analysis = tech_collector.analyze_tech_stack(
#             company_id=company_id,
#             technologies=all_tech_detections
#         )
#         collector_score = collector_analysis["score"]
        
#         # Combine scores (weighted average)
#         collector_weight = 0.3  # 30% weight to collector score
#         original_weight = 0.7   # 70% weight to original score
        
#         final_score = (
#             original_score * original_weight +
#             collector_score * collector_weight
#         )
        
#         state.techstack_scores[company_id] = round(final_score, 2)
#         state.techstack_analyses[company_id] = {
#             "original": original_analysis,
#             "collector": collector_analysis,
#             "combined_score": final_score
#         }
        
#         # Use TARGET company name from state, not job posting company name
#         company_name = company_name_lookup.get(company_id, company_id)
#         log_techstack_results(
#             company_name=company_name,
#             original_score=original_score,
#             collector_score=collector_score,
#             final_score=final_score,
#             keywords_count=len(all_techstack_keywords),
#             ai_tools_count=len(original_analysis.get("ai_tools_found", []))
#         )

#     logger.info(f"   ‚úÖ Scored techstack for {len(state.techstack_scores)} companies")
#     return state


# def step5_save_to_json(state: Pipeline2State) -> Pipeline2State:
#     """Save results to local JSON files (legacy/debug mode)."""
#     logger.info("-" * 40)
#     logger.info("üíæ [5/5] SAVING RESULTS TO JSON")
    
#     output_dir = Path(state.output_dir)
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
#     # Save all job postings
#     all_jobs_file = output_dir / f"all_jobs_{timestamp}.json"
#     with open(all_jobs_file, "w", encoding="utf-8") as f:
#         json.dump(state.job_postings, f, indent=2, default=str)
#     logger.info(f"   üìÑ All jobs: {all_jobs_file}")
    
#     # Save AI-related jobs only
#     ai_jobs = [p for p in state.job_postings if p.get("is_ai_role")]
#     ai_jobs_file = output_dir / f"ai_jobs_{timestamp}.json"
#     with open(ai_jobs_file, "w", encoding="utf-8") as f:
#         json.dump(ai_jobs, f, indent=2, default=str)
#     logger.info(f"   üìÑ AI jobs: {ai_jobs_file}")
    
#     # Save per-company results
#     company_jobs = defaultdict(list)
#     for posting in state.job_postings:
#         company_jobs[posting["company_id"]].append(posting)
    
#     for company_id, jobs in company_jobs.items():
#         company_name = jobs[0].get("company_name", company_id) if jobs else company_id
#         safe_name = safe_filename(company_name)
        
#         company_file = output_dir / f"{safe_name}_{timestamp}.json"
#         company_data = {
#             "company_id": company_id,
#             "company_name": company_name,
#             "total_jobs": len(jobs),
#             "ai_jobs": sum(1 for j in jobs if j.get("is_ai_role")),
#             "job_market_score": state.job_market_scores.get(company_id, 0),
#             "techstack_score": state.techstack_scores.get(company_id, 0),
#             "techstack_keywords": state.company_techstacks.get(company_id, []),
#             "techstack_analysis": state.techstack_analyses.get(company_id, {}),
#             "jobs": jobs
#         }
#         with open(company_file, "w", encoding="utf-8") as f:
#             json.dump(company_data, f, indent=2, default=str)
    
#     # Save summary
#     summary_file = output_dir / f"summary_{timestamp}.json"
#     summary_data = {
#         **state.summary,
#         "job_market_scores": state.job_market_scores,
#         "techstack_scores": state.techstack_scores,
#         "company_techstacks": state.company_techstacks,
#         "techstack_analyses": state.techstack_analyses,
#         "companies": [c.get("name", c.get("id")) for c in state.companies]
#     }
#     with open(summary_file, "w", encoding="utf-8") as f:
#         json.dump(summary_data, f, indent=2, default=str)
#     logger.info(f"   üìÑ Summary: {summary_file}")
    
#     logger.info(f"   ‚úÖ Saved to {output_dir}")
#     return state


# def step5_store_to_s3_and_snowflake(state: Pipeline2State) -> Pipeline2State:
#     """
#     Store results in S3 (raw data) and Snowflake (aggregated signals).
#     """
#     from app.services.s3_storage import get_s3_service
#     from app.services.snowflake import SnowflakeService
    
#     logger.info("-" * 40)
#     logger.info("‚òÅÔ∏è [5/5] STORING TO S3 & SNOWFLAKE")
    
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
#     # Initialize services
#     s3 = get_s3_service()
#     db = SnowflakeService()
    
#     try:
#         # Group jobs by company
#         company_jobs = defaultdict(list)
#         for posting in state.job_postings:
#             company_jobs[posting["company_id"]].append(posting)
        
#         for company_id, jobs in company_jobs.items():
#             if not jobs:
#                 continue
            
#             company_name = jobs[0].get("company_name", company_id)
#             ticker = None
#             for company in state.companies:
#                 if company.get("id") == company_id:
#                     ticker = company.get("ticker", "").upper()
#                     break
#             if not ticker:
#                 ticker = safe_filename(company_name).upper()
            
#             # Get tech stack analyses
#             techstack_analysis = state.techstack_analyses.get(company_id, {})
#             original_analysis = techstack_analysis.get("original", {})
#             collector_analysis = techstack_analysis.get("collector", {})
            
#             # -----------------------------------------
#             # S3: Upload raw job postings
#             # -----------------------------------------
#             jobs_s3_key = f"signals/jobs/{ticker}/{timestamp}.json"
#             jobs_data = {
#                 "company_id": company_id,
#                 "company_name": company_name,
#                 "ticker": ticker,
#                 "collection_date": timestamp,
#                 "total_jobs": len(jobs),
#                 "ai_jobs": sum(1 for j in jobs if j.get("is_ai_role")),
#                 "job_market_score": state.job_market_scores.get(company_id, 0),
#                 "techstack_score": state.techstack_scores.get(company_id, 0),
#                 "techstack_analysis": techstack_analysis,
#                 "jobs": jobs
#             }
#             s3.upload_json(jobs_data, jobs_s3_key)
#             logger.info(f"   üì§ S3 Jobs: {jobs_s3_key}")
            
#             # -----------------------------------------
#             # S3: Upload tech stack analysis
#             # -----------------------------------------
#             techstack_keywords = state.company_techstacks.get(company_id, [])
#             techstack_score = state.techstack_scores.get(company_id, 0.0)
            
#             techstack_s3_key = f"signals/techstack/{ticker}/{timestamp}.json"
#             techstack_data = {
#                 "company_id": company_id,
#                 "company_name": company_name,
#                 "ticker": ticker,
#                 "collection_date": timestamp,
#                 "techstack_score": techstack_score,
#                 "all_keywords": techstack_keywords,
#                 "ai_tools_found": original_analysis.get("ai_tools_found", []),
#                 "total_keywords": original_analysis.get("total_keywords", 0),
#                 "total_ai_tools": original_analysis.get("total_ai_tools", 0),
#                 "techstack_analysis": techstack_analysis
#             }
#             s3.upload_json(techstack_data, techstack_s3_key)
#             logger.info(f"   üì§ S3 TechStack: {techstack_s3_key}")
            
#             # -----------------------------------------
#             # Snowflake: Insert external signal
#             # -----------------------------------------
#             job_market_analysis = state.job_market_analyses.get(company_id, {})
#             score = state.job_market_scores.get(company_id, 0.0)

#             ai_count = job_market_analysis.get("ai_jobs", 0)
#             total_tech_jobs = job_market_analysis.get("total_tech_jobs", 0)

#             # Build summary text
#             summary = f"Found {ai_count} AI roles out of {total_tech_jobs} tech jobs"

#             # Build raw_payload with detailed metrics from analysis
#             raw_payload = {
#                 "collection_date": timestamp,
#                 "s3_jobs_key": jobs_s3_key,
#                 "s3_techstack_key": techstack_s3_key,
#                 "total_jobs": len(jobs),
#                 "total_tech_jobs": total_tech_jobs,
#                 "ai_jobs": ai_count,
#                 "ai_ratio": job_market_analysis.get("ai_ratio", 0),
#                 "score_breakdown": job_market_analysis.get("score_breakdown", {}),
#                 "all_skills": job_market_analysis.get("all_skills", []),
#                 "confidence": job_market_analysis.get("confidence", 0.5),
#                 "sources": list(set(j.get("source", "unknown") for j in jobs)),
#                 "techstack": {
#                     "score": techstack_score,
#                     "all_keywords": techstack_keywords,
#                     "ai_tools_found": original_analysis.get("ai_tools_found", []),
#                     "techstack_analysis": techstack_analysis,
#                 },
#             }
            
#             # Determine primary source
#             source_counts = defaultdict(int)
#             for job in jobs:
#                 source_counts[job.get("source", "other")] += 1
#             primary_source = max(source_counts, key=source_counts.get) if source_counts else "other"
            
#             # Insert into Snowflake
#             signal_id = f"{company_id}_job_market_{timestamp}"
#             db.insert_external_signal(
#                 signal_id=signal_id,
#                 company_id=company_id,
#                 category="job_market",
#                 source=primary_source,
#                 score=score,
#                 evidence_count=ai_count,
#                 summary=summary,
#                 raw_payload=raw_payload,
#             )
#             logger.info(f"   üíæ Snowflake: {company_name} (score: {score})")
        
#         logger.info(f"   ‚úÖ Stored {len(company_jobs)} companies in S3 + Snowflake")
#         return state
    
#     finally:
#         db.close()


# async def run_job_signals(
#     state: Pipeline2State,
#     *,
#     skip_storage: bool = False,
#     use_local_storage: bool = False,
# ) -> Pipeline2State:
#     """
#     Run the job signals collection pipeline (extract, classify, score).

#     Args:
#         state: Pipeline state with companies loaded
#         skip_storage: If True, skip all storage steps (for pipeline2_runner integration)
#         use_local_storage: If True and skip_storage=False, save to local JSON instead of S3/Snowflake

#     Returns:
#         Updated pipeline state with job postings, classifications, and scores
#     """
#     # Step 1: Initialize
#     state = step1_init_job_collection(state)
#     # Step 2: Fetch job postings
#     state = await step2_fetch_job_postings(state)
#     # Step 3: Classify AI jobs
#     state = step3_classify_ai_jobs(state)
#     # Step 4: Score job market
#     state = step4_score_job_market(state)
#     # Step 4b: Score tech stack
#     state = step4b_score_techstack(state)
#     # Step 5: Storage (optional - pipeline2_runner handles this separately)
#     if not skip_storage:
#         if use_local_storage:
#             state = step5_save_to_json(state)
#         else:
#             state = step5_store_to_s3_and_snowflake(state)
#     return state

# """
# Job Signals Pipeline ‚Äî Technology Hiring
# app/pipelines/job_signals.py

# Scrapes job postings, classifies AI roles, and scores hiring signals.
# This produces the technology_hiring signal category for CS2/CS3.

# It answers: "Is this company actively hiring AI/ML talent?"

# REMOVED in this cleanup:
#   - step4b_score_techstack (moved to tech_signals.py / BuiltWith+Wappalyzer)
#   - step5_save_to_json (no local file storage)
#   - All local data/ writes
#   - TechStackCollector imports (decoupled)
# """
# from __future__ import annotations

# import asyncio
# import json
# import logging
# import re
# from collections import defaultdict
# from datetime import datetime, timezone
# from typing import Any, Dict, List, Optional

# from app.config import (
#     settings,
#     get_company_search_name,
#     get_company_aliases,
#     get_search_name_by_official,
#     get_aliases_by_official,
# )
# from app.models.signal import JobPosting
# from app.pipelines.keywords import AI_KEYWORDS, AI_TECHSTACK_KEYWORDS
# from app.pipelines.signal_pipeline_state import SignalPipelineState
# from app.pipelines.utils import clean_nan, safe_filename

# # RapidFuzz for fuzzy company name matching
# from rapidfuzz import fuzz

# logger = logging.getLogger(__name__)


# # ---------------------------------------------------------------------------
# # Fuzzy matching
# # ---------------------------------------------------------------------------

# def is_company_match_fuzzy(
#     job_company: str,
#     target_company: str,
#     threshold: float = 75.0,
#     ticker: Optional[str] = None,
# ) -> bool:
#     """Use fuzzy matching to determine if job company matches target."""
#     if not job_company or not target_company:
#         return False

#     job_clean = str(job_company).strip().lower()

#     valid_names = [target_company]
#     if ticker:
#         valid_names.extend(get_company_aliases(ticker))
#     else:
#         aliases = get_aliases_by_official(target_company)
#         if aliases:
#             valid_names.extend(aliases)

#     valid_names_clean = list({n.strip().lower() for n in valid_names if n})

#     if job_clean in valid_names_clean:
#         return True

#     for vn in valid_names_clean:
#         scores = [
#             fuzz.token_sort_ratio(job_clean, vn),
#             fuzz.partial_ratio(job_clean, vn),
#             fuzz.ratio(job_clean, vn),
#         ]
#         if max(scores) >= threshold:
#             return True

#     return False


# # ---------------------------------------------------------------------------
# # Pipeline steps
# # ---------------------------------------------------------------------------

# def step1_init(state: SignalPipelineState) -> SignalPipelineState:
#     """Initialize job collection."""
#     state.mark_started()
#     logger.info("-" * 40)
#     logger.info("üìÅ [1/4] INITIALIZING JOB COLLECTION")
#     return state


# async def step2_fetch_job_postings(
#     state: SignalPipelineState,
#     *,
#     sites: Optional[List[str]] = None,
#     results_wanted: Optional[int] = None,
#     hours_old: Optional[int] = None,
# ) -> SignalPipelineState:
#     """Scrape job postings for all companies in state."""
#     logger.info("-" * 40)
#     logger.info("üîç [2/4] FETCHING JOB POSTINGS")

#     sites = sites or settings.JOBSPY_DEFAULT_SITES
#     results_wanted = results_wanted or settings.JOBSPY_RESULTS_WANTED
#     hours_old = hours_old or settings.JOBSPY_HOURS_OLD

#     logger.info(f"   Sites: {', '.join(sites)}")
#     logger.info(f"   Max results: {results_wanted}")
#     logger.info(f"   Hours old: {hours_old}")

#     try:
#         from jobspy import scrape_jobs
#     except ImportError as e:
#         msg = "python-jobspy not installed. Run: pip install python-jobspy"
#         logger.error(f"   ‚ùå {msg}")
#         state.add_error("job_fetch", msg)
#         raise ImportError(msg) from e

#     for company in state.companies:
#         company_id = company.get("id", "")
#         company_name = company.get("name", "")
#         ticker = company.get("ticker", "").upper()
#         if not company_name:
#             continue

#         search_name = (
#             get_company_search_name(ticker)
#             or get_search_name_by_official(company_name)
#             or company_name
#         )

#         await asyncio.sleep(max(state.request_delay, settings.JOBSPY_REQUEST_DELAY))

#         try:
#             logger.info(f"   üì• Scraping: {company_name} (search: '{search_name}')...")

#             jobs_df = scrape_jobs(
#                 site_name=sites,
#                 search_term=search_name,
#                 results_wanted=results_wanted,
#                 hours_old=hours_old,
#                 country_indeed="USA",
#                 linkedin_fetch_description=True,
#             )

#             postings: List[Dict[str, Any]] = []
#             filtered_count = 0
#             total_raw = 0

#             if jobs_df is not None and not jobs_df.empty:
#                 total_raw = len(jobs_df)
#                 for _, row in jobs_df.iterrows():
#                     job_company = (
#                         str(row.get("company", ""))
#                         if clean_nan(row.get("company"))
#                         else ""
#                     )
#                     source = str(row.get("site", "unknown"))

#                     if not is_company_match_fuzzy(
#                         job_company, search_name,
#                         threshold=settings.JOBSPY_FUZZY_MATCH_THRESHOLD,
#                         ticker=ticker,
#                     ):
#                         filtered_count += 1
#                         continue

#                     posting = JobPosting(
#                         company_id=company_id,
#                         company_name=job_company,
#                         title=str(row.get("title", "")),
#                         description=str(row.get("description", "")),
#                         location=(
#                             str(row.get("location", ""))
#                             if clean_nan(row.get("location"))
#                             else None
#                         ),
#                         posted_date=clean_nan(row.get("date_posted")),
#                         source=source,
#                         url=(
#                             str(row.get("job_url", ""))
#                             if clean_nan(row.get("job_url"))
#                             else None
#                         ),
#                     )
#                     postings.append(posting.model_dump())

#             state.job_postings.extend(postings)
#             state.summary["job_postings_collected"] += len(postings)

#             logger.info(f"      ‚Ä¢ Raw: {total_raw} | Matched: {len(postings)} | Filtered: {filtered_count}")
#             if total_raw > 0 and len(postings) == 0:
#                 logger.warning("      ‚ö†Ô∏è  All jobs filtered. Try lowering JOBSPY_FUZZY_MATCH_THRESHOLD")

#         except Exception as e:
#             state.add_error("job_fetch", str(e), company_id)
#             logger.error(f"      ‚ùå Error: {e}")

#     logger.info(f"   ‚úÖ Total collected: {len(state.job_postings)} job postings")
#     return state


# def _has_keyword(text: str, keyword: str) -> bool:
#     """Check if keyword exists in text with word-boundary awareness."""
#     if len(keyword) <= 3:
#         pattern = r"(?:^|[\s,\-_/\(\)])" + re.escape(keyword) + r"(?:$|[\s,\-_/\(\)])"
#         return bool(re.search(pattern, text, re.IGNORECASE))
#     return keyword in text


# def step3_classify_ai_jobs(state: SignalPipelineState) -> SignalPipelineState:
#     """Classify job postings as AI-related."""
#     logger.info("-" * 40)
#     logger.info("ü§ñ [3/4] CLASSIFYING AI-RELATED JOBS")

#     for posting in state.job_postings:
#         title = posting.get("title", "")
#         desc = posting.get("description", "") or ""
#         has_desc = desc and desc.lower() not in ("none", "nan", "")
#         text = f"{title} {desc}".lower()

#         ai_kw = [kw for kw in AI_KEYWORDS if _has_keyword(text, kw)]
#         ts_kw = [kw for kw in AI_TECHSTACK_KEYWORDS if _has_keyword(text, kw)]

#         posting["ai_keywords_found"] = ai_kw
#         posting["techstack_keywords_found"] = ts_kw

#         thresh = (
#             settings.JOBSPY_AI_KEYWORDS_THRESHOLD_WITH_DESC
#             if has_desc
#             else settings.JOBSPY_AI_KEYWORDS_THRESHOLD_NO_DESC
#         )
#         posting["is_ai_role"] = len(ai_kw) >= thresh
#         posting["ai_score"] = min(
#             settings.JOBSPY_MAX_SCORE,
#             len(ai_kw) * settings.JOBSPY_AI_SCORE_MULTIPLIER,
#         )

#     ai_count = sum(1 for p in state.job_postings if p.get("is_ai_role"))
#     total = len(state.job_postings)
#     logger.info(f"   ‚Ä¢ Total: {total} | AI-related: {ai_count}")
#     if total > 0:
#         logger.info(f"   ‚Ä¢ AI ratio: {ai_count / total * 100:.1f}%")
#     return state


# def calculate_job_score(jobs: List[Dict[str, Any]]) -> Dict[str, Any]:
#     """
#     Calculate technology_hiring score (0-100).

#     Components:
#       - AI Ratio: (ai_jobs / total) * 40    (max 40)
#       - Volume:   ai_jobs * 2               (max 30, need 15)
#       - Diversity: unique_keywords / 15 * 30 (max 30)
#     """
#     total = len(jobs)
#     ai_jobs = sum(1 for j in jobs if j.get("is_ai_role", False))
#     all_kw = set()
#     for j in jobs:
#         all_kw.update(j.get("ai_keywords_found", []))

#     ai_ratio = ai_jobs / total if total > 0 else 0
#     ratio_score = min(ai_ratio * 100, 40)
#     volume_score = min(ai_jobs * 2, 30)
#     diversity_score = min(len(all_kw) / 15, 1) * 30
#     final = ratio_score + volume_score + diversity_score

#     if total >= 100:
#         confidence = 0.95
#     elif total >= 50:
#         confidence = 0.85
#     elif total >= 20:
#         confidence = 0.70
#     else:
#         confidence = 0.50

#     return {
#         "score": round(final, 1),
#         "ai_jobs": ai_jobs,
#         "total_jobs": total,
#         "ai_ratio": round(ai_ratio, 3),
#         "ai_keywords": list(all_kw),
#         "score_breakdown": {
#             "ratio_score": round(ratio_score, 1),
#             "volume_score": round(volume_score, 1),
#             "diversity_score": round(diversity_score, 1),
#         },
#         "confidence": confidence,
#     }


# def step4_score_job_market(state: SignalPipelineState) -> SignalPipelineState:
#     """Score job market (technology_hiring) for each company."""
#     logger.info("-" * 40)
#     logger.info("üìä [4/4] SCORING JOB MARKET")

#     name_lookup = {c.get("id"): c.get("name", c.get("id")) for c in state.companies}
#     company_jobs: Dict[str, List] = defaultdict(list)
#     for p in state.job_postings:
#         company_jobs[p["company_id"]].append(p)

#     for cid, jobs in company_jobs.items():
#         if not jobs:
#             state.job_market_scores[cid] = 0.0
#             continue

#         analysis = calculate_job_score(jobs)
#         state.job_market_scores[cid] = round(analysis["score"], 2)
#         state.job_market_analyses[cid] = analysis

#         name = name_lookup.get(cid, cid)
#         bd = analysis["score_breakdown"]
#         logger.info(
#             f"   ‚Ä¢ {name}: {analysis['score']:.1f}/100 "
#             f"(ratio={bd['ratio_score']:.1f}, vol={bd['volume_score']:.1f}, "
#             f"div={bd['diversity_score']:.1f})"
#         )

#     logger.info(f"   ‚úÖ Scored {len(state.job_market_scores)} companies")
#     return state


# def step5_store_to_s3_and_snowflake(state: SignalPipelineState) -> SignalPipelineState:
#     """Store job results in S3 (raw) + Snowflake (signals)."""
#     from app.services.s3_storage import get_s3_service
#     from app.services.snowflake import SnowflakeService

#     logger.info("-" * 40)
#     logger.info("‚òÅÔ∏è [5/5] STORING TO S3 & SNOWFLAKE")

#     timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
#     s3 = get_s3_service()
#     db = SnowflakeService()

#     try:
#         company_jobs: Dict[str, List] = defaultdict(list)
#         for p in state.job_postings:
#             company_jobs[p["company_id"]].append(p)

#         for cid, jobs in company_jobs.items():
#             if not jobs:
#                 continue

#             company_name = jobs[0].get("company_name", cid)
#             ticker = None
#             for c in state.companies:
#                 if c.get("id") == cid:
#                     ticker = c.get("ticker", "").upper()
#                     break
#             ticker = ticker or safe_filename(company_name).upper()

#             analysis = state.job_market_analyses.get(cid, {})
#             score = state.job_market_scores.get(cid, 0.0)

#             # S3: raw job postings
#             s3_key = f"signals/jobs/{ticker}/{timestamp}.json"
#             s3.upload_json(
#                 {
#                     "company_id": cid,
#                     "company_name": company_name,
#                     "ticker": ticker,
#                     "collection_date": timestamp,
#                     "total_jobs": len(jobs),
#                     "ai_jobs": analysis.get("ai_jobs", 0),
#                     "job_market_score": score,
#                     "score_breakdown": analysis.get("score_breakdown", {}),
#                     "jobs": jobs,
#                 },
#                 s3_key,
#             )
#             logger.info(f"   üì§ S3: {s3_key}")

#             # Snowflake: insert signal
#             ai_count = analysis.get("ai_jobs", 0)
#             sources = list({j.get("source", "other") for j in jobs})
#             primary_src = max(
#                 defaultdict(int, {j.get("source", "other"): 1 for j in jobs}),
#                 key=lambda k: sum(1 for j in jobs if j.get("source") == k),
#                 default="other",
#             )

#             db.insert_external_signal(
#                 signal_id=f"{cid}_job_market_{timestamp}",
#                 company_id=cid,
#                 category="job_market",
#                 source=primary_src,
#                 score=score,
#                 evidence_count=ai_count,
#                 summary=f"Found {ai_count} AI roles out of {len(jobs)} jobs",
#                 raw_payload={
#                     "collection_date": timestamp,
#                     "s3_key": s3_key,
#                     "total_jobs": len(jobs),
#                     "ai_jobs": ai_count,
#                     "ai_ratio": analysis.get("ai_ratio", 0),
#                     "score_breakdown": analysis.get("score_breakdown", {}),
#                     "confidence": analysis.get("confidence", 0.5),
#                     "sources": sources,
#                 },
#             )
#             logger.info(f"   üíæ Snowflake: {company_name} (score: {score})")

#         logger.info(f"   ‚úÖ Stored {len(company_jobs)} companies to S3 + Snowflake")
#         return state
#     finally:
#         db.close()


# # ---------------------------------------------------------------------------
# # Main pipeline runner
# # ---------------------------------------------------------------------------

# async def run_job_signals(
#     state: SignalPipelineState,
#     *,
#     skip_storage: bool = False,
# ) -> SignalPipelineState:
#     """
#     Run the job signals pipeline: scrape ‚Üí classify ‚Üí score ‚Üí store.

#     Args:
#         state: Pipeline state with companies loaded
#         skip_storage: If True, skip S3/Snowflake storage
#     """
#     state = step1_init(state)
#     state = await step2_fetch_job_postings(state)
#     state = step3_classify_ai_jobs(state)
#     state = step4_score_job_market(state)
#     if not skip_storage:
#         state = step5_store_to_s3_and_snowflake(state)
#     return state


"""
Job Signals Pipeline ‚Äî Technology Hiring
app/pipelines/job_signals.py

ALIGNED WITH CASE STUDY 2 PDF SPEC (pages 14-16).

Key changes:
  - Added _is_tech_job() filter per PDF page 16
  - Scoring formula now 60/20/20 per PDF page 15:
      AI ratio (within tech jobs) * 60  (max 60)
      Skill diversity / 10 * 20         (max 20)
      Volume bonus min(ai_jobs/5, 1)*20 (max 20)
  - classify uses multi-word AI_KEYWORDS (no single-word false positives)
  - AI_SKILLS used for diversity scoring
"""
from __future__ import annotations

import asyncio
import json
import logging
import re
from collections import defaultdict
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

from app.config import (
    settings,
    get_company_search_name,
    get_company_aliases,
    get_search_name_by_official,
    get_aliases_by_official,
)
from app.models.signal import JobPosting
from app.pipelines.keywords import (
    AI_KEYWORDS,
    AI_SKILLS,
    AI_TECHSTACK_KEYWORDS,
    TECH_JOB_TITLE_KEYWORDS,
)
from app.pipelines.signal_pipeline_state import SignalPipelineState
from app.pipelines.utils import clean_nan, safe_filename

from rapidfuzz import fuzz

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Fuzzy matching (unchanged)
# ---------------------------------------------------------------------------

def is_company_match_fuzzy(
    job_company: str,
    target_company: str,
    threshold: float = 75.0,
    ticker: Optional[str] = None,
) -> bool:
    if not job_company or not target_company:
        return False
    job_clean = str(job_company).strip().lower()
    valid_names = [target_company]
    if ticker:
        valid_names.extend(get_company_aliases(ticker))
    else:
        aliases = get_aliases_by_official(target_company)
        if aliases:
            valid_names.extend(aliases)
    valid_names_clean = list({n.strip().lower() for n in valid_names if n})
    if job_clean in valid_names_clean:
        return True
    for vn in valid_names_clean:
        scores = [
            fuzz.token_sort_ratio(job_clean, vn),
            fuzz.partial_ratio(job_clean, vn),
            fuzz.ratio(job_clean, vn),
        ]
        if max(scores) >= threshold:
            return True
    return False


# ---------------------------------------------------------------------------
# Tech job filter (PDF page 16)
# ---------------------------------------------------------------------------

def _is_tech_job(posting: Dict[str, Any]) -> bool:
    """
    Check if a posting is a technology job by scanning the TITLE only.
    Per CS2 PDF page 16: filter tech jobs before computing AI ratio.
    """
    title = posting.get("title", "").lower()
    return any(kw in title for kw in TECH_JOB_TITLE_KEYWORDS)


# ---------------------------------------------------------------------------
# Deduplication
# ---------------------------------------------------------------------------

def _deduplicate_postings(postings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Remove duplicate job postings using two dedup strategies:
      1. By URL ‚Äî exact same job link from the same or different source
      2. By title + company + location ‚Äî same job posted across sites

    When duplicates exist, prefer the one with a longer description.
    """
    seen_urls: Dict[str, Dict[str, Any]] = {}
    seen_keys: Dict[str, Dict[str, Any]] = {}

    def _keep_better(existing: Dict, new: Dict) -> Dict:
        """Keep whichever posting has the longer description."""
        if len(new.get("description") or "") > len(existing.get("description") or ""):
            return new
        return existing

    for p in postings:
        # --- Strategy 1: Dedup by URL ---
        url = (p.get("url") or "").strip()
        if url:
            # Normalize Indeed URLs: strip tracking params, keep job key
            url_key = url.split("?")[0].lower() if "indeed.com" not in url else url.lower()
            # For Indeed, extract the jk= param as the unique key
            if "jk=" in url:
                jk = url.split("jk=")[-1].split("&")[0]
                url_key = f"indeed|{jk}"

            if url_key in seen_urls:
                seen_urls[url_key] = _keep_better(seen_urls[url_key], p)
                continue
            seen_urls[url_key] = p

        # --- Strategy 2: Dedup by title + company + location ---
        title = p.get("title", "").strip().lower()
        company = p.get("company_name", "").strip().lower()
        location = (p.get("location") or "").strip().lower()
        key = f"{title}|{company}|{location}"

        if key in seen_keys:
            seen_keys[key] = _keep_better(seen_keys[key], p)
            continue
        seen_keys[key] = p

    # Merge: URL-deduped postings take priority, then add any title-based
    # that weren't already caught by URL
    final: Dict[str, Dict[str, Any]] = {}
    for p in list(seen_urls.values()) + list(seen_keys.values()):
        pid = p.get("id", id(p))
        if pid not in final:
            final[pid] = p

    return list(final.values())


# ---------------------------------------------------------------------------
# Pipeline steps
# ---------------------------------------------------------------------------

def step1_init(state: SignalPipelineState) -> SignalPipelineState:
    state.mark_started()
    logger.info("-" * 40)
    logger.info("üìÅ [1/4] INITIALIZING JOB COLLECTION")
    return state


async def step2_fetch_job_postings(
    state: SignalPipelineState,
    *,
    sites: Optional[List[str]] = None,
    results_wanted: Optional[int] = None,
    hours_old: Optional[int] = None,
) -> SignalPipelineState:
    """Scrape job postings for all companies in state."""
    logger.info("-" * 40)
    logger.info("üîç [2/4] FETCHING JOB POSTINGS")

    sites = sites or settings.JOBSPY_DEFAULT_SITES
    results_wanted = results_wanted or settings.JOBSPY_RESULTS_WANTED
    hours_old = hours_old or settings.JOBSPY_HOURS_OLD

    logger.info(f"   Sites: {', '.join(sites)}")
    logger.info(f"   Max results: {results_wanted}")
    logger.info(f"   Hours old: {hours_old}")

    try:
        from jobspy import scrape_jobs
    except ImportError as e:
        msg = "python-jobspy not installed. Run: pip install python-jobspy"
        logger.error(f"   ‚ùå {msg}")
        state.add_error("job_fetch", msg)
        raise ImportError(msg) from e

    for company in state.companies:
        company_id = company.get("id", "")
        company_name = company.get("name", "")
        ticker = company.get("ticker", "").upper()
        if not company_name:
            continue

        search_name = (
            get_company_search_name(ticker)
            or get_search_name_by_official(company_name)
            or company_name
        )

        await asyncio.sleep(max(state.request_delay, settings.JOBSPY_REQUEST_DELAY))

        try:
            logger.info(f"   üì• Scraping: {company_name} (search: '{search_name}')...")

            jobs_df = scrape_jobs(
                site_name=sites,
                search_term=search_name,
                results_wanted=results_wanted,
                hours_old=hours_old,
                country_indeed="USA",
                linkedin_fetch_description=True,
            )

            postings: List[Dict[str, Any]] = []
            filtered_count = 0
            total_raw = 0

            if jobs_df is not None and not jobs_df.empty:
                total_raw = len(jobs_df)
                for _, row in jobs_df.iterrows():
                    job_company = (
                        str(row.get("company", ""))
                        if clean_nan(row.get("company"))
                        else ""
                    )
                    source = str(row.get("site", "unknown"))

                    if not is_company_match_fuzzy(
                        job_company, search_name,
                        threshold=settings.JOBSPY_FUZZY_MATCH_THRESHOLD,
                        ticker=ticker,
                    ):
                        filtered_count += 1
                        continue

                    posting = JobPosting(
                        company_id=company_id,
                        company_name=job_company,
                        title=str(row.get("title", "")),
                        description=str(row.get("description", "")),
                        location=(
                            str(row.get("location", ""))
                            if clean_nan(row.get("location"))
                            else None
                        ),
                        posted_date=clean_nan(row.get("date_posted")),
                        source=source,
                        url=(
                            str(row.get("job_url", ""))
                            if clean_nan(row.get("job_url"))
                            else None
                        ),
                    )
                    postings.append(posting.model_dump())

            state.job_postings.extend(postings)
            state.summary["job_postings_collected"] += len(postings)

            logger.info(f"      ‚Ä¢ Raw: {total_raw} | Matched: {len(postings)} | Filtered: {filtered_count}")

        except Exception as e:
            state.add_error("job_fetch", str(e), company_id)
            logger.error(f"      ‚ùå Error: {e}")

    # --- Deduplicate job postings ---
    before_dedup = len(state.job_postings)
    state.job_postings = _deduplicate_postings(state.job_postings)
    dupes_removed = before_dedup - len(state.job_postings)

    logger.info(f"   ‚úÖ Total collected: {before_dedup} job postings")
    if dupes_removed > 0:
        logger.info(f"   üßπ Removed {dupes_removed} duplicates ‚Üí {len(state.job_postings)} unique postings")
    return state


# ---------------------------------------------------------------------------
# Classification (PDF page 14-15)
# ---------------------------------------------------------------------------

def _has_keyword(text: str, keyword: str) -> bool:
    """Match multi-word keywords in text. All AI_KEYWORDS are multi-word
    or unambiguous, so simple substring match is safe."""
    return keyword in text


def step3_classify_ai_jobs(state: SignalPipelineState) -> SignalPipelineState:
    """
    Classify each job posting as AI-related.
    Per PDF page 15: check title + description for AI_KEYWORDS.
    Also extract AI_SKILLS for diversity scoring.
    """
    logger.info("-" * 40)
    logger.info("ü§ñ [3/4] CLASSIFYING AI-RELATED JOBS")

    for posting in state.job_postings:
        title = posting.get("title", "")
        desc = posting.get("description", "") or ""

        # Combine title + description for keyword search
        text = f"{title} {desc}".lower()

        # Find AI keywords (for is_ai_role classification)
        ai_kw = [kw for kw in AI_KEYWORDS if _has_keyword(text, kw)]

        # Find AI skills (for diversity scoring)
        skills = [sk for sk in AI_SKILLS if _has_keyword(text, sk)]

        # Find techstack keywords (kept for metadata)
        ts_kw = [kw for kw in AI_TECHSTACK_KEYWORDS if _has_keyword(text, kw)]

        posting["ai_keywords_found"] = ai_kw
        posting["ai_skills_found"] = skills
        posting["techstack_keywords_found"] = ts_kw

        # A job is AI-related if ANY AI keyword matches (PDF page 15 line 94)
        posting["is_ai_role"] = len(ai_kw) > 0

        # Score for metadata (not used in final scoring formula)
        posting["ai_score"] = min(100.0, len(ai_kw) * 15.0)

    ai_count = sum(1 for p in state.job_postings if p.get("is_ai_role"))
    total = len(state.job_postings)
    tech_count = sum(1 for p in state.job_postings if _is_tech_job(p))

    logger.info(f"   ‚Ä¢ Total: {total} | Tech jobs: {tech_count} | AI-related: {ai_count}")
    if tech_count > 0:
        logger.info(f"   ‚Ä¢ AI ratio (within tech): {ai_count / tech_count * 100:.1f}%")
    elif total > 0:
        logger.info(f"   ‚Ä¢ AI ratio (all jobs): {ai_count / total * 100:.1f}%")

    return state


# ---------------------------------------------------------------------------
# Scoring (PDF page 15)
# ---------------------------------------------------------------------------

def calculate_job_score(jobs: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Calculate technology_hiring score (0-100) per CS2 PDF page 15.

    Formula:
      - AI ratio:       min(ai_ratio * 60, 60)       max 60 pts
        where ai_ratio = ai_jobs / total_TECH_jobs (not all jobs)
      - Skill diversity: min(len(skills) / 10, 1) * 20  max 20 pts
      - Volume bonus:    min(ai_jobs / 5, 1) * 20       max 20 pts

    Confidence based on total_tech_jobs (more data = higher confidence).
    """
    total_all = len(jobs)
    tech_jobs = [j for j in jobs if _is_tech_job(j)]
    total_tech = len(tech_jobs)

    # AI jobs must also be tech jobs to count
    ai_jobs_list = [j for j in tech_jobs if j.get("is_ai_role", False)]
    ai_jobs = len(ai_jobs_list)

    # Collect all unique AI skills across ALL postings (including non-tech)
    all_skills = set()
    for j in jobs:
        all_skills.update(j.get("ai_skills_found", []))

    # Also count skills from ai_keywords_found for backward compat
    all_kw = set()
    for j in jobs:
        all_kw.update(j.get("ai_keywords_found", []))

    # --- Scoring per PDF ---
    ai_ratio = ai_jobs / total_tech if total_tech > 0 else 0
    ratio_score = min(ai_ratio * 60, 60)
    diversity_score = min(len(all_skills) / 10, 1) * 20
    volume_score = min(ai_jobs / 5, 1) * 20
    final = round(ratio_score + diversity_score + volume_score, 1)

    # Confidence based on tech job sample size (PDF page 15 line 80)
    confidence = min(0.5 + total_tech / 100, 0.95)

    return {
        "score": final,
        "ai_jobs": ai_jobs,
        "total_jobs": total_all,
        "total_tech_jobs": total_tech,
        "ai_ratio": round(ai_ratio, 3),
        "ai_keywords": sorted(all_kw),
        "ai_skills": sorted(all_skills),
        "score_breakdown": {
            "ratio_score": round(ratio_score, 1),
            "volume_score": round(volume_score, 1),
            "diversity_score": round(diversity_score, 1),
        },
        "confidence": round(confidence, 3),
    }


def step4_score_job_market(state: SignalPipelineState) -> SignalPipelineState:
    """Score job market (technology_hiring) for each company."""
    logger.info("-" * 40)
    logger.info("üìä [4/4] SCORING JOB MARKET")

    name_lookup = {c.get("id"): c.get("name", c.get("id")) for c in state.companies}
    company_jobs: Dict[str, List] = defaultdict(list)
    for p in state.job_postings:
        company_jobs[p["company_id"]].append(p)

    for cid, jobs in company_jobs.items():
        if not jobs:
            state.job_market_scores[cid] = 0.0
            continue

        analysis = calculate_job_score(jobs)
        state.job_market_scores[cid] = round(analysis["score"], 2)
        state.job_market_analyses[cid] = analysis

        name = name_lookup.get(cid, cid)
        bd = analysis["score_breakdown"]
        logger.info(
            f"   ‚Ä¢ {name}: {analysis['score']:.1f}/100 "
            f"(ratio={bd['ratio_score']:.1f}, vol={bd['volume_score']:.1f}, "
            f"div={bd['diversity_score']:.1f}) "
            f"[{analysis['ai_jobs']} AI / {analysis['total_tech_jobs']} tech / {analysis['total_jobs']} total]"
        )

    logger.info(f"   ‚úÖ Scored {len(state.job_market_scores)} companies")
    return state


# ---------------------------------------------------------------------------
# S3 + Snowflake storage (unchanged logic, updated metadata)
# ---------------------------------------------------------------------------

def step5_store_to_s3_and_snowflake(state: SignalPipelineState) -> SignalPipelineState:
    from app.services.s3_storage import get_s3_service
    from app.services.snowflake import SnowflakeService

    logger.info("-" * 40)
    logger.info("‚òÅÔ∏è [5/5] STORING TO S3 & SNOWFLAKE")

    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    s3 = get_s3_service()
    db = SnowflakeService()

    try:
        company_jobs: Dict[str, List] = defaultdict(list)
        for p in state.job_postings:
            company_jobs[p["company_id"]].append(p)

        for cid, jobs in company_jobs.items():
            if not jobs:
                continue

            company_name = jobs[0].get("company_name", cid)
            ticker = None
            for c in state.companies:
                if c.get("id") == cid:
                    ticker = c.get("ticker", "").upper()
                    break
            ticker = ticker or safe_filename(company_name).upper()

            analysis = state.job_market_analyses.get(cid, {})
            score = state.job_market_scores.get(cid, 0.0)

            s3_key = f"signals/jobs/{ticker}/{timestamp}.json"
            s3.upload_json(
                {
                    "company_id": cid,
                    "company_name": company_name,
                    "ticker": ticker,
                    "collection_date": timestamp,
                    "total_jobs": len(jobs),
                    "total_tech_jobs": analysis.get("total_tech_jobs", 0),
                    "ai_jobs": analysis.get("ai_jobs", 0),
                    "job_market_score": score,
                    "score_breakdown": analysis.get("score_breakdown", {}),
                    "jobs": jobs,
                },
                s3_key,
            )
            logger.info(f"   üì§ S3: {s3_key}")

            ai_count = analysis.get("ai_jobs", 0)
            sources = list({j.get("source", "other") for j in jobs})
            primary_src = max(
                defaultdict(int, {j.get("source", "other"): 1 for j in jobs}),
                key=lambda k: sum(1 for j in jobs if j.get("source") == k),
                default="other",
            )

            db.insert_external_signal(
                signal_id=f"{cid}_job_market_{timestamp}",
                company_id=cid,
                category="job_market",
                source=primary_src,
                score=score,
                evidence_count=ai_count,
                summary=f"Found {ai_count} AI roles out of {analysis.get('total_tech_jobs', 0)} tech jobs ({len(jobs)} total)",
                raw_payload={
                    "collection_date": timestamp,
                    "s3_key": s3_key,
                    "total_jobs": len(jobs),
                    "total_tech_jobs": analysis.get("total_tech_jobs", 0),
                    "ai_jobs": ai_count,
                    "ai_ratio": analysis.get("ai_ratio", 0),
                    "score_breakdown": analysis.get("score_breakdown", {}),
                    "confidence": analysis.get("confidence", 0.5),
                    "ai_skills": analysis.get("ai_skills", []),
                    "sources": sources,
                },
            )
            logger.info(f"   üíæ Snowflake: {company_name} (score: {score})")

        logger.info(f"   ‚úÖ Stored {len(company_jobs)} companies to S3 + Snowflake")
        return state
    finally:
        db.close()


# ---------------------------------------------------------------------------
# Main pipeline runner
# ---------------------------------------------------------------------------

async def run_job_signals(
    state: SignalPipelineState,
    *,
    skip_storage: bool = False,
) -> SignalPipelineState:
    state = step1_init(state)
    state = await step2_fetch_job_postings(state)
    state = step3_classify_ai_jobs(state)
    state = step4_score_job_market(state)
    if not skip_storage:
        state = step5_store_to_s3_and_snowflake(state)
    return state